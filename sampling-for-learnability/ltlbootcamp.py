# -*- coding: utf-8 -*-
"""LTLBootCamp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1avmI9TzqXvSku_eOAhRdlEdz2I1JEhxv
"""

#!pip install    optax jax
# !pip uninstall -y jax jaxlib jax-cuda12-plugin
# # The new, simplified command for installing JAX with CUDA support
# !pip install -U "jax[cuda]"

#!pip install --upgrade rlax



import jax
import jax.numpy as jnp
from jax import lax
from jax.random import PRNGKey
from collections import OrderedDict
from typing import NamedTuple, Tuple, List, Dict, Any, Union
from functools import partial
import numpy as np
import matplotlib.pyplot as plt
#import networkx as nx
import re
import random as py_random
from jax import random
from functools import partial
from collections import OrderedDict
#import graphviz
#from PIL import Image, ImageDraw, ImageFont
import os

def ltl_tuple_to_string( ltl_tuple):
    """
    Recursively converts a nested LTL tuple into a human-readable string.

    Args:
        ltl_tuple: The LTL formula represented as a string (for atomic
                propositions) or a tuple (for logical operators).

    Returns:
        A string representation of the LTL formula.
    """
    
    # Base Case: If the input is not a tuple, it's an atomic proposition (string).
    if not isinstance(ltl_tuple, tuple):
        return str(ltl_tuple)

    # Recursive Step: The input is a tuple, so process the operator.
    operator = ltl_tuple[0]
    
    # --- Unary Operators (1 operand) ---
    if operator == 'not':
        # Format: !(operand)
        operand_str = ltl_tuple_to_string(ltl_tuple[1])
        return f"!({operand_str})"
        
    elif operator == 'eventually':
        # Format: F(operand)
        operand_str = ltl_tuple_to_string(ltl_tuple[1])
        return f"F({operand_str})"
        
    elif operator == 'globally':
        # Format: G(operand)
        operand_str = ltl_tuple_to_string(ltl_tuple[1])
        return f"G({operand_str})"
        
    elif operator == 'next':
        # Format: X(operand)
        operand_str = ltl_tuple_to_string(ltl_tuple[1])
        return f"X({operand_str})"

    # --- Binary Operators (2 operands) ---
    elif operator == 'and':
        # Format: (left & right)
        left_str = ltl_tuple_to_string(ltl_tuple[1])
        right_str = ltl_tuple_to_string(ltl_tuple[2])
        return f"({left_str} & {right_str})"
        
    elif operator == 'or':
        # Format: (left | right)
        left_str = ltl_tuple_to_string(ltl_tuple[1])
        right_str = ltl_tuple_to_string(ltl_tuple[2])
        return f"({left_str} | {right_str})"
        
    elif operator == 'until':
        # Format: (left U right)
        left_str = ltl_tuple_to_string(ltl_tuple[1])
        right_str = ltl_tuple_to_string(ltl_tuple[2])
        return f"({left_str} U {right_str})"
        
    # Fallback for any unknown operator
    else:
        # Just return the tuple as a string if the operator is not recognized
        return str(ltl_tuple)

def get_propositions_in_formula(ltl_formula):
    """
    Recursively finds all unique propositions (atomic strings)
    in a formula represented as a nested tuple.
    """
    props = set()

    if isinstance(ltl_formula, str):
        # Is a string, check if it's a proposition
        if ltl_formula != 'True' and ltl_formula != 'False':
            props.add(ltl_formula)
        return props

    if isinstance(ltl_formula, tuple):
        # Is a tuple, e.g., ('and', 'a', 'b')
        # The first element is the operator, skip it.
        # Recursively check all other elements in the tuple.
        for sub_formula in ltl_formula[1:]:
            props.update(get_propositions_in_formula(sub_formula))

    return props




# Define a BuilderState to pass through loops
class BuilderState(NamedTuple):
    """Holds the state of the graph construction."""
    node_idx: jnp.ndarray
    edge_idx: jnp.ndarray
    nodes: jnp.ndarray
    senders: jnp.ndarray
    receivers: jnp.ndarray
    edge_types: jnp.ndarray

class SingleFormulaState(NamedTuple):
    """
    Holds the state for *one* nested 'Until' formula.
    Used as the input/output for the vmapped function.
    
    active_pointers: Boolean array, True if sub-formula at depth `i` is active.
    to_avoid: Integer array of propositions 'a' in `!a U ...`.
    to_progress: Integer array of propositions 'b' in `... U (b & ...)`.
    """
    active_pointers: jnp.ndarray
    to_avoid: jnp.ndarray
    to_progress: jnp.ndarray

class ConjunctionState(NamedTuple):
    """
    Holds the batched state for a *conjunction* of N formulas.
    All arrays are padded to GLOBAL_MAX_DEPTH.
    
    active_pointers: (N, GLOBAL_MAX_DEPTH) bool
    to_avoid: (N, GLOBAL_MAX_DEPTH) int
    to_progress: (N, GLOBAL_MAX_DEPTH) int
    depths: (N,) int - The *actual* depth of each formula.
    already_true: (N,) bool - Mask of formulas that have resolved to True.
    """
    active_pointers: jnp.ndarray
    to_avoid: jnp.ndarray
    to_progress: jnp.ndarray
    depths: jnp.ndarray
    already_true: jnp.ndarray


class JaxUntilTaskSampler:
    """
    A JAX-based LTL sampler for nested 'Until' formulas.

    This class samples a conjunction of 'Until' tasks and directly
    encodes them into a JAX-compatible `ConjunctionState` for use
    with jitted progress functions.
    """
    def __init__(self, 
                 propositions: List[str], 
                 min_levels: int = 1, 
                 max_levels: int = 2, 
                 min_conjunctions: int = 1, 
                 max_conjunctions: int = 2,
                ):
        """
        Initializes the sampler.

        Args:
            propositions: List of all available proposition names.
            min_levels: Minimum nesting depth of a single 'Until' formula.
            max_levels: Maximum nesting depth (GLOBAL_MAX_DEPTH).
            min_conjunctions: Minimum number of formulas in the conjunction.
            max_conjunctions: Maximum number of formulas (GLOBAL_MAX_CONJUNCTIONS).
        """
        self.min_levels = int(min_levels)
        self.max_levels = int(max_levels)
        self.min_conjunctions = int(min_conjunctions)
        self.max_conjunctions = int(max_conjunctions)
        self.propositions = sorted(list(set(propositions)))

        self.LTL_BASE_VOCAB = {
            "and": 0, "or": 1, "not": 2, "next": 3, "until": 4,
            "always": 5, "eventually": 6, "True": 7, "False": 8,
        }
        
        self.PROPS_OFFSET = len(self.LTL_BASE_VOCAB)
        for i, el in enumerate(self.propositions):
            self.LTL_BASE_VOCAB[el] = self.PROPS_OFFSET + i
        self.prop_map=self.LTL_BASE_VOCAB
   
        
        # Get all proposition indices as a JAX array for sampling
        
        
        # Calculate the absolute maximum number of propositions we could need
        self.max_props_needed = 2 * self.max_levels * self.max_conjunctions
        self.vocab_size = len(self.LTL_BASE_VOCAB)
        self.feature_size = self.vocab_size + 1 # One-hot + is_root

        self.prop_indices = jnp.arange(len(self.propositions), dtype=jnp.int32)
        
        self.EDGE_TYPES = {
            "self": 0,
            "arg": 1,    # For unary operators
            "arg1": 2,   # For binary operator, arg 1
            "arg2": 3,   # For binary operator, arg 2
        }
        # Create the inverse mapping for labeling
        self.INV_EDGE_TYPES = {v: k for k, v in self.EDGE_TYPES.items()}
        self.INV_LTL_BASE_VOCAB = {v: k for k, v in self.LTL_BASE_VOCAB.items()}
        

        self._vmapped_progress = jax.vmap(
        self._progress_single_formula,
        in_axes=(SingleFormulaState(0, 0, 0), None, 0),
        out_axes=(SingleFormulaState(0, 0, 0), 0, 0)
    )
        # Assert that we have enough unique propositions
        if self.max_props_needed > len(self.propositions):
            raise ValueError(
                f"Not enough propositions! Need at most {self.max_props_needed} "
                f"(2 * max_levels * max_conjunctions), but only have {len(self.propositions)}."
            )

    
   ############################### SAMPLE #######################################################################
    
    @partial(jax.jit, static_argnames=['self'])
    def sample(self, key: PRNGKey) -> ConjunctionState:
        """
        Samples a new LTL task and returns its JAX ConjunctionState.

        This function is designed to be JIT-compiled.

        Args:
            key: A JAX PRNGKey.

        Returns:
            A `ConjunctionState` tuple containing the encoded formula.
        """
        # Define global padding sizes from config
        N = self.max_conjunctions # Max number of conjunctions
        M = self.max_levels      # Max nesting depth
        
        # Split key for all random operations
        key, n_conjs_key, depths_key, choice_key = jax.random.split(key, 4)

        # 1. Sample the number of *actual* conjunctions
        n_conjs = jax.random.randint(
            n_conjs_key, 
            shape=(), 
            minval=self.min_conjunctions, 
            maxval=N + 1 # maxval is exclusive
        )
        
        # 2. Create masks based on the number of conjunctions
        # (N,) bool: True for active, False for padded
        conjunction_mask = jnp.arange(N) < n_conjs 
        # (N,) bool: False for active, True for padded (initial state)
        already_true = ~conjunction_mask 
        
        # 3. Sample depths for all N slots
        # (N,) int: Sampled depths
        sampled_depths = jax.random.randint(
            depths_key, 
            shape=(N,), 
            minval=self.min_levels, 
            maxval=M + 1 # maxval is exclusive
        )
        # Use real depths for active conjunctions, dummy depth (e.g., 1) for padded
        depths = jnp.where(conjunction_mask, sampled_depths, 1)

        # 4. Create initial active_pointers
        # (N, M) bool: All zeros
        active_pointers = jnp.zeros((N, M), dtype=bool)
        
        # --- THIS IS THE FIX ---
        # Set the *entire* first column (a static slice [:, 0]) 
        # to the *dynamic* conjunction_mask. This is JIT-compatible.
        active_pointers = active_pointers.at[:, 0].set(conjunction_mask)
        # --- END FIX ---
        
        # 5. Sample all propositions needed
        # We sample enough for the *entire* (N, M) block
        sampled_indices = jax.random.choice(
            choice_key, 
            self.prop_indices, 
            shape=(self.max_props_needed,), 
            replace=False
        )
        
        # Reshape into (N, M, 2)
        all_props = sampled_indices.reshape((N, M, 2))
        
        # Separate into full 'to_avoid' and 'to_progress' arrays
        to_avoid_full = all_props[..., 0]   # (N, M)
        to_progress_full = all_props[..., 1] # (N, M)
        
        # 6. Mask the prop arrays based on actual depths
        # (N, M) bool: True for slots within the *actual* depth
        depth_mask = jnp.arange(M) < depths[:, None]
        
        # Set all padded slots (outside actual depth) to -1
        to_avoid = jnp.where(depth_mask, to_avoid_full, -1)
        to_progress = jnp.where(depth_mask, to_progress_full, -1)
        
        # 7. Construct and return the state
        return ConjunctionState(
            active_pointers=active_pointers,
            to_avoid=to_avoid,
            to_progress=to_progress,
            depths=depths,
            already_true=already_true
        ), n_conjs, jnp.average(sampled_depths)

    @partial(jax.jit, static_argnames=['self'])
    def _progress_single_formula(
        self,
        state: SingleFormulaState,
        current_props: jnp.ndarray,
        depth: jnp.ndarray # The *actual* depth of this formula
    ) -> Tuple[SingleFormulaState, jnp.ndarray, jnp.ndarray]:
        """
        Progresses a *single* nested 'Until' formula.
        This function is designed to be vmapped.
        """
        MAX_DEPTH = state.to_avoid.shape[0] # This is the GLOBAL_MAX_DEPTH
        new_active = jnp.zeros(MAX_DEPTH, dtype=bool)
        is_true_overall = jnp.array(False)
    
        for i in range(MAX_DEPTH):
            # --- 1. 'Stays Active' (from active_pointers[i]) ---
            is_active_i = state.active_pointers[i]
            phi_progresses_i = ~current_props[state.to_avoid[i]]
            stays_active = is_active_i & phi_progresses_i
    
            # --- 2. 'Gets Activated' (from active_pointers[i-1]) ---
            gets_activated = jnp.array(False)
            if i > 0:
                is_active_prev = state.active_pointers[i-1]
                psi_progresses_prev = current_props[state.to_progress[i-1]]
                gets_activated = is_active_prev & psi_progresses_prev
    
            new_active = new_active.at[i].set(stays_active | gets_activated)
    
            # --- 3. Check for immediate 'True' satisfaction ---
            # This now uses the 'depth' argument to check at the correct level.
            is_last_real_prop = (i == depth - 1)
            is_active_at_depth = state.active_pointers[i]
            psi_progresses_at_depth = current_props[state.to_progress[i]]
            
            # If this is the last *real* proposition for this formula,
            # and it's active and its 'psi' progresses, the formula resolves to True.
            becomes_true = is_active_at_depth & psi_progresses_at_depth & is_last_real_prop
            is_true_overall = is_true_overall | becomes_true
    
        # The entire formula is False if no branches are active in the new state
        # AND it didn't just become True.
        is_false_overall = ~jnp.any(new_active) & ~is_true_overall
    
        # If formula resolved to True or False, clear all active pointers.
        # This prevents resolved formulas from progressing further.
        final_active_pointers = lax.cond(
            is_true_overall | is_false_overall,
            lambda: jnp.zeros_like(new_active),
            lambda: new_active
        )
        new_state = state._replace(active_pointers=final_active_pointers)
        
        return new_state, is_true_overall, is_false_overall
    
    # --- Vmapped and Public Functions ---
    
    # Create a vmapped version of the single-formula progress function.
    # We map over:
    #   - SingleFormulaState (all fields, axis 0)
    #   - current_props (None, broadcast)
    #   - depths (axis 0)
    
    @partial(jax.jit, static_argnames=['self'])
    def progress(
        self, 
        state: ConjunctionState,
        current_props: jnp.ndarray
    ) -> Tuple[ConjunctionState, jnp.ndarray, jnp.ndarray]:
        """
        Progresses an entire conjunction of N formulas in parallel.
        """
        
        # 1. Create a mask of tasks that are *still progressing*
        #    (i.e., not already True).
        progressing_mask = ~state.already_true # Shape (N,)
    
        # 2. We only want to progress tasks that are still progressing.
        #    We "mask" the input `active_pointers`.
        #    `active_pointers` for non-progressing tasks are set to all-False.
        input_active_pointers = state.active_pointers * jnp.expand_dims(progressing_mask, -1)
        
        vmap_input_state = SingleFormulaState(
            input_active_pointers, state.to_avoid, state.to_progress
        )
    
        # 3. Run the vmapped progress function
        vmap_output_state, is_true_this_step, is_false_this_step = \
            self._vmapped_progress(vmap_input_state, current_props, state.depths)
        
        # 4. Update the `already_true` mask
        # A task is now true if it was *already* true OR it *just became* true.
        new_already_true = state.already_true | is_true_this_step
        
        # 5. Determine overall conjunction status
        # Conjunction is False if any *progressing* task *just became* False.
        is_false_overall = jnp.any(is_false_this_step)
        
        # Conjunction is True if *all* tasks are now in the `already_true` state.
        is_true_overall = jnp.all(new_already_true)
        
        # 6. Create the new state.
        # The new `active_pointers` are what vmap returned.
        # (Tasks that just finished are zeroed by `_progress_single_formula`).
        # (Tasks that *were* finished were zeroed on input).
        new_conjunction_state = ConjunctionState(
            active_pointers=vmap_output_state.active_pointers,
            to_avoid=state.to_avoid,
            to_progress=state.to_progress,
            depths=state.depths,
            already_true=new_already_true
        )
        is_false_overall = jnp.any(is_false_this_step & progressing_mask)
        
        return new_conjunction_state, is_true_overall, is_false_overall

    
    
    
    
    
    ###################################### AST TREE ###############################################################
    @partial(jax.jit, static_argnames=['self'])
    def _one_hot_base(self, token_id: jnp.ndarray) -> jnp.ndarray:
        return jax.nn.one_hot(token_id, self.vocab_size, dtype=jnp.float32)
    @partial(jax.jit, static_argnames=['self'])
    def _add_node(
        self,
        builder_state: BuilderState, 
        token_id: jnp.ndarray
    ) -> Tuple[BuilderState, jnp.ndarray]:
        """
        Adds a node to the graph buffers and returns its index.
        The 'is_root' flag (last feature) defaults to 0.0.
        """
        idx = builder_state.node_idx
        
        # Create base features
        base_features = self._one_hot_base(token_id)
        
        # Create the 'is_root' feature, defaulting to 0.0
        is_root_feature = jnp.array([0.0], dtype=jnp.float32)
        
        # Concat base features and is_root feature
        node_feature_vector = jnp.concatenate(
            [base_features, is_root_feature], 
            axis=0
        )
        
        # Add node feature
        nodes = builder_state.nodes.at[idx].set(node_feature_vector)
        
        # Add self-loop edge
        senders = builder_state.senders.at[builder_state.edge_idx].set(idx)
        receivers = builder_state.receivers.at[builder_state.edge_idx].set(idx)
        edge_types = builder_state.edge_types.at[builder_state.edge_idx].set(self.EDGE_TYPES["self"])
        
        new_state = builder_state._replace(
            node_idx=idx + 1,
            edge_idx=builder_state.edge_idx + 1,
            nodes=nodes,
            senders=senders,
            receivers=receivers,
            edge_types=edge_types
        )
        return new_state, idx
    @partial(jax.jit, static_argnames=['self'])
    def _add_edge(
        self,
        builder_state: BuilderState, 
        sender: jnp.ndarray, 
        receiver: jnp.ndarray, 
        edge_type: int
    ) -> BuilderState:
        """Adds a directional edge to the graph buffers."""
        idx = builder_state.edge_idx
        senders = builder_state.senders.at[idx].set(sender)
        receivers = builder_state.receivers.at[idx].set(receiver)
        edge_types = builder_state.edge_types.at[idx].set(edge_type)
        
        return builder_state._replace(
            edge_idx=idx + 1,
            senders=senders,
            receivers=receivers,
            edge_types=edge_types
        )
    @partial(jax.jit, static_argnames=['self'])
    def _build_binary_tree(
        self,
        builder_state: BuilderState,
        leaf_indices: jnp.ndarray,
        leaf_mask: jnp.ndarray,
        op_token: int,
    ) -> Tuple[BuilderState, jnp.ndarray]:
        """
        Builds a JAX-native binary tree (like _build_tree) from a list of leaf node indices.
        
        Returns:
            (new_builder_state, root_node_index)
            Returns root_node_index = -1 if there are 0 active leaves.
        """
        
        # --- 1. Filter active leaf indices ---
        active_indices = jnp.where(leaf_mask, leaf_indices, -1)
        is_active = (active_indices != -1)
        active_count = jnp.sum(is_active)
        sorted_indices = jnp.argsort(jnp.where(is_active, jnp.arange(leaf_mask.shape[0]), leaf_mask.shape[0]))
        
        # Statically-sized array, active indices at the front.
        compressed_leaves = active_indices[sorted_indices]
    
        # --- 2. Build the binary tree from the compressed list ---
        
        # --- THIS IS THE FIX ---
        # The signature must be (loop_index, carry_state)
        def build_tree_recursive(
            start_idx: int,                          # loop_index 'i'
            state: Tuple[BuilderState, jnp.ndarray]  # carry_state 'val'
        ) -> Tuple[BuilderState, jnp.ndarray]:
        # --- END FIX ---
            """Iteratively combines leaf nodes under an operation."""
            builder_state, root_idx = state
            right_child_idx = compressed_leaves[start_idx]
            
            # Add new parent node (is_root defaults to 0.0)
            builder_state, new_root_idx = self._add_node(builder_state, jnp.array(op_token))
            
            # Add edges: child -> parent
            builder_state = self._add_edge(builder_state, root_idx, new_root_idx, self.EDGE_TYPES["arg1"])
            builder_state = self._add_edge(builder_state, right_child_idx, new_root_idx, self.EDGE_TYPES["arg2"])
            
            return (builder_state, new_root_idx)
    
        # --- 3. Handle 0, 1, or N active leaves ---
        def case_zero():
            # *** MODIFIED ***
            # 0 active leaves. Do not create a True/False node.
            # Return a null index (-1) and the unmodified state.
            return builder_state, jnp.array(-1)
    
        def case_one():
            # One leaf, just return its index. No new nodes.
            root_idx = compressed_leaves[0]
            return builder_state, root_idx
    
        def case_many():
            # N > 1 leaves. Build the binary tree.
            initial_root_idx = compressed_leaves[0]
            initial_state = (builder_state, initial_root_idx)
            
            final_state, final_root_idx = lax.fori_loop(
                1, 
                active_count, 
                build_tree_recursive, # This function now has the correct signature
                initial_state
            )
            return final_state, final_root_idx
    
        # Main logic
        branch_index = jnp.minimum(active_count, 2)
        
        return lax.switch(
            branch_index,
            [case_zero, case_one, case_many]
        )
    @partial(jax.jit, static_argnames=['self'])
    def _build_subformula_until(
        self,
        builder_state: BuilderState,
        avoid_props: jnp.ndarray,
        prog_props: jnp.ndarray,
        formula_depth: jnp.ndarray,
        start_depth: jnp.ndarray
    ) -> Tuple[BuilderState, jnp.ndarray]:
        """
        Builds the static AST for a *single* sub-formula:
        !avoid[i] U (prog[i] & (!avoid[i+1] U (...)))
        """
        
        # 1. Build the innermost (base case) formula
        last_idx = formula_depth - 1
        
        b_state, until_idx = self._add_node(builder_state, jnp.array(self.LTL_BASE_VOCAB["until"]))
        b_state, not_idx = self._add_node(b_state, jnp.array(self.LTL_BASE_VOCAB["not"]))
        
        avoid_token = avoid_props[last_idx] + self.PROPS_OFFSET
        b_state, avoid_idx = self._add_node(b_state, avoid_token)
        
        prog_token = prog_props[last_idx] + self.PROPS_OFFSET
        b_state, prog_idx = self._add_node(b_state, prog_token)
        
        b_state = self._add_edge(b_state, not_idx, until_idx, self.EDGE_TYPES["arg1"])
        b_state = self._add_edge(b_state, prog_idx, until_idx, self.EDGE_TYPES["arg2"])
        b_state = self._add_edge(b_state, avoid_idx, not_idx, self.EDGE_TYPES["arg"])
        
        current_root_idx = until_idx
    
        # 2. Loop to wrap the formula outwards
        def wrap_loop_body(m, state):
            b_state, current_root_idx = state
            
            b_state, new_until_idx = self._add_node(b_state, jnp.array(self.LTL_BASE_VOCAB["until"]))
            b_state, new_not_idx = self._add_node(b_state, jnp.array(self.LTL_BASE_VOCAB["not"]))
            
            avoid_token = avoid_props[m] + self.PROPS_OFFSET
            b_state, new_avoid_idx = self._add_node(b_state, avoid_token)
            
            b_state, new_and_idx = self._add_node(b_state, jnp.array(self.LTL_BASE_VOCAB["and"]))
            
            prog_token = prog_props[m] + self.PROPS_OFFSET
            b_state, new_prog_idx = self._add_node(b_state, prog_token)
            
            b_state = self._add_edge(b_state, new_not_idx, new_until_idx, self.EDGE_TYPES["arg1"])
            b_state = self._add_edge(b_state, new_and_idx, new_until_idx, self.EDGE_TYPES["arg2"])
            b_state = self._add_edge(b_state, new_avoid_idx, new_not_idx, self.EDGE_TYPES["arg"])
            b_state = self._add_edge(b_state, new_prog_idx, new_and_idx, self.EDGE_TYPES["arg1"])
            b_state = self._add_edge(b_state, current_root_idx, new_and_idx, self.EDGE_TYPES["arg2"])
            
            return (b_state, new_until_idx)
    
        # Re-parametrizing for lax.fori_loop (which increments)
        def loop_wrapper(i, state):
            m = (formula_depth - 2) - i
            return wrap_loop_body(m, state)
    
        n_wraps = (formula_depth - 1) - start_depth
        
        final_state, final_root_idx = lax.cond(
            n_wraps > 0,
            lambda: lax.fori_loop(0, n_wraps, loop_wrapper, (b_state, current_root_idx)),
            lambda: (b_state, current_root_idx)
        )
    
        return final_state, final_root_idx
    
    @partial(jax.jit, static_argnames=['self'])
    def _build_formula_or_tree(self, 
        builder_state: BuilderState,
        formula_idx: int,
        state: ConjunctionState
    ) -> Tuple[BuilderState, jnp.ndarray]:
        """
        Builds the 'OR' tree for a single formula 'n'.
        
        *** MODIFIED ***
        Returns root_node_index = -1 if formula is 'True' or 'False'.
        """
        M = state.active_pointers.shape[1]
        
        is_already_true = state.already_true[formula_idx]
        active_mask = state.active_pointers[formula_idx] # (M,) bool
        is_false = ~jnp.any(active_mask) & ~is_already_true
        
        avoid_props = state.to_avoid[formula_idx]
        prog_props = state.to_progress[formula_idx]
        formula_depth = state.depths[formula_idx]
    
        # --- 1. Handle simple TRUE/FALSE cases ---
        def case_true():
            # *** MODIFIED ***
            # Do not add a node. Return null index.
            return builder_state, jnp.array(-1)
    
        def case_false():
            # *** MODIFIED ***
            # Do not add a node. Return null index.
            return builder_state, jnp.array(-1)
    
        # --- 2. Handle active (OR) case ---
        def case_active():
            # Use lax.scan to iterate over all possible start_depths (0..M-1).
            def scan_body(carry_state, m):
                b_state = carry_state
                is_active = active_mask[m] & (m < formula_depth)
                
                def build_it():
                    return self._build_subformula_until(
                        b_state, avoid_props, prog_props, formula_depth, m
                    )
                
                def dont_build_it():
                    return b_state, jnp.array(-1)
    
                b_state, root_idx = lax.cond(
                   is_active,
                    build_it,
                    dont_build_it
                )
                
                final_root_idx = jnp.where(is_active, root_idx, -1)
                return b_state, final_root_idx
    
            # Run the scan.
            final_b_state, all_leaf_indices = lax.scan(
                scan_body,
                builder_state,
                jnp.arange(M)
            )
            
            # Create an 'OR' tree from the resulting leaf indices.
            active_leaf_mask = (all_leaf_indices != -1)
            
            # This will hit case_one() or case_many().
            # It will NOT hit case_zero() because is_false check
            # in the outer switch already handled the 0-active-pointer case.
            return self._build_binary_tree(
                final_b_state,
                all_leaf_indices,
                active_leaf_mask,
                self.LTL_BASE_VOCAB["or"]
            )
    
        # --- 3. Main Switch ---
        branch_index = jnp.where(is_already_true, 0, jnp.where(is_false, 1, 2))
        
        return lax.switch(
            branch_index,
            [case_true, case_false, case_active]
        )
    
    
    @partial(jax.jit, static_argnames=['self'])
    def build_ast(self, state: ConjunctionState) -> OrderedDict:
        """
        JIT-compilable function to convert a ConjunctionState into a binary AST graph.
        
        The graph is returned in a JAX-friendly OrderedDict (GraphTuple) format
        with statically-sized buffers.
        
        The last feature of the node tensor is 'is_root'.
        
        *** MODIFIED ***
        This function no longer creates nodes for 'True' or 'False'.
        If the entire formula resolves to True/False, the graph will be empty
        (n_node=1 for padding) and no node will be marked as root.
        """
        
        N, M = state.active_pointers.shape
        
        # --- 1. Define Conservative Static Buffer Sizes ---
        MAX_NODES_PER_UNTIL_TREE = 5 * M - 1 
        MAX_NODES_PER_FORMULA = (M - 1) + (M * MAX_NODES_PER_UNTIL_TREE) + 1
        MAX_NODES = 1 + (N - 1) + (N * MAX_NODES_PER_FORMULA)
        MAX_EDGES = MAX_NODES * 3
    
        # --- 2. Initialize Buffers ---
        nodes = jnp.zeros((MAX_NODES, self.feature_size), dtype=jnp.float32)
        senders = jnp.full((MAX_EDGES,), 0, dtype=jnp.int32)
        receivers = jnp.full((MAX_EDGES,), 0, dtype=jnp.int32)
        edge_types = jnp.full((MAX_EDGES,),self.EDGE_TYPES["self"], dtype=jnp.int32)
    
        builder_state = BuilderState(
            node_idx=jnp.array(1),
            edge_idx=jnp.array(0),
            nodes=nodes,
            senders=senders,
            receivers=receivers,
            edge_types=edge_types
        )
        
    
        # --- 4. Build the 'AND' tree of all N formulas ---
        def global_and_scan_body(carry_state, n):
            b_state = carry_state
            
            # This now returns -1 for True/False formulas
            b_state, formula_root_idx = self._build_formula_or_tree(b_state, n, state)
            
            return b_state, formula_root_idx
    
        final_b_state, all_formula_leaf_indices = lax.scan(
            global_and_scan_body,
            builder_state,
            jnp.arange(N)
        )
    
        # --- 5. Build the final 'AND' tree from the formula roots ---
        
        # *** MODIFIED ***
        # The mask is now based on which indices are *not* null (-1).
        valid_leaf_mask = (all_formula_leaf_indices != -1)
        
        # This builds the final 'AND' tree.
        # If valid_leaf_mask is all False, this returns global_root_idx = -1
        final_b_state, global_root_idx = self._build_binary_tree(
            final_b_state,
            all_formula_leaf_indices,
            valid_leaf_mask, # The modified mask
            self.LTL_BASE_VOCAB["and"]
        )
        
        # --- 6. Set the 'is_root' flag on the global root node ---
        
        # *** MODIFIED ***
        # Only set the 'is_root' flag if the global_root_idx is valid (not -1).
        final_nodes = lax.cond(
            global_root_idx != -1,
            lambda: final_b_state.nodes.at[global_root_idx, -1].set(1.0),
            lambda: final_b_state.nodes
        )
        
        # --- 7. Finalize and Return GraphTuple ---
        final_n_node = final_b_state.node_idx
        final_n_edge = final_b_state.edge_idx
        
        return OrderedDict([
            ('nodes', final_nodes), # Use the conditionally updated nodes
            ('senders', final_b_state.senders),
            ('receivers', final_b_state.receivers),
            ('n_node', final_n_node.reshape(1)),
            ('n_edge', final_n_edge.reshape(1)),
            ('edge_types', final_b_state.edge_types)
        ])

    
    
    
    ##################################### DECODE #############################################################
    

    def decode_formula(self, 
        state: ConjunctionState, 
    ) -> Any:
        """
        Decodes a JAX ConjunctionState back into a human-readable Python tuple.
        
        This represents the *current* state, handling 'ORs' from active pointers.
        """
        
        def _build_tree(op: str, clauses: List[Any]) -> Any:
            """Recursively builds a nested 'and' or 'or' tuple tree."""
            if not clauses:
                return 'True' if op == 'and' else 'False'
            if len(clauses) == 1:
                return clauses[0]
            return (op, clauses[0], _build_tree(op, clauses[1:]))
       
        def _reconstruct_single_formula(
            avoid_arr: jnp.ndarray, 
            prog_arr: jnp.ndarray, 
            depth: int, 
        ) -> Any:
            """Helper to rebuild the *original* nested tuple from JAX arrays."""
            avoid_arr=np.array(avoid_arr)
            prog_arr=np.array(prog_arr)
            # Start from the innermost formula
            # !avoid[depth-1] U prog[depth-1]
            curr = ('until', 
                    ('not', self.propositions[avoid_arr[depth-1]]), 
                    self.propositions[prog_arr[depth-1]])
            
            # Wrap it outwards
            for i in range(depth - 2, -1, -1):
                # !(avoid[i]) U (prog[i] & curr)
                curr = ('until',
                        ('not', self.propositions[avoid_arr[i]]),
                        ('and', self.propositions[prog_arr[i]], curr))
            return curr
        
        num_conjunctions = state.depths.shape[0]
        final_conjunctions = []
    
        for n in range(num_conjunctions):
            if state.already_true[n]:
    
                continue
                
            depth = int(state.depths[n])
            
            # Reconstruct the original full formula for this index
            original_formula = _reconstruct_single_formula(
                state.to_avoid[n], state.to_progress[n], depth,
            )
            
            # Find which sub-formulas are active
            active_indices = jnp.where(state.active_pointers[n, :depth])[0]
            
            if active_indices.shape[0] == 0:
                return 'False'
    
            # Create an 'OR' list of all active sub-formulas
            or_clauses = []
            for start_depth in active_indices.tolist():
                # "Drill down" to the correct sub-formula
                sub_formula = original_formula
                for _ in range(start_depth):
                    # ('until', A, ('and', B, Rest)) -> get Rest
                    sub_formula = sub_formula[2][2]
                or_clauses.append(sub_formula)
                
            final_conjunctions.append(_build_tree('or', or_clauses))
            
        # Combine all conjunctions with 'AND'
        return ltl_tuple_to_string(_build_tree('and', final_conjunctions))
    
    
#########################     ENCODE ########################################################################################
    
  
    
    def encode_formula(self, 
        ltl_tuple: Union[tuple, str],
    ) -> ConjunctionState:
        """
        Encodes a Python tuple representation of LTL formulas into a JAX ConjunctionState.
        
        Handles:
        - 'True', 'False'
        - A single 'until' formula (fresh)
        - A single 'or' formula (progressed state)
        - An 'and' of the above
        """
       
        def _parse_single_until(
            formula_tuple: tuple,  
        ) -> Tuple[ jnp.ndarray, jnp.ndarray, int]:
            """Parses one !(a) U (b & !(c) U d) formula into JAX arrays."""
            to_avoid_list = []
            to_progress_list = []
            curr = formula_tuple
            
            depth = 0
            while True:
                depth += 1
                # ('until', ('not', A), B)
                _, not_a, b_and_rest = curr
                
                # A = ('not', prop)
                avoid_prop = not_a[1]
                to_avoid_list.append(self.prop_map[avoid_prop])
                
                # B = ('and', prop, rest_formula)
                if isinstance(b_and_rest, tuple) and b_and_rest[0] == 'and':
                    _, progress_prop, rest_formula = b_and_rest
                    to_progress_list.append(self.prop_map[progress_prop])
                    curr = rest_formula
                # B = prop (base case)
                else:
                    progress_prop = b_and_rest
                    to_progress_list.append(self.prop_map[progress_prop])
                    break # Reached the end of the nesting
        
            # --- Padding ---
            # We use -1 for padding, assuming prop indices are >= 0
            pad_len = self.max_depth - depth
            to_avoid = jnp.array(to_avoid_list + [-1] * pad_len, dtype=jnp.int32)
            to_progress = jnp.array(to_progress_list + [-1] * pad_len, dtype=jnp.int32)
            
            # Initial state: only the outermost formula (index 0) is active
            active_pointers = jnp.zeros(self.max_depth, dtype=bool).at[0].set(True)
            
            return active_pointers, to_avoid, to_progress, depth
        def _parse_fresh_until(
            formula_tuple: tuple, 
        ) -> Tuple[ jnp.ndarray, jnp.ndarray, int]:
            """
            Parses one *fresh* !(a) U (b & ...) formula into JAX arrays.
            This is the original parsing logic, used as a helper.
            """
            to_avoid_list = []
            to_progress_list = []
            curr = formula_tuple
            
            depth = 0
            while True:
                depth += 1
                _, not_a, b_and_rest = curr
                avoid_prop = not_a[1]
                to_avoid_list.append(self.prop_map[avoid_prop])
                
                if isinstance(b_and_rest, tuple) and b_and_rest[0] == 'and':
                    _, progress_prop, rest_formula = b_and_rest
                    to_progress_list.append(self.prop_map[progress_prop])
                    curr = rest_formula
                else:
                    progress_prop = b_and_rest
                    to_progress_list.append(self.prop_map[progress_prop])
                    break 
        
            pad_len = self.max_depth - depth
            to_avoid = jnp.array(to_avoid_list + [-1] * pad_len, dtype=jnp.int32)
            to_progress = jnp.array(to_progress_list + [-1] * pad_len, dtype=jnp.int32)
            active_pointers = jnp.zeros(self.max_depth, dtype=bool).at[0].set(True)
            
            return active_pointers, to_avoid, to_progress, depth
        
        def _find_subformula_depth(full_formula: tuple, sub_formula: tuple) -> int:
            """Finds the depth of sub_formula within full_formula. (0-indexed)"""
            if str(full_formula) == str(sub_formula):
                return 0
            
            if (isinstance(full_formula, tuple) and 
                full_formula[0] == 'until' and len(full_formula) == 3 and
                isinstance(full_formula[2], tuple) and
                full_formula[2][0] == 'and' and len(full_formula[2]) == 3):
                
                rest_formula = full_formula[2][2]
                depth = _find_subformula_depth(rest_formula, sub_formula)
                if depth != -1:
                    return depth + 1
                    
            return -1 # Not found
        
        def _parse_formula_state(
            formula_tuple: Union[tuple, str], 
        ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, int, bool]:
            """
            Parses a single formula, which can be 'True', 'False', 
            a fresh 'until' tuple, or an 'or' tuple representing a progressed state.
            
            Returns: (active_pointers, to_avoid, to_progress, depth, already_true)
            """
            
            # --- Handle resolved states ---
            if formula_tuple == 'True':
                return (jnp.zeros(self.max_depth, dtype=bool), 
                        jnp.full(self.max_depth, -1, dtype=jnp.int32), 
                        jnp.full(self.max_depth, -1, dtype=jnp.int32), 
                        1, True) # depth=1 (dummy)
                
            if formula_tuple == 'False':
                return (jnp.zeros(self.max_depth, dtype=bool), 
                        jnp.full(self.max_depth, -1, dtype=jnp.int32), 
                        jnp.full(self.max_depth, -1, dtype=jnp.int32), 
                        1, False) # depth=1 (dummy)
        
            # --- 1. Find all 'until' clauses ---
            or_clauses = []
            if formula_tuple[0] == 'or':
                queue = [formula_tuple]
                while queue:
                    item = queue.pop()
                    if isinstance(item, tuple) and item[0] == 'or':
                        queue.append(item[1])
                        queue.append(item[2])
                    elif isinstance(item, tuple) and item[0] == 'until':
                        or_clauses.append(item)
            elif formula_tuple[0] == 'until':
                or_clauses = [formula_tuple]
            else:
                 raise ValueError(f"Cannot parse formula state: {formula_tuple}")
        
            if not or_clauses: # e.g., an 'or' tree that resolved to 'False'
                return (jnp.zeros(self.max_depth, dtype=bool), 
                        jnp.full(self.max_depth, -1, dtype=jnp.int32), 
                        jnp.full(self.max_depth, -1, dtype=jnp.int32), 
                        1, False)
        
            # --- 2. Find the "base" formula (the one that contains all others) ---
            parsed_formula_tuples = [f for f in or_clauses]
            base_formula_tuple = None
            for f_i in parsed_formula_tuples:
                is_base = True
                for f_j in parsed_formula_tuples:
                    if _find_subformula_depth(f_i, f_j) == -1:
                        is_base = False # f_i does not contain f_j
                        break
                if is_base:
                    base_formula_tuple = f_i
                    break
                    
            if base_formula_tuple is None:
                raise ValueError("Cannot encode 'or' of disjoint formulas. "
                                 "The 'or' must represent a single formula's progression.")
        
            # --- 3. Parse the base formula to get canonical arrays/depth ---
            _, to_avoid, to_progress, depth = _parse_fresh_until(
                base_formula_tuple, self.prop_map, self.max_depth
            )
        
            # --- 4. Set active pointers based on 'or' clauses ---
            active_pointers = jnp.zeros(self.max_depth, dtype=bool)
            for sub_tuple in parsed_formula_tuples:
                relative_depth = _find_subformula_depth(base_formula_tuple, sub_tuple)
                active_pointers = active_pointers.at[relative_depth].set(True)
        
            return active_pointers, to_avoid, to_progress, depth, False 
    
        
       
        
        
        # 1. Collect all formulas from the 'and' tree
        formulas_to_parse = []
        if not isinstance(ltl_tuple, tuple): # 'True' or 'False'
            formulas_to_parse = [ltl_tuple]
        elif ltl_tuple[0] == 'and':
            queue = [ltl_tuple]
            while queue:
                item = queue.pop()
                if isinstance(item, tuple) and item[0] == 'and':
                    queue.append(item[1])
                    queue.append(item[2])
                else: # 'until', 'or', 'True', 'False'
                    formulas_to_parse.append(item)
        else: # A single 'until' or 'or' formula
            formulas_to_parse = [ltl_tuple]
    
        num_conjunctions = len(formulas_to_parse)
        if num_conjunctions > self.max_conjunctions:
            raise ValueError(f"Found {num_conjunctions} formulas, but self.max_conjunctions={self.max_conjunctions}")
    
        # 2. Parse each formula state
        all_actives, all_avoids, all_progress, all_depths, all_already_true = [], [], [], [], []
        
        for f_tuple in formulas_to_parse:
            act, avoid, prog, depth, already_true = _parse_formula_state(
                f_tuple, self.prop_map, self.max_depth
            )
            all_actives.append(act)
            all_avoids.append(avoid)
            all_progress.append(prog)
            all_depths.append(depth)
            all_already_true.append(already_true)
        
        # 3. Pad the *batch* of formulas up to self.max_conjunctions
        pad_n = self.max_conjunctions - num_conjunctions
        all_already_true.extend([True] * pad_n) # Padded tasks are "already true"
    
        pad_active = jnp.zeros(self.max_depth, dtype=bool)
        pad_avoid = jnp.full(self.max_depth, -1, dtype=jnp.int32)
        pad_progress = jnp.full(self.max_depth, -1, dtype=jnp.int32)
        pad_depth = 1 # Dummy depth
        
        all_actives.extend([pad_active] * pad_n)
        all_avoids.extend([pad_avoid] * pad_n)
        all_progress.extend([pad_progress] * pad_n)
        all_depths.extend([pad_depth] * pad_n)
    
        # 4. Stack into a single ConjunctionState
        return ConjunctionState(
            active_pointers=jnp.stack(all_actives),
            to_avoid=jnp.stack(all_avoids),
            to_progress=jnp.stack(all_progress),
            depths=jnp.array(all_depths, dtype=jnp.int32),
            already_true=jnp.array(all_already_true, dtype=bool)
        )

#############################################  VISUALIZE AST    ###########################################

    def visualize_ast(self, ast_data: OrderedDict, img_size=(1290, 1080)):
        """
        Visualizes an LTL AST graph from a data dictionary using
        Networkx and Matplotlib.

        Args:
            ast_data (OrderedDict): An ordered dictionary containing the graph data.
            img_size (tuple): The target (width, height) in pixels for the
                              output image.

        Returns:
            PIL.Image.Image: A PIL Image object of the rendered graph, or None
                             if plotting fails.
        """
        
        try:
            # 1. Extract data
            nodes = ast_data['nodes']
            senders = ast_data['senders']
            receivers = ast_data['receivers']
            
            # <-- MODIFIED: Use .item() to get Python scalars from JAX arrays
            # This is safer and more direct than .reshape(1)[0]
            n_node = ast_data['n_node'].item()
            n_edge = ast_data['n_edge'].item()
            
            edge_types = ast_data['edge_types']
            
            # 2. Slice data
            nodes = nodes[:n_node]
            senders = senders[:n_edge]
            receivers = receivers[:n_edge]
            edge_types = edge_types[:n_edge]

        except KeyError as e:
            print(f"Error: Missing key in ast_data: {e}", file=sys.stderr)
            return None
        except Exception as e:
            print(f"Error extracting data: {e}", file=sys.stderr)
            return None

        # 3. Build the Networkx Graph
        G = nx.DiGraph()
        node_labels = {}
        node_colors = []
        edge_labels = {}
        
        # 4. Add all nodes
        for i in range(n_node):
            node_id = str(i) # Use string IDs for consistency
            node_features = nodes[i]
            
            try:
                # <-- MODIFIED: .item() converts unhashable JAX scalar to Python int
                vocab_index = np.argmax(node_features[:-1]).item() 
                label = self.INV_LTL_BASE_VOCAB.get(vocab_index, f"UNK_{vocab_index}")
            except Exception as e:
                label = f"Error: {e}"

            # <-- MODIFIED: .item() converts JAX bool to Python bool
            is_root = (node_features[-1] == 1).item() 
            
            G.add_node(node_id)
            
            if is_root:
                node_colors.append('lightgreen')
                node_labels[node_id] = f"{label}\n(ROOT)"
            else:
                node_colors.append('lightblue')
                node_labels[node_id] = label

        # 5. Add all edges
        for j in range(n_edge):
            # Use .item() to convert numpy types to standard python types
            sender_id = str(senders[j].item()) 
            receiver_id = str(receivers[j].item())
            
            edge_type_index = edge_types[j]
            if hasattr(edge_type_index, 'item'):
                edge_type_index = edge_type_index.item()
                
            edge_label = self.INV_EDGE_TYPES.get(edge_type_index, f"UNK_{edge_type_index}")
            
            # Skip "self" loops
            if edge_label == "self":
                continue
                
            G.add_edge(sender_id, receiver_id)
            edge_labels[(sender_id, receiver_id)] = edge_label

        # 6. Plot with Matplotlib
        dpi = 100
        figsize = (img_size[0] / dpi, img_size[1] / dpi)
        
        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
        
        try:
            # 7. Get layout
            try:
                pos = nx.drawing.nx_pydot.graphviz_layout(G, prog='dot')
            except ImportError:
                print("Warning: pydot not found. Using spring_layout.", file=sys.stderr)
                print("Install pydot for a hierarchical tree layout.", file=sys.stderr)
                pos = nx.spring_layout(G, seed=42)
            
            # 8. Draw graph components
            nx.draw_networkx_nodes(
                G, pos, ax=ax, node_color=node_colors, 
                node_size=50, node_shape='s'
            )
            nx.draw_networkx_edges(
                G, pos, ax=ax, arrowstyle='->', arrowsize=20, 
                node_size=50, connectionstyle='arc3,rad=0.1'
            )
            nx.draw_networkx_labels(
                G, pos, ax=ax, labels=node_labels, font_size=9
            )
            nx.draw_networkx_edge_labels(
                G, pos, ax=ax, edge_labels=edge_labels, font_size=7
            )
            
            ax.axis('off')
            fig.tight_layout(pad=0)
            
            # 9. Save plot to in-memory buffer
            buf = io.BytesIO()
            fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)
            buf.seek(0)
            
            # 10. Open as PIL Image and resize
            img = Image.open(buf)
            img = img.resize(img_size, Image.Resampling.LANCZOS)
            
            return img

        except Exception as e:
            print(f"Error during Networkx/Matplotlib plotting: {e}", file=sys.stderr)
            return None
        finally:
            plt.close(fig)



###################################  EVENTUALLY    ########################################################



import logging
import random as py_random
import jax
import jax.numpy as jnp
from jax import random
import numpy as np
from functools import partial
from jax import lax
from jax.random import PRNGKey
from collections import OrderedDict
from typing import NamedTuple, Tuple, List, Dict, Any, Union
from functools import partial
import numpy as np
import matplotlib.pyplot as plt
#import networkx as nx
import re
import matplotlib.pyplot as plt
#import graphviz
#from PIL import Image, ImageDraw, ImageFont
import sys
import os
#import io



class AstBuildState(NamedTuple):
    nodes: jnp.ndarray       # (MAX_NODES, feature_size)
    senders: jnp.ndarray     # (MAX_EDGES,)
    receivers: jnp.ndarray   # (MAX_EDGES,)
    edge_types: jnp.ndarray  # (MAX_EDGES,)
    node_idx: jnp.ndarray    # Scalar array, (1,)
    edge_idx: jnp.ndarray    # Scalar array, (1,)


class JaxEventuallyTaskSampler:
    """
    JAX-based sampler for LTL 'Eventually' formulas.

    This class stores the static sampling configuration and provides a
    JIT-compiled `sample` method to generate formula matrices.

    The formula structure is a conjunction of 'Eventually' clauses,
    where each claue can be nested and include disjunctions.
    """

    def __init__(
        self,
        propositions: List,
        min_levels: int,
        max_levels: int,
        min_conjunctions: int,
        max_conjunctions: int,
        disjunction_prob: float = 0.25
    ):
        """
        Initializes the sampler with static configuration.

        Args:
            num_propositions: Number of available atomic propositions (p1, p2...).
            min_levels: Minimum nesting depth (F) for a conjunct.
            max_levels: Maximum nesting depth (F) for a conjunct.
            min_conjunctions: Minimum number of conjuncts (G(...)) in the formula.
            max_conjunctions: Maximum number of conjuncts (G(...)) in the formula.
            max_depth: Static max depth for the formula matrix (for JIT).
            max_conjuncts: Static max conjuncts for the formula matrix (for JIT).
            disjunction_prob: Probability of a clause being a disjunction F(p1 | p2).
        """
        # Store all configuration parameters as instance attributes
        self.propositions = sorted(list(set(propositions)))
        self.min_levels = min_levels
        self.max_levels = max_levels
        self.min_conjunctions = min_conjunctions
        self.max_conjunctions = max_conjunctions
        self.max_depth = max_levels
        self.max_conjuncts = max_conjunctions
        self.disjunction_prob = disjunction_prob
        self.EDGE_TYPES = {
        "self": 0,
        "arg": 1,   # For unary operators
        "arg1": 2,  # For binary operator, arg 1
        "arg2": 3,  # For binary operator, arg 2
         }
        self.INV_EDGE_TYPES = {v: k for k, v in self.EDGE_TYPES.items()}
        self.LTL_BASE_VOCAB = {
            "and": 0, "or": 1, "not": 2, "next": 3, "until": 4,
            "always": 5, "eventually": 6, "True": 7, "False": 8,
        }
        
        self.PROPS_OFFSET = len(self.LTL_BASE_VOCAB)
        for i, el in enumerate(self.propositions):
            self.LTL_BASE_VOCAB[el] = self.PROPS_OFFSET + i
        self.num_propositions=len(self.propositions)
        self.INV_LTL_BASE_VOCAB = {v: k for k, v in self.LTL_BASE_VOCAB.items()}
        self.PROP_TO_INDEX = {prop: i+1 for i, prop in enumerate(self.propositions)}

        self.vocab_size = len(self.LTL_BASE_VOCAB)
        self.feature_size = self.vocab_size + 1 # One-hot + is_root
        
        D = self.max_depth + 1
        K = self.max_conjuncts + 1
        
        # Max nodes: K * (5*D + 1) covers subtrees + linkers + 'True' node
        self.MAX_NODES = K * (5 * D + 1) 
        
        # Max edges: Each node has 1 'self' edge and at most 2 child edges.
        self.MAX_EDGES = self.MAX_NODES * 3
        
        # Store token IDs for JAX functions
        self.TOKEN_ID_AND = jnp.array(self.LTL_BASE_VOCAB['and'])
        self.TOKEN_ID_OR = jnp.array(self.LTL_BASE_VOCAB['or'])
        self.TOKEN_ID_F = jnp.array(self.LTL_BASE_VOCAB['eventually'])
        self.TOKEN_ID_TRUE = jnp.array(self.LTL_BASE_VOCAB['True'])
        
   
    def _sample_sequence_jax(self, key, num_levels, max_depth, num_propositions, disjunction_prob):
        """
        JAX-jittable function to sample a single conjunct sequence using lax.scan.
        This function is vmapped in the main sampler.
        """
        
        def scan_body(carry, d):
            """
            Body of the scan function, samples one level (depth).
            'd' is the current depth index (from 0 to max_depth-1).
            Uses rejection sampling to ensure new props are not in 'last'.
            """
            key, last_p1, last_p2 = carry
            key, k_p1_loop, k_p2_loop, k_disj = random.split(key, 4)
    
            # 1. Sample p1, ensuring it's not the same as last_p1 or last_p2
            def p1_loop_cond(val):
                # Condition to *continue* looping
                k, p1 = val
                return (p1 == last_p1) | (p1 == last_p2)
    
            def p1_loop_body(val):
                # Sample a new p1
                k, p1_old = val
                k, subkey = random.split(k)
                p1_new = random.randint(subkey, (), 1, num_propositions + 1)
                return k, p1_new
    
            # Initialize with dummy value 0 (which will trigger loop if last_p1/p2 is 0)
            # or a valid prop. Rejection sampling handles all cases.
            k_p1_loop, p1_init = p1_loop_body((k_p1_loop, last_p1)) 
            _, p1 = jax.lax.while_loop(p1_loop_cond, p1_loop_body, (k_p1_loop, p1_init))
    
    
            # 2. Sample if this is a disjunction
            is_disj = random.bernoulli(k_disj, disjunction_prob)
    
            # 3. Sample p2, ensuring it's not p1, last_p1, or last_p2
            def p2_loop_cond(val):
                # Condition to *continue* looping
                k, p2 = val
                return (p2 == p1) | (p2 == last_p1) | (p2 == last_p2)
    
            def p2_loop_body(val):
                # Sample a new p2
                k, p2_old = val
                k, subkey = random.split(k)
                p2_new = random.randint(subkey, (), 1, num_propositions + 1)
                return k, p2_new
    
            # Initialize with dummy value
            k_p2_loop, p2_init = p2_loop_body((k_p2_loop, p1))
            _, p2 = jax.lax.while_loop(p2_loop_cond, p2_loop_body, (k_p2_loop, p2_init))
            
            # 4. Apply masks
            # Is this depth level active for this conjunct?
            is_active = d < num_levels
            
            final_p1 = jnp.where(is_active, p1, 0)
            final_p2 = jnp.where(is_active & is_disj, p2, 0)
            
            new_carry = (key, final_p1, final_p2) # Pass new "last" props
            output = (final_p1, final_p2)       # The matrix row for this depth
            return new_carry, output
    
        init_carry = (key, 0, 0) # (key, last_p1, last_p2)
        depth_indices = jnp.arange(max_depth)
        
        # Run the scan over all depths
        (final_key, _, _), (p1s, p2s) = jax.lax.scan(scan_body, init_carry, depth_indices)
        
        # p1s and p2s have shape (max_depth,)
        return jnp.stack([p1s, p2s], axis=0) # Shape (2, max_depth)
    
    
    @partial(jax.jit, static_argnames=['self'])
    def sample(self,
        key,
    ):
        """
        JAX-jittable sampler for the LTL formula matrix.
        """
        key, k_conj, k_levels, k_seqs = random.split(key, 4)
    
        # 1. Sample number of conjuncts
        num_conjs = random.randint(k_conj, (), self.min_conjunctions, self.max_conjunctions + 1)
        
        # 2. Create mask for active conjuncts
        conj_mask = jnp.arange(self.max_conjuncts) < num_conjs
        
        # 3. Sample number of levels (depth) for each *potential* conjunct
        levels = random.randint(k_levels, (self.max_conjuncts,), self.min_levels, self.max_levels + 1)
        
        # 4. Sample all sequences in parallel using vmap
        seq_keys = random.split(k_seqs, self.max_conjuncts)
        
        # vmap the sequence sampler
        # in_axes = (key, num_levels, max_depth, num_propositions, disjunction_prob)
        vmap_sampler = jax.vmap(
            self._sample_sequence_jax, 
            in_axes=(0, 0, None, None, None),
            out_axes=2 # Output shape (2, max_depth, max_conjuncts)
        )
        
        formula_matrix = vmap_sampler(
            seq_keys, levels, self.max_depth, self.num_propositions, self.disjunction_prob
        )
        
        # 5. Apply the conjunct mask
        final_matrix = formula_matrix * conj_mask[None, None, :]
        
        return final_matrix, num_conjs, jnp.average(levels)


    ###########################    PROGRESS     ########################################################

    @jax.jit
    def _check_proposition_implication(p1_i, p2_i, p1_j, p2_j):
        """
        Checks if (p1_i | p2_i) => (p1_j | p2_j).
        
        This is true if the set of propositions {p1_i, p2_i} (ignoring 0s)
        is a subset of {p1_j, p2_j} (ignoring 0s).
        """
        
        # Check if p1_i is in the j-set {p1_j, p2_j}
        # p1_i is "ok" if it's 0 (inactive) or if it's present in j.
        p1_i_in_j = (p1_i == p1_j) | (p1_i == p2_j)
        p1_i_ok = (p1_i == 0) | p1_i_in_j
    
        # Check if p2_i is in the j-set {p1_j, p2_j}
        # p2_i is "ok" if it's 0 (inactive) or if it's present in j.
        p2_i_in_j = (p2_i == p1_j) | (p2_i == p2_j)
        p2_i_ok = (p2_i == 0) | p2_i_in_j
    
        # Both must be satisfied for the subset relationship to hold.
        return p1_i_ok & p2_i_ok
    
    
    @partial(jax.jit, static_argnames=['max_depth'])
    def _check_conjunct_implication(conj_i, conj_j, max_depth):
        """
        Checks if conjunct C_i (implier) implies C_j (implied).
        
        This uses a 'subset-subsequence' check.
        
        Args:
            conj_i (jnp.ndarray): Shape (2, max_depth), the implier conjunct.
            conj_j (jnp.ndarray): Shape (2, max_depth), the implied conjunct.
            max_depth (int): Static argument for loop bounds.
            
        Returns:
            bool: True if C_i implies C_j.
        """
    
        # We scan over the depths of C_j (the "implied" sequence).
        # The carry `i_idx_carry` is the depth we are searching *from* in C_i.
        
        def scan_body(i_idx_carry, j_depth):
            # Get the propositions for the current depth of C_j
            p1_j = conj_j[0, j_depth]
            p2_j = conj_j[1, j_depth]
            
            # Is this depth of C_j active? (i.e., p1_j > 0)
            is_j_active = p1_j > 0
            
            # --- Inner loop: Find a matching i-depth ---
            # We need to find the *first* depth k >= i_idx_carry in C_i
            # such that C_i[k] is active and implies C_j[j_depth].
            
            def find_i_loop_cond(val):
                # val is (k, found_match)
                k, found_match = val
                # Continue looping if we're still within bounds and haven't found a match
                return (k < max_depth) & ~found_match
    
            def find_i_loop_body(val):
                k, found_match = val
                p1_i = conj_i[0, k]
                p2_i = conj_i[1, k]
                
                # Is this depth of C_i active?
                is_i_active = p1_i > 0
                
                # Check for proposition-level implication
                i_implies_j = _check_proposition_implication(p1_i, p2_i, p1_j, p2_j)
                
                # We have a match if C_i[k] is active AND it implies C_j[j_depth]
                is_match = is_i_active & i_implies_j
                
                # Return the next k and whether we found a match
                return (k + 1, is_match)
    
            # Start the inner-loop search from `i_idx_carry`
            initial_val = (i_idx_carry, jnp.array(False))
            
            # `final_k` will be the (k_of_match + 1) or max_depth
            # `final_found` will be True if a match was found
            final_k, final_found = jax.lax.while_loop(
                find_i_loop_cond, 
                find_i_loop_body, 
                initial_val
            )
            
            # This j_depth is satisfied if:
            # 1. It was not active in the first place (~is_j_active)
            # 2. It was active, and we found a matching i_depth (final_found)
            j_level_satisfied = ~is_j_active | final_found
            
            # The new carry for the *next* scan iteration is `final_k`.
            # This ensures our subsequence search in C_i only moves forward.
            # The output for this scan step is whether this j_level was satisfied.
            return final_k, j_level_satisfied
    
        # Initial carry: Start searching C_i from depth 0
        init_carry = 0 
        j_depths = jnp.arange(max_depth)
        
        # Run the scan over all of C_j's depths
        (final_i_idx, all_j_levels_satisfied) = jax.lax.scan(
            scan_body, init_carry, j_depths
        )
        
        # The implication holds only if *all* of C_j's depths were satisfied.
        return jnp.all(all_j_levels_satisfied)
    
    
    # --- Main Simplification Function ---
    
    @partial(jax.jit, static_argnames=['max_depth', 'max_conjuncts'])
    def simplify_conjuncts(formula_matrix, max_depth, max_conjuncts):
        """
        Simplifies the formula matrix by removing redundant conjuncts.
        
        A conjunct C_j is removed if any *other* active conjunct C_i implies it.
        
        Args:
            formula_matrix (jnp.ndarray): Shape (2, max_depth, max_conjuncts).
            max_depth (int): Static argument.
            max_conjuncts (int): Static argument.
            
        Returns:
            jnp.ndarray: A new formula matrix with redundant conjuncts zeroed out.
        """
        
        # --- 1. Vmap the conjunct implication check ---
        
        # We want to run _check_conjunct_implication for all pairs (i, j).
        # We will vmap over the *last* axis (axis=2) of two matrices.
        vmapped_implies = jax.vmap(
            _check_conjunct_implication, 
            in_axes=(2, 2, None), # (conj_i, conj_j, max_depth)
            out_axes=0
        )
        
        # --- 2. Create all-pairs (i, j) ---
        
        # We need to compare every conjunct with every other conjunct.
        i_indices = jnp.arange(max_conjuncts)
        j_indices = jnp.arange(max_conjuncts)
        
        # Create all (i, j) pairs
        # i_pairs = [0, 0, ..., 1, 1, ..., N, N, ...]
        i_pairs = jnp.repeat(i_indices, max_conjuncts)
        # j_pairs = [0, 1, ..., N, 0, 1, ..., N, ...]
        j_pairs = jnp.tile(j_indices, max_conjuncts)
    
        # Get the data for all pairs.
        # all_conj_i[..., k] will be the i-conjunct for the k-th pair.
        # all_conj_j[..., k] will be the j-conjunct for the k-th pair.
        # Shape of both is (2, max_depth, max_conjuncts * max_conjuncts)
        all_conj_i = formula_matrix[:, :, i_pairs]
        all_conj_j = formula_matrix[:, :, j_pairs]
    
        # --- 3. Run the implication check for all pairs ---
        
        # This is a 1D array of shape (max_conjuncts * max_conjuncts,)
        implication_flat = vmapped_implies(all_conj_i, all_conj_j, max_depth)
        
        # Reshape to a 2D matrix: (i, j)
        # implication_matrix[i, j] is True if C_i implies C_j
        implication_matrix = implication_flat.reshape(max_conjuncts, max_conjuncts)
        
        # --- 4. Identify conjuncts to remove ---
        
        # A conjunct `j` is active if its p1 at depth 0 is > 0
        is_active = formula_matrix[0, 0, :] > 0
        
        # Broadcast active masks for i and j
        is_i_active = is_active[:, None]  # Shape (max_conjuncts, 1)
        is_j_active = is_active[None, :]  # Shape (1, max_conjuncts)
    
        # A conjunct cannot imply itself for removal purposes
        i_not_eq_j = ~jnp.eye(max_conjuncts, dtype=bool)
        
        # `valid_implication[i, j]` is True if:
        # 1. i != j
        # 2. C_i is active
        # 3. C_j is active
        # 4. C_i implies C_j
        valid_implication = i_not_eq_j & is_i_active & is_j_active & implication_matrix
        
        # We want to remove any conjunct `j` (the "implied") if *any* *other*
        # conjunct `i` (the "implier") implies it.
        # We check for `any` along axis 0 (the `i` axis).
        # This gives a 1D mask, True for each `j` that should be removed.
        to_remove_mask_1d = jnp.any(valid_implication, axis=0) # Shape (max_conjuncts,)
        
        # --- 5. Create the new simplified matrix ---
        
        # We want a "keep" mask, which is the opposite
        keep_mask_1d = ~to_remove_mask_1d
        
        # Broadcast the 1D keep_mask to the 3D matrix shape
        keep_mask_broadcast = keep_mask_1d[None, None, :]
        
        # Zero out the conjuncts that are marked for removal
        simplified_matrix = jnp.where(
            keep_mask_broadcast,
            formula_matrix,
            0
        )
        
        return simplified_matrix
    
    @partial(jax.jit, static_argnames=['self'])
    def progress(self, formula_matrix, propositions_true):
        """
        Progresses the LTL formula matrix based on the propositions that are true.
    
        Args:
            formula_matrix (jnp.ndarray): The formula representation, shape (2, max_depth, max_conjuncts).
            propositions_true (jnp.ndarray): Boolean array of shape (num_propositions).
                                             propositions_true[i] is True if proposition i is true.
                                             propositions_true[0] is always False (padding).
    
        Returns:
            jnp.ndarray: The new formula_matrix after one step of progression.
        """
        padding = jnp.array([False])
        propositions_true = jnp.concatenate([padding, propositions_true])
        
        # Get the propositions at the current depth (depth 0) for all conjuncts
        p1s = formula_matrix[0, 0, :]  # Shape (max_conjuncts,)
        p2s = formula_matrix[1, 0, :]  # Shape (max_conjuncts,)
    
        # Check if the conditions are met
        # propositions_true[0] is False, so if p1s or p2s is 0, this will be False.
        p1s_met = propositions_true[p1s]
        p2s_met = propositions_true[p2s]
        
        # A condition is met if it's active (p1s > 0) and either p1 or p2 is true
        active_mask = p1s > 0
        condition_met_mask = active_mask & (p1s_met | p2s_met) # Shape (max_conjuncts,)
    
        # Create the "progressed" matrix (roll depth dimension up by 1)
        # This simulates F(p & F(...)) -> F(...)
        progressed_matrix = jnp.roll(formula_matrix, shift=-1, axis=1)
        # Zero out the last depth-level after rolling
        progressed_matrix = progressed_matrix.at[:, self.max_depth - 1, :].set(0)
    
        # If the condition was not met, the formula remains the same (retained_matrix)
        retained_matrix = formula_matrix
    
        # Use jnp.where to select between progressed and retained for each conjunct
        # We need to broadcast the 1D mask to the 3D matrix shape
        progress_mask_broadcast = jnp.broadcast_to(
            condition_met_mask[None, None, :], 
            formula_matrix.shape
        )
    
        new_formula_matrix = jnp.where(
            progress_mask_broadcast,
            progressed_matrix,
            retained_matrix
        )
        is_true = jnp.all(new_formula_matrix[0, 0, :] == 0)
        
        return new_formula_matrix, is_true, jnp.array(False)


    #########################   AST BUILDER ##########################################################




    @partial(jax.jit, static_argnames=['self'])
    def _one_hot_feat(self, token_id, is_root):
        """Helper to create a node feature vector."""
        token_vec = jax.nn.one_hot(token_id, self.vocab_size, dtype=jnp.float32)
        root_vec = jnp.array([is_root], dtype=jnp.float32)
        return jnp.concatenate([token_vec, root_vec])

    @partial(jax.jit, static_argnames=['self'])
    def _add_edge(self, state: AstBuildState, sender_idx, receiver_idx, edge_type):
        """Helper to add an edge to the graph state."""
        e_idx = state.edge_idx[0]
        new_senders = state.senders.at[e_idx].set(sender_idx)
        new_receivers = state.receivers.at[e_idx].set(receiver_idx)
        new_edge_types = state.edge_types.at[e_idx].set(edge_type)
        return state._replace(
            senders=new_senders,
            receivers=new_receivers,
            edge_types=new_edge_types,
            edge_idx=state.edge_idx + 1
        )

    @partial(jax.jit, static_argnames=['self'])
    def _add_node(self, state: AstBuildState, token_id, is_root=0.0):
        """Helper to add a node and its 'self' edge."""
        n_idx = state.node_idx[0]
        feat = self._one_hot_feat(token_id, is_root)
        new_nodes = state.nodes.at[n_idx].set(feat)
        
        new_state = state._replace(
            nodes=new_nodes,
            node_idx=state.node_idx + 1
        )
        # Add the 'self' edge
        new_state = self._add_edge(new_state, n_idx, n_idx, self.EDGE_TYPES["self"])
        return new_state, n_idx

    @partial(jax.jit, static_argnames=['self'])
    def _build_conjunct_scan_body(self, carry, x_slice):
        """
        lax.scan body to build one conjunct tree, iterating *backward* over depth.
        This function is called *inside* another scan.
        """
        state, child_root_idx = carry
        p1, p2 = x_slice # p1, p2 are prop_nums (1-indexed) or 0
        
        # --- 1. Check if this depth-level is active ---
        is_active = p1 > 0
        
        def build_nodes_fn(carry_in):
            """Function for lax.cond if p1 > 0."""
            state_in, child_root_idx_in = carry_in
            
            # --- 2. Build the 'term' subtree (p1 or (or p1 p2)) ---
            is_disj = p2 > 0
            
            def build_disj_term(state_t):
                # Create 'or', 'p1', 'p2' nodes
                state_t, or_idx = self._add_node(state_t, self.TOKEN_ID_OR)
                # Convert 1-indexed prop_num to 0-indexed vocab ID
                p1_token_id = p1 + self.PROPS_OFFSET - 1
                p2_token_id = p2 + self.PROPS_OFFSET - 1
                state_t, p1_idx = self._add_node(state_t, p1_token_id)
                state_t, p2_idx = self._add_node(state_t, p2_token_id)
                # Add edges
                state_t = self._add_edge(state_t, p1_idx, or_idx, self.EDGE_TYPES["arg1"])
                state_t = self._add_edge(state_t, p2_idx, or_idx, self.EDGE_TYPES["arg2"])
                return state_t, or_idx
                
            def build_atom_term(state_t):
                # Create 'p1' node
                p1_token_id = p1 + self.PROPS_OFFSET - 1
                state_t, p1_idx = self._add_node(state_t, p1_token_id)
                return state_t, p1_idx

            state_in, term_root_idx = jax.lax.cond(
                is_disj, build_disj_term, build_atom_term, state_in
            )

            # --- 3. Build the 'F' or 'F(&...)' structure ---
            is_innermost = child_root_idx_in < 0

            def build_innermost_f(state_f):
                # Structure: F -> term
                state_f, f_idx = self._add_node(state_f, self.TOKEN_ID_F)
                state_f = self._add_edge(state_f, term_root_idx, f_idx, self.EDGE_TYPES["arg"])
                return state_f, f_idx
            
            def build_outer_f_and(state_f):
                # Structure: F -> & -> term
                #                    -> child_root (from prev step)
                state_f, f_idx = self._add_node(state_f, self.TOKEN_ID_F)
                state_f, and_idx = self._add_node(state_f, self.TOKEN_ID_AND)
                # F -> &
                state_f = self._add_edge(state_f, and_idx, f_idx, self.EDGE_TYPES["arg"])
                # & -> term
                state_f = self._add_edge(state_f, term_root_idx, and_idx, self.EDGE_TYPES["arg1"])
                # & -> child
                state_f = self._add_edge(state_f, child_root_idx_in, and_idx, self.EDGE_TYPES["arg2"])
                return state_f, f_idx

            state_out, new_root_idx = jax.lax.cond(
                is_innermost, build_innermost_f, build_outer_f_and, state_in
            )
            
            return state_out, new_root_idx

        def no_op_fn(carry_in):
            """Function for lax.cond if p1 == 0. No change."""
            return carry_in

        # Run the conditional build
        final_state, final_root_idx = jax.lax.cond(
            is_active, build_nodes_fn, no_op_fn, (state, child_root_idx)
        )
        
        # Return new carry, and the root of the subtree built *at this step*
        # The final carry will have the root of the *entire* conjunct tree.
        return (final_state, final_root_idx), final_root_idx


    @partial(jax.jit, static_argnames=['self'])
    def _link_conjuncts_scan_body(self, carry, conj_root_idx):
        """
        lax.scan body to link the roots of the conjunct subtrees
        together with 'and' nodes.
        """
        state, root_so_far_idx = carry
        
        # Check if this conjunct was active (root_idx >= 0)
        is_active = conj_root_idx >= 0

        def handle_active_node(carry_in):
            """lax.cond helper: This conjunct is active."""
            state_in, root_so_far_idx_in = carry_in
            
            is_first_active_node = root_so_far_idx_in < 0
            
            def add_first_node(state_f):
                # This is the first active conjunct. Its root is the new root-so-far.
                # No new nodes/edges are added yet.
                return state_f, conj_root_idx
            
            def add_subsequent_node(state_s):
                # This is not the first. Link it with the previous root-so-far
                # using a new 'and' node.
                # Structure: & -> root_so_far
                #              -> conj_root_idx
                state_s, and_idx = self._add_node(state_s, self.TOKEN_ID_AND)
                state_s = self._add_edge(state_s, root_so_far_idx_in, and_idx, self.EDGE_TYPES["arg1"])
                state_s = self._add_edge(state_s, conj_root_idx, and_idx, self.EDGE_TYPES["arg2"])
                # The new 'and' node is now the root-so-far.
                return state_s, and_idx

            new_state, new_root_idx = jax.lax.cond(
                is_first_active_node, add_first_node, add_subsequent_node, state_in
            )
            return new_state, new_root_idx

        def handle_inactive_node(carry_in):
            """lax.cond helper: This conjunct is inactive. Do nothing."""
            return carry_in

        # Run the conditional logic
        final_state, final_root_so_far = jax.lax.cond(
            is_active, handle_active_node, handle_inactive_node, carry
        )

        return (final_state, final_root_so_far), None # No scan output needed


    @partial(jax.jit, static_argnames=['self'])
    def build_ast(self, formula_matrix):
        """
        JAX-jittable method to build an AST graph from a formula matrix.
        
        Args:
            formula_matrix (jnp.ndarray): Shape (2, max_depth, max_conjuncts)
            
        Returns:
            OrderedDict: A dict representing the graph in jraph-compatible format.
        """
        
        
             
        init_state = AstBuildState(
            nodes=jnp.zeros((self.MAX_NODES, self.feature_size), dtype=jnp.float32),
            senders=jnp.full((self.MAX_EDGES,), -1, dtype=jnp.int32),
            receivers=jnp.full((self.MAX_EDGES,), -1, dtype=jnp.int32),
            edge_types=jnp.full((self.MAX_EDGES,), -1, dtype=jnp.int32),
            node_idx=jnp.array([1]),
            edge_idx=jnp.array([0])
        )

        # --- 1. Build Conjunct Forest ---
        # We scan over the conjuncts (axis 2 of the matrix)
        # For each one, we run an *inner scan* over depth.
        
        def build_forest_scan_body(state, c_idx):
            """Outer scan body (over conjuncts)"""
            # Get the (p1s, p2s) for this conjunct
            p1s = formula_matrix[0, :, c_idx]
            p2s = formula_matrix[1, :, c_idx]
            
            # We must scan backward over depth (D-1 down to 0)
            depth_indices_reversed = jnp.arange(self.max_depth - 1, -1, -1)
            
            # Pack p1s, p2s for the scan
            # We only need the values at the reversed depth indices
            xs_inner = (p1s[depth_indices_reversed], p2s[depth_indices_reversed])
            
            # This is the 'carry' for the *inner* scan
            init_inner_carry = (state, jnp.array(-1)) # (state, child_root_idx)
            
            # Run the inner scan to build one conjunct subtree
            (final_state, final_conj_root_idx), _ = jax.lax.scan(
                self._build_conjunct_scan_body, init_inner_carry, xs_inner
            )
            
            # Return the updated state, and the root ID of the tree we just built
            return final_state, final_conj_root_idx
        
        # Run the outer scan over all conjuncts
        # `final_forest_state` has all nodes/edges for all disjoint trees
        # `all_conjunct_roots` has shape (max_conjuncts,)
        final_forest_state, all_conjunct_roots = jax.lax.scan(
            build_forest_scan_body, init_state, jnp.arange(self.max_conjuncts)
        )

        # --- 2. Link Conjuncts ---
        # Now, link the roots in `all_conjunct_roots` with 'and' nodes
        init_link_carry = (final_forest_state, jnp.array(-1)) # (state, root_so_far)
        
        (final_linked_state, final_root_idx), _ = jax.lax.scan(
            self._link_conjuncts_scan_body, init_link_carry, all_conjunct_roots
        )
        
        # --- 3. Handle 'True' case (no active conjuncts) ---
        is_empty_formula = final_root_idx < 0
        
        def add_true_node_fn(state):
            """If formula was empty, add a 'True' node."""
            new_state, true_idx = self._add_node(state, self.TOKEN_ID_TRUE)
            return new_state, true_idx
            
        def keep_existing_root_fn(state):
            """Formula was not empty, just pass state and root."""
            return state, final_root_idx
            
        final_state, final_root_idx_safe = jax.lax.cond(
            is_empty_formula, add_true_node_fn, keep_existing_root_fn, final_linked_state
        )

        # --- 4. Set 'is_root' feature ---
        # The 'is_root' feature is the last one in the feature vector
        final_nodes = final_state.nodes.at[final_root_idx_safe, -1].set(1.0)
        final_state = final_state._replace(nodes=final_nodes)

        # --- 5. Format Output ---
        # Get final counts
        final_n_node = final_state.node_idx
        final_n_edge = final_state.edge_idx
        
        return OrderedDict([
            ('nodes', final_state.nodes),
            ('senders', final_state.senders),
            ('receivers', final_state.receivers),
            ('n_node', final_n_node.reshape(1)),
            ('n_edge', final_n_edge.reshape(1)),
            ('edge_types', final_state.edge_types)
        ])


    ####################################     ENCODE     DECODE   #################################################
    
    def encode_formula(self, formula_tuple):
        """
        Encodes a Python tuple representation of a formula into a NumPy matrix.
        Not JAX-jittable.
        """
        matrix = np.zeros((2, self.max_depth, self.max_conjuncts), dtype=np.int32)
        
        # 1. Collect top-level conjuncts
        conjuncts = []
        def collect_conjuncts(f):
            if not f:
                return
            if f[0] == 'and':
                collect_conjuncts(f[1])
                collect_conjuncts(f[2])
            else:
                conjuncts.append(f)
                
        collect_conjuncts(formula_tuple)
        
        # 2. Iterate over conjuncts and fill matrix
        for c_idx, task in enumerate(conjuncts):
            if c_idx >= self.max_conjuncts:
                break # Too many conjuncts for our matrix
            
            d_idx = 0
            current_task = task
            
            # 3. Unroll the 'eventually (and ...)' structure
            while current_task and current_task[0] == 'eventually' and d_idx < self.max_depth:
                term = current_task[1]
                
                if term[0] == 'and':
                    prop_part = term[1]
                    current_task = term[2] # This is the next 'eventually'
                else:
                    prop_part = term       # This is the last term in the sequence
                    current_task = None    # Stop unrolling
                
                # 4. Encode the proposition part (atom or disjunction)
                if isinstance(prop_part, str):
                    if prop_part in PROP_TO_INDEX:
                        matrix[0, d_idx, c_idx] = PROP_TO_INDEX[prop_part]
                        matrix[1, d_idx, c_idx] = 0
                elif prop_part[0] == 'or':
                    if prop_part[1] in LTL_BASE_VOCAB:
                        matrix[0, d_idx, c_idx] = PROP_TO_INDEX[prop_part[1]]
                    if prop_part[2] in LTL_BASE_VOCAB:
                        matrix[1, d_idx, c_idx] = PROP_TO_INDEX[prop_part[2]]
                
                d_idx += 1
                
        return jnp.array(matrix)


    

    

    def decode_formula(self, formula_matrix):
        """
        Decodes a formula matrix (NumPy or JAX array) back into a Python tuple.
        Not JAX-jittable.
        """
        conjuncts = []
        # Ensure we're working with NumPy for easy iteration
        matrix = np.asarray(formula_matrix)
        max_depth, max_conjuncts = matrix.shape[1], matrix.shape[2]
        
        for c_idx in range(max_conjuncts):
            # 1. Check if this conjunct is active
            if matrix[0, 0, c_idx] == 0:
                continue
                
            # 2. Read the sequence of propositions for this conjunct
            seq_parts = []
            for d_idx in range(max_depth):
                p1_int = matrix[0, d_idx, c_idx]
                p2_int = matrix[1, d_idx, c_idx]
                
                if p1_int == 0:
                    break # End of this sequence
                
                if p2_int == 0:
                    # Single proposition
                    term = self.INV_LTL_BASE_VOCAB.get(p1_int+self.PROPS_OFFSET-1, f"P{p1_int}?")
                else:
                    # Disjunction
                    term = ('or', 
                           self.INV_LTL_BASE_VOCAB.get(p1_int+self.PROPS_OFFSET-1, f"P{p1_int}?"), 
                            self.INV_LTL_BASE_VOCAB.get(p2_int+self.PROPS_OFFSET-1, f"P{p2_int}?")
                           )
                seq_parts.append(term)
            
            if not seq_parts:
                continue
                
            # 3. Rebuild the nested 'eventually' structure from the sequence
            # (This logic is from your _get_sequence helper)
            task = None
            for term in reversed(seq_parts):
                if task is None:
                    # This is the innermost 'eventually'
                    task = ('eventually', term)
                else:
                    # This is an outer 'eventually'
                    task = ('eventually', ('and', term, task))
            
            if task:
                conjuncts.append(task)
                
        # 4. Combine all conjuncts with 'and'
        if not conjuncts:
            return True 
            
        # 4. Combine all conjuncts with 'and' in a consistent order
        #    (Builds (and, C1, (and, C2, C3)))
        ltl = conjuncts[-1]
        for i in range(len(conjuncts) - 2, -1, -1):
            ltl = ('and', conjuncts[i], ltl)
            
        return ltl_tuple_to_string(ltl)

    def visualize_ast(self, ast_data: OrderedDict, img_size=(1290, 1080)):
        """
        Visualizes an LTL AST graph from a data dictionary using
        Networkx and Matplotlib.

        Args:
            ast_data (OrderedDict): An ordered dictionary containing the graph data.
            img_size (tuple): The target (width, height) in pixels for the
                              output image.

        Returns:
            PIL.Image.Image: A PIL Image object of the rendered graph, or None
                             if plotting fails.
        """
        
        try:
            # 1. Extract data
            nodes = ast_data['nodes']
            senders = ast_data['senders']
            receivers = ast_data['receivers']
            
            # <-- MODIFIED: Use .item() to get Python scalars from JAX arrays
            # This is safer and more direct than .reshape(1)[0]
            n_node = ast_data['n_node'].item()
            n_edge = ast_data['n_edge'].item()
            
            edge_types = ast_data['edge_types']
            
            # 2. Slice data
            nodes = nodes[:n_node]
            senders = senders[:n_edge]
            receivers = receivers[:n_edge]
            edge_types = edge_types[:n_edge]

        except KeyError as e:
            print(f"Error: Missing key in ast_data: {e}", file=sys.stderr)
            return None
        except Exception as e:
            print(f"Error extracting data: {e}", file=sys.stderr)
            return None

        # 3. Build the Networkx Graph
        G = nx.DiGraph()
        node_labels = {}
        node_colors = []
        edge_labels = {}
        
        # 4. Add all nodes
        for i in range(n_node):
            node_id = str(i) # Use string IDs for consistency
            node_features = nodes[i]
            
            try:
                # <-- MODIFIED: .item() converts unhashable JAX scalar to Python int
                vocab_index = np.argmax(node_features[:-1]).item() 
                label = self.INV_LTL_BASE_VOCAB.get(vocab_index, f"UNK_{vocab_index}")
            except Exception as e:
                label = f"Error: {e}"

            # <-- MODIFIED: .item() converts JAX bool to Python bool
            is_root = (node_features[-1] == 1).item() 
            
            G.add_node(node_id)
            
            if is_root:
                node_colors.append('lightgreen')
                node_labels[node_id] = f"{label}\n(ROOT)"
            else:
                node_colors.append('lightblue')
                node_labels[node_id] = label

        # 5. Add all edges
        for j in range(n_edge):
            # Use .item() to convert numpy types to standard python types
            sender_id = str(senders[j].item()) 
            receiver_id = str(receivers[j].item())
            
            edge_type_index = edge_types[j]
            if hasattr(edge_type_index, 'item'):
                edge_type_index = edge_type_index.item()
                
            edge_label = self.INV_EDGE_TYPES.get(edge_type_index, f"UNK_{edge_type_index}")
            
            # Skip "self" loops
            if edge_label == "self":
                continue
                
            G.add_edge(sender_id, receiver_id)
            edge_labels[(sender_id, receiver_id)] = edge_label

        # 6. Plot with Matplotlib
        dpi = 100
        figsize = (img_size[0] / dpi, img_size[1] / dpi)
        
        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
        
        try:
            # 7. Get layout
            try:
                pos = nx.drawing.nx_pydot.graphviz_layout(G, prog='dot')
            except ImportError:
                print("Warning: pydot not found. Using spring_layout.", file=sys.stderr)
                print("Install pydot for a hierarchical tree layout.", file=sys.stderr)
                pos = nx.spring_layout(G, seed=42)
            
            # 8. Draw graph components
            nx.draw_networkx_nodes(
                G, pos, ax=ax, node_color=node_colors, 
                node_size=50, node_shape='s'
            )
            nx.draw_networkx_edges(
                G, pos, ax=ax, arrowstyle='->', arrowsize=20, 
                node_size=50, connectionstyle='arc3,rad=0.1'
            )
            nx.draw_networkx_labels(
                G, pos, ax=ax, labels=node_labels, font_size=9
            )
            nx.draw_networkx_edge_labels(
                G, pos, ax=ax, edge_labels=edge_labels, font_size=7
            )
            
            ax.axis('off')
            fig.tight_layout(pad=0)
            
            # 9. Save plot to in-memory buffer
            buf = io.BytesIO()
            fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)
            buf.seek(0)
            
            # 10. Open as PIL Image and resize
            img = Image.open(buf)
            img = img.resize(img_size, Image.Resampling.LANCZOS)
            
            return img

        except Exception as e:
            print(f"Error during Networkx/Matplotlib plotting: {e}", file=sys.stderr)
            return None
        finally:
            plt.close(fig)


import jax
import jax.numpy as jnp
import chex
from flax import struct
from functools import partial
from typing import Tuple
from gymnax.environments import spaces  

# --- JAX-compatible State ---
# This dataclass holds all dynamic variables of the environment.
# An instance of this is passed to step() and returned by reset() and step().
@struct.dataclass
class SimpleLTLState:
    time: chex.Array
    proposition: chex.Array  # Stores the integer index of the last action
    num_episodes: chex.Array
    key: chex.PRNGKey

# --- JAX-compatible Parameters ---
# This holds static configuration.
@struct.dataclass
class SimpleLTLEnvParams:
    timeout: int
    num_letters: int

class JaxSimpleLTLEnv:
    """
    A JAX-compatible, functional version of SimpleLTLEnv.

    This environment's logic is simple:
    - The state consists of the current time and the last proposition (action).
    - An action IS a proposition.
    - The episode ends when 'timeout' is exceeded.
    - The reward is always 0.
    - The observation is always 0.
    """

    def __init__(self, letters: str, timeout: int):
        """
        letters:
            - (str) String of propositions, e.g., "abcdef"
        timeout:
            - (int) Maximum length of the episode
        """
        # --- Static (compile-time) attributes ---
        self.unique_letters = sorted(list(set(letters)))
        self.num_letter_types = len(self.unique_letters)

        # This string map is NOT JAX-compatible and cannot be used
        # inside jitted functions. It's only for non-jit helpers.
        self._letter_map = {i: letter for i, letter in enumerate(self.unique_letters)}

        # Store default parameters
        self.default_params = SimpleLTLEnvParams(
            timeout=timeout,
            num_letters=self.num_letter_types
        )

    @property
    def params(self) -> SimpleLTLEnvParams:
        """Default environment parameters."""
        return self.default_params

    @partial(jax.jit, static_argnames=("self",))
    def reset(
        self, key: chex.PRNGKey, params: SimpleLTLEnvParams
    ) -> Tuple[chex.Array, SimpleLTLState]:
        """Resets the environment state."""

        # Get observation (always 0)
        obs = self._get_observation(None) # State-independent

        state = SimpleLTLState(
            time=jnp.array(0),
            proposition=jnp.array(-1), # -1 indicates no proposition yet
            num_episodes=jnp.array(0), # This counter resets with the env
            key=key
        )
        return obs, state

    @partial(jax.jit, static_argnames=("self",))
    def step(
        self,
        key: chex.PRNGKey,
        state: SimpleLTLState,
        action: int,
        params: SimpleLTLEnvParams,
    ) -> Tuple[chex.Array, SimpleLTLState, chex.Array, chex.Array, dict]:
        """
        This function executes an action in the environment.
        """

        # Update time and check for timeout
        new_time = state.time + 1
        done = new_time >= params.timeout

        # Reward is always 0
        reward = jnp.array(0.0)

        # Observation is always 0
        obs = self._get_observation(state)

        # The new proposition is the action taken
        new_proposition = jnp.array(action)

        # Create the new, immutable state
        new_state = SimpleLTLState(
            time=new_time,
            proposition=new_proposition,
            num_episodes=state.num_episodes, # Does not increment
            key=key
        )

        return obs, new_state, reward, done, {}

    # === Property & Helper Functions ===
    # These match the API expected by your LTLEnv wrapper

    def action_space(self, params: SimpleLTLEnvParams) -> spaces.Discrete:
        """Action space: one discrete action per letter."""
        return spaces.Discrete(params.num_letters)

    def observation_space(self, params: SimpleLTLEnvParams) -> spaces.Discrete:
        """Observation space: always 0."""
        return spaces.Discrete(1)

    @partial(jax.jit, static_argnames=("self",))
    def _get_observation(self, state: SimpleLTLState) -> chex.Array:
        """Returns the observation (which is always 0)."""
        return jnp.array(0)

    @partial(jax.jit, static_argnames=("self","params"))
    def get_events(self, state: SimpleLTLState, params: SimpleLTLEnvParams) -> chex.Array:
        """
        Gets the current "truth assignment" based on the state.
        In this env, the truth assignment is just a one-hot vector
        of the last action (proposition) taken.
        """
        return jax.lax.cond(
            state.proposition == -1,
            lambda: jnp.zeros(params.num_letters, dtype=jnp.bool_),
            lambda: jax.nn.one_hot(state.proposition, params.num_letters, dtype=jnp.bool_)
        )

    def get_propositions(self) -> List:
        return self.unique_letters

    # --- Non-JAX helper for debugging ---

    def get_events_str(self, state: SimpleLTLState) -> str:
        """
        Non-JITtable helper to get the string name of the current proposition.
        DO NOT use this inside a jitted function.
        """
        prop_idx = int(state.proposition)
        if prop_idx in self._letter_map:
            return self._letter_map[prop_idx]
        return "None"

# Example of how to use it (similar to the original SimpleLTLEnvDefault)
class JaxSimpleLTLEnvDefault(JaxSimpleLTLEnv):
    def __init__(self):
        super().__init__(letters="abcdefghijkl", timeout=75)

"""# Wrapper"""

from dataclasses import dataclass
import jax
import jax.numpy as jnp
from dataclasses import dataclass, replace
from flax.struct import PyTreeNode

class LTLEnvState(PyTreeNode):
    env_state: any  # Underlying env state
    ltl_goal: jnp.ndarray   # Current LTL formula
    ltl_original: jnp.ndarray  # Original LTL formula
    key: jnp.ndarray  # PRNG key

class LTLEnv:
    """
    Functional wrapper adding LTL goals.
    Adds LTL formula to observations, progresses it, and modifies rewards.
    """
    def __init__(self, env: JaxSimpleLTLEnv, sampler, params: any, progression_mode: str = "full", intrinsic: float = 0.0):
        self.env = env
        self.params = params
        self.progression_mode = progression_mode
        self.propositions = env.get_propositions()
        self.sampler = sampler
        self.intrinsic = intrinsic
        self.observation_space = spaces.Dict({
            'features': env.observation_space(params),
            'text': spaces.Box(low=0, high=0, shape=(), dtype=object) if progression_mode in ["full", "none"]
                   else spaces.Box(low=-1, high=1, shape=(len(self.propositions),), dtype=jnp.float32)
        })
    @partial(jax.jit, static_argnames=("self", "params"))
    def reset(self, key: jnp.ndarray, params: any) -> tuple[dict, LTLEnvState]:
        """Reset env, sample LTL goal, return dict obs and state."""
        key, subkey, sample_key= jax.random.split(key,3)
        obs, env_state = self.env.reset(subkey, params)

        final_array, _, _ = self.sample_ltl_goal(sample_key)
        ltl_state = LTLEnvState(
            env_state=env_state,
            ltl_goal=final_array,
            ltl_original=final_array,
            key=key,
        )
        graph = self.sampler.build_ast(final_array)
        ltl_obs=graph
        return ltl_obs, ltl_state


    @partial(jax.jit, static_argnames=("self", "params"))
    def step(self, key: jnp.ndarray, state: LTLEnvState, action: int, params: any) -> tuple[dict, float, bool, dict, LTLEnvState]:
        """Step env, progress LTL, and reset automatically if done."""
        # Split key for the step and a potential reset
        step_key, reset_key = jax.random.split(key)

        # --- 1. Perform the normal environment step ---
        obs, new_env_state, reward, done, info = self.env.step(step_key, state.env_state, action, params)

        # --- 2. Progress the LTL goal ---
        truth_assignment = self.get_events(new_env_state, params)
        ltl_goal, is_true, is_false= self.sampler.progress(state.ltl_goal, truth_assignment)
        
        ltl_done = jnp.logical_or(is_true, is_false)
        ltl_reward = jax.lax.cond(
            is_true,
            lambda: 1.0,
            lambda: jax.lax.cond(
                is_false,
                lambda: -1.0,
                lambda: self.intrinsic
            )
        )

        # --- 4. Determine final reward and done for the current transition ---
        final_reward = reward + ltl_reward
        final_done = jnp.logical_or(done, ltl_done)

        # --- 5. Conditionally determine the next state and observation ---
        # This is the core of the auto-reset logic.
        # We define two functions: one for the 'done' case (reset) and one for the 'not done' case.
        # `jax.lax.cond` will execute one of them based on `final_done`.
        # Both functions must return PyTrees (e.g., tuples, dicts) with the exact same structure.

        def reset_case(_):
            """Called when final_done is True. Resets the environment."""
            # We pass the separate reset_key to the reset function.
            return self.reset(reset_key, params)

        def step_case(_):
            """Called when final_done is False. Returns the result of the normal step."""
            new_state = LTLEnvState(
                env_state=new_env_state,
                ltl_goal=ltl_goal,
                ltl_original=state.ltl_original,
                key=step_key, # The new state gets the used step_key
            )
            graph = self.sampler.build_ast(ltl_goal)
            new_obs = graph
            return new_obs, new_state

        # `cond` returns the output of either `reset_case` or `step_case`.
        # Both functions return a tuple of (observation, state).
        final_obs, final_state = jax.lax.cond(
            final_done,
            reset_case,
            step_case,
            operand=None # No operand needed as functions use variables from the outer scope
        )

        # --- 6. Return the final transition ---
        # The reward and done flags are from the current step, but the observation
        # and state are for the *next* step (which is a reset if done).
        return final_obs, final_reward, final_done, info, final_state


    
    def sample_ltl_goal(self,key) -> any:
        """Sample LTL formula, adjust timeout for SequenceSampler."""
        final_array,  _, _ = self.sampler.sample(key)
        return final_array, _, _

    def get_events(self, env_state: SimpleLTLState, params: SimpleLTLEnvParams) -> chex.Array:
        """Get current propositions from the underlying env using its state."""
        return self.env.get_events(env_state, params)






"""# GCN Model"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
from typing import Callable, List, NamedTuple, Optional, Any
import distrax
from flax.linen.initializers import constant, orthogonal

def segment_sum(data: jnp.ndarray, segment_ids: jnp.ndarray, num_segments: int) -> jnp.ndarray:
    """Computes the sum of elements within segments of an array."""
    # Note: jax.ops.segment_sum is deprecated. Using jax.lax.segment_sum instead.
    # We pad segment_ids to avoid jax.lax.segment_sum's check.
    # This assumes segment_ids are contiguous from 0 to num_segments - 1.
    return jax.ops.segment_sum(data, segment_ids, num_segments=num_segments)

class RelationalUpdate(nn.Module):
    """
    A Flax module to compute messages based on relation type.
    It applies a different linear transformation for each relation.
    """
    features: int
    num_relations: int

    @nn.compact
    def __call__(self, nodes: jnp.ndarray, senders: jnp.ndarray, edge_types: jnp.ndarray) -> jnp.ndarray:
        """
        Args:
            nodes: The node features array of shape `[num_nodes, in_features]`.
            senders: The sender node indices for each edge of shape `[num_edges]`.
            edge_types: The integer type for each edge of shape `[num_edges]`.

        Returns:
            An array of computed messages of shape `[num_edges, out_features]`.
        """
        in_features = nodes.shape[-1]

        # Create a stack of weight matrices, one for each relation type.
        kernels = self.param(
            'kernels',
            nn.initializers.lecun_normal(),
            (self.num_relations, in_features, self.features)
        )

        # Get the features of the sender nodes for each edge.
        sender_features = nodes[senders]  # Shape: [num_edges, in_features]

        # Select the appropriate kernel for each edge based on its type.
        edge_kernels = kernels[edge_types] # Shape: [num_edges, in_features, out_features]

        # Compute messages: messages[e] = W_type(e) * h_sender(e)
        # einsum is efficient for this batched matrix-vector product.
        messages = jnp.einsum('eif,ei->ef', edge_kernels, sender_features) # Shape: [num_edges, out_features]

        return messages

def CustomRelationalGraphConvolution(
    update_node_module: nn.Module,
    symmetric_normalization: bool = True
) -> Callable[[dict], dict]:
    """
    Returns a function that applies a Relational Graph Convolution layer.
    This function wraps the message computation and performs aggregation.
    """
    def _ApplyRGCN(graph: dict) -> dict:
        nodes, senders, receivers, edge_types = (
            graph["nodes"], graph["senders"], graph["receivers"], graph["edge_types"]
        )

        # Compute messages using the provided relation-specific update module.
        messages = update_node_module(nodes, senders, edge_types)

        total_num_nodes = nodes.shape[0]

        # Aggregate messages at receiver nodes.
        if symmetric_normalization:
            ones = jnp.ones_like(senders, dtype=jnp.float32)
            # Ensure degrees are calculated correctly even for isolated nodes
            sender_degree = segment_sum(ones, senders, total_num_nodes).clip(1.0)
            receiver_degree = segment_sum(ones, receivers, total_num_nodes).clip(1.0)

            norm_senders = jax.lax.rsqrt(sender_degree)
            norm_receivers = jax.lax.rsqrt(receiver_degree)

            messages = messages * norm_senders[senders, None]
            aggregated_nodes = segment_sum(messages, receivers, total_num_nodes)
            aggregated_nodes = aggregated_nodes * norm_receivers[:, None]
        else:
            aggregated_nodes = segment_sum(messages, receivers, total_num_nodes)

        return {**graph, "nodes": aggregated_nodes}

    return _ApplyRGCN


# --- GNN Module (Provided) ---

class RGCNRootShared_no_jraph(nn.Module):
    """An RGCN with shared weights and root-based readout."""
    hidden_dim: int
    num_layers: int
    output_dim: int
    num_edge_types: int

    @nn.compact
    def __call__(self, graph: dict) -> jnp.ndarray:
        # Separate the 'is_root' flag from the node features.
        h_features = graph["nodes"][:, :-1]
        is_root_nodes = graph["nodes"][:, -1:]

        # Initial linear projection.
        h_0 = nn.Dense(features=self.hidden_dim, name='input_dense')(h_features)
        h = h_0

        # Define the single, shared convolutional layer module.
        # Its input will have size 2 * hidden_dim due to the skip connection.
        shared_update_module = RelationalUpdate(
            features=self.hidden_dim,
            num_relations=self.num_edge_types,
            name='shared_rgcn_update'
        )
        rgcn_layer = CustomRelationalGraphConvolution(update_node_module=shared_update_module)

        # Prepare graph structure (excluding nodes) for convolution loops
        conv_graph = {key: val for key, val in graph.items() if key != 'nodes'}

        for _ in range(self.num_layers):
            h_cat = jnp.concatenate([h, h_0], axis=-1)
            current_layer_graph = {**conv_graph, "nodes": h_cat}

            graph_after_rgcn = rgcn_layer(current_layer_graph)
            # Use tanh activation as in the DGL example.
            h = nn.tanh(graph_after_rgcn["nodes"])

        # Graph Readout: Select and sum root node embeddings.
        num_graphs = graph["n_node"].shape[0]

        # This logic handles batching (num_graphs > 1) and single instances (num_graphs=1)
        if num_graphs > 0:
            # Create segment_ids for segment_sum
            # This assumes nodes are packed contiguously per graph
            num_total_nodes = h.shape[0]
            num_nodes_per_graph = num_total_nodes // num_graphs
            segment_ids = jnp.repeat(jnp.arange(num_graphs), repeats=num_nodes_per_graph)
            graph_embeddings = segment_sum(h * is_root_nodes, segment_ids, num_segments=num_graphs)
        else:
            graph_embeddings = jnp.zeros((0, h.shape[-1]))

        output = nn.Dense(features=self.output_dim, name='output_dense')(graph_embeddings)
        return jnp.squeeze(output, axis=0) # Squeeze just in case batch size was 1

"""# ACModel"""

class ActorCritic(nn.Module):
    """
    JAX-native Actor-Critic model that combines visual and textual (GNN) embeddings.
    Accepts an Observation dataclass and returns a distrax.Distribution.
    """
    text_embedding_size: int = 32
    output_dim: int = 12


    def setup(self):
        """Initializes the sub-modules of the actor-critic model."""
        VmappedGNN = nn.vmap(
            RGCNRootShared_no_jraph,
            in_axes=0,             # Map over the first axis of the input PyTree (the Graph dict)
            out_axes=0,            # Stack outputs along the first axis
            variable_axes={'params': None}, # Do not map/split the model parameters
            split_rngs={'params': False}    # Do not split RNGs for parameter initialization
        )

        self.gnn = VmappedGNN(
            output_dim=self.text_embedding_size,
            hidden_dim=32,
            num_layers=8,
            num_edge_types=4
        )

        # Define actor network layers directly to output logits.
        actor_layers = []


        actor_layers.append(nn.Dense(features=self.output_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)))

        self.actor_net = nn.Sequential(actor_layers)

        # Critic network remains the same
        self.critic_net = nn.Sequential([
            nn.Dense(features=1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))
        ])

    def __call__(self, obs: Dict, carry=None, reset=None) -> Tuple[jnp.ndarray, distrax.Distribution, Any]:
        """
        Forward pass for the Actor-Critic model.

        Args:
            obs: An Observation dataclass instance.

        Returns:
            A tuple containing:
            - v (jnp.ndarray): The state-value estimate (critic).
            - distribution (distrax.Distribution): The policy distribution (actor).
            - carry (Any): The recurrent hidden state (None for this model).
        """



        # Process text features with GNN
        embedding_gnn = self.gnn(obs)
        # --- End Modification ---


        # --- Actor Pass ---
        # Get unnormalized logits from the actor network
        logits = self.actor_net(embedding_gnn)

        # --- MODIFICATION: Create distribution ---
        distribution = distrax.Categorical(logits=logits)
        # --- End Modification ---

        # --- Critic Pass ---
        # Get the state-value estimate from the critic network
        v = self.critic_net(embedding_gnn)


        return distribution, v

"""# Base Algo"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
import jax
import jax.numpy as jnp
import flax.linen as nn
from typing import Callable, Dict, Any, List
import rlax
import chex
from flax.struct import PyTreeNode
from dataclasses import dataclass as py_dataclass, field as py_field


# This dataclass is still useful for defining the expected structure of experience data.
@dataclass
class Experience(PyTreeNode):
    """Stores all data for a batch of experiences."""
    obs: chex.ArrayTree
    mask: chex.Array
    action: chex.Array
    value: chex.Array
    reward: chex.Array
    advantage: chex.Array
    returnn: chex.Array
    log_prob: chex.Array

@dataclass # This is from flax.struct, making it a PyTreeNode
class AlgoState:
    """
    High-level state of the algorithm.
    This class is a JAX PyTree and is fully JIT-compatible.
    It ONLY contains JAX-compatible types (arrays, params, opt_state).
    """
    params: Dict[str, Any]
    opt_state: Any

@py_dataclass # This is a standard Python dataclass
class LoggingState:
    """
    Holds non-JAX logging data (standard Python lists).
    This object lives *outside* the JIT loop and is updated in pure Python.
    """
    log_return: List[float] = py_field(default_factory=list)
    log_reshaped_return: List[float] = py_field(default_factory=list)
    log_num_frames: List[int] = py_field(default_factory=list)

@dataclass
class RolloutState(PyTreeNode):
    """State carried through the JAX scan loop. Must be JAX-compatible."""
    rng: chex.PRNGKey
    env_state: Any
    obs: chex.ArrayTree
    mask: chex.Array
    ep_return: chex.Array
    ep_reshaped_return: chex.Array
    ep_num_frames: chex.Array


class BaseAlgo(ABC):
    """Base class for JAX-based RL algorithms with JIT-compiled experience collection."""
    def __init__(self, env: JaxSimpleLTLEnv, env_params: Any, acmodel: nn.Module, num_procs: int,
                 num_frames_per_proc: int, discount: float, gae_lambda: float,
                 preprocess_obss: Callable = None, reshape_reward: Callable = None):
        self.env = env
        self.env_params = env_params
        self.acmodel = acmodel
        self.num_procs = num_procs
        self.num_frames_per_proc = num_frames_per_proc
        self.num_frames = num_procs * num_frames_per_proc
        self.discount = discount
        self.gae_lambda = gae_lambda
        self.preprocess_obss = preprocess_obss or (lambda x, rng=None: x)
        self.reshape_reward = reshape_reward
        # Vmap the environment step function for parallel execution.
        step_environment = lambda key, state, action, params: self.env.step(key, state, action, params)
        self.vmapped_env_step = jax.vmap(
            step_environment, in_axes=(0, 0, 0, None)
        )
    @partial(jax.jit, static_argnames=['self'])   
    def collect_experiences(self, algo_state: AlgoState, rollout_state: RolloutState) -> tuple[dict, dict, AlgoState, RolloutState]:
        """Collects rollouts and computes advantages. Can be JIT-compiled."""


        
        def step_fn(carry: RolloutState, _):
            """
            This function is scanned over, representing one time step for ALL parallel processes.
            """
            rng, env_state, obs, mask = carry.rng, carry.env_state, carry.obs, carry.mask
            ep_return, ep_reshaped_return, ep_num_frames = carry.ep_return, carry.ep_reshaped_return, carry.ep_num_frames

            rng, subkey_policy, subkey_step = jax.random.split(rng, 3)

            dist, value = self.acmodel.apply({'params': algo_state.params}, obs)
            value = value.squeeze(-1)
            # Sample actions for all processes from the batched distribution
            action = dist.sample(seed=subkey_policy)
            log_prob = dist.log_prob(action)

            step_keys = jax.random.split(subkey_step, self.num_procs)
            next_obs,reward,done, info, next_env_state= self.vmapped_env_step(
                step_keys,
                env_state,
                action,
                self.env_params
            )



            # Assuming reshape_reward is a simple function. If not, it needs to be JAX-compatible.
            reshaped_reward = reward * (self.discount ** ep_num_frames)

            new_ep_return = ep_return + reward
            new_ep_reshaped_return = ep_reshaped_return + reshaped_reward
            new_ep_num_frames = ep_num_frames + 1

            experience_step = {
                "obs": obs, "mask": mask, "action": action, "value": value,
                "reward": reward, "log_prob": log_prob,
                "done": done, "ep_return_at_done": new_ep_return,
                "ep_reshaped_return_at_done": new_ep_reshaped_return,
                "ep_num_frames_at_done": new_ep_num_frames
            }
            next_mask = 1.0 - done
            next_carry = RolloutState(
                rng=rng, env_state=next_env_state, obs=next_obs,
                mask=next_mask, ep_return=new_ep_return * next_mask,
                ep_reshaped_return=new_ep_reshaped_return * next_mask,
                ep_num_frames=jnp.int32(new_ep_num_frames * next_mask)
            )
            return next_carry, experience_step

        # --- KEY CHANGE: Scan the batch-oriented step_fn directly ---
        # No vmap is needed here because step_fn already handles the batch of processes.
        final_rollout_state, experiences = jax.lax.scan(
            step_fn, rollout_state, None, length=self.num_frames_per_proc
        )

        # 'experiences' is now a dictionary of arrays, a standard JAX Pytree.
        # Each value has shape: (num_frames_per_proc, num_procs, ...)

        # --- KEY CHANGE: Use a single batched call for the next value ---
        _, next_value = self.acmodel.apply({'params': algo_state.params}, final_rollout_state.obs)
        next_value = next_value.squeeze(-1)
         # --- FIX: Construct the full value sequence for GAE calculation ---
        # GAE requires values from step 0 to k, where k is the last step.
        # `experiences['value']` contains values [v0, v1, ..., vk-1]
        # `next_value` is vk. We concatenate them for the rlax function.
        all_values = jnp.concatenate(
            [experiences['value'], next_value[None, :]], axis=0
        )

        # Ensure discounts are zero for terminal states to reset GAE calculation
        # Note: The reward and discount sequences should have length k.
        discounts = self.discount * (1.0 - experiences['done'])

        # --- FIX: Vmap the GAE calculation over the batch dimension ---
        vmapped_gae = jax.vmap(
            rlax.truncated_generalized_advantage_estimation,
            # This tuple maps to the POSITIONAL arguments of the function below
            in_axes=(1, 1, None, 1),
            out_axes=1
        )

        # --- FIX: Call the vmapped function with POSITIONAL arguments ---
        # The order must match the signature: (r_t, discount_t, lambda_, values)
        advantages = vmapped_gae(
            experiences['reward'],   # Corresponds to r_t
            discounts,               # Corresponds to discount_t
            self.gae_lambda,         # Corresponds to lambda_
            all_values               # Corresponds to values
        )

        experiences['advantage'] = advantages
        experiences['returnn'] = advantages + experiences['value']

        keys_to_keep = ["obs", "mask", "action", "value", "reward", "log_prob", "advantage", "returnn"]
        exps_dict = {key: experiences[key] for key in keys_to_keep}

        # Reshape data to (num_procs * num_frames_per_proc, ...) for the update step
        exps = jax.tree_map(lambda x: jnp.swapaxes(x, 0, 1).reshape(-1, *x.shape[2:]), exps_dict)

        jax_logs = {
            "ep_return_at_done": experiences['ep_return_at_done'],
            "ep_reshaped_return_at_done": experiences['ep_reshaped_return_at_done'],
            "ep_num_frames_at_done": experiences['ep_num_frames_at_done'],
            "done_mask": experiences['done'] # Mask to filter the arrays
        }
        
        return exps, jax_logs, final_rollout_state



    @abstractmethod
    def update_parameters(self, exps: dict, state: AlgoState, rng: jnp.ndarray) -> tuple[Dict, AlgoState]:
        pass

"""# PPO Algo"""

#
# PPO JAX Implementation
#
from typing import Callable, Dict, Any, Tuple

import chex
import jax
import jax.numpy as jnp
import optax
import flax.linen as nn
from flax.struct import dataclass, field

# Import the dataclasses and BaseAlgo class provided in the prompt
# Note: Some definitions from the prompt are included here for completeness.


@dataclass
class AlgoState:
    """High-level state of the algorithm, including model parameters and optimizer state."""
    params: Dict[str, Any]
    opt_state: Any

class PPO(BaseAlgo):
    """
    The Proximal Policy Optimization (PPO) algorithm implemented in JAX.

    This class inherits from the JAX-based `BaseAlgo` and implements the
    parameter update step according to the PPO objective function.
    """

    def __init__(self,
                 env: JaxSimpleLTLEnv,
                 env_params: Any,
                 acmodel: nn.Module,
                 num_procs: int,
                 num_frames_per_proc: int,
                 discount: float = 0.99,
                 lr: float = 0.001,
                 gae_lambda: float = 0.95,
                 entropy_coef: float = 0.01,
                 value_loss_coef: float = 0.5,
                 max_grad_norm: float = 0.5,
                 adam_eps: float = 1e-8,
                 clip_eps: float = 0.2,
                 epochs: int = 4,
                 batch_size: int = 256,
                 reshape_reward: Callable = None):
        """Initializes the PPO algorithm with its specific hyperparameters."""
        super().__init__(env, env_params, acmodel, num_procs, num_frames_per_proc, discount,
                         gae_lambda, reshape_reward)

        # PPO-specific hyperparameters
        self.entropy_coef = entropy_coef
        self.value_loss_coef = value_loss_coef
        self.clip_eps = clip_eps
        self.epochs = epochs
        self.batch_size = batch_size

        # The total number of frames is flattened for minibatching
        assert self.num_frames % self.batch_size == 0, "Total frames must be divisible by batch_size."

        # Initialize the optimizer using Optax
        # We chain gradient clipping with the Adam optimizer
        self.optimizer = optax.chain(
            optax.clip_by_global_norm(max_grad_norm),
            optax.adam(learning_rate=lr, eps=adam_eps)
        )

        # JIT-compile the core update loop for maximum performance
        self._jitted_update_epochs = jax.jit(self._update_epochs)

    def update_parameters(self, exps: Experience, algo_state: AlgoState, rng: chex.PRNGKey) -> Tuple[Dict, AlgoState]:
        """
        Updates the actor-critic model parameters using PPO.

        This method serves as a JAX-friendly wrapper around the core, JIT-compiled
        update logic contained in `_update_epochs`.

        Args:
            exps: A PyTree of collected experiences.
            algo_state: The current state of the algorithm (parameters, optimizer state).
            rng: A JAX random key for shuffling data.

        Returns:
            A tuple containing a dictionary of logs and the updated `AlgoState`.
        """
        # Execute the JIT-compiled update function
        (params, opt_state), logs = self._jitted_update_epochs(
            algo_state.params,
            algo_state.opt_state,
            exps,
            rng
        )

        # Create the new state with updated parameters and optimizer state
        new_algo_state = AlgoState(
            params=params,
            opt_state=opt_state
        )

        return logs, new_algo_state

    def _loss_fn(self, params: Dict, batch: Experience) -> Tuple[chex.Array, Tuple]:
        """
        Computes the PPO loss for a single minibatch of experience.
        This function is designed to be used with `jax.grad`.

        Args:
            params: The model parameters.
            batch: A PyTree representing a minibatch of experiences.

        Returns:
            A tuple containing the total loss and auxiliary metrics (policy loss,
            value loss, entropy) for logging.
        """
        # Forward pass to get policy distribution and value estimates
        dist, value = self.acmodel.apply({'params': params}, batch["obs"])

        # --- Policy Loss (Clipped Surrogate Objective) ---
        new_log_prob = dist.log_prob(batch["action"])
        ratio = jnp.exp(new_log_prob - batch["log_prob"])

        # Normalize advantages for stability (a common practice)
        advantage = (batch["advantage"] - batch["advantage"].mean()) / (batch["advantage"].std() + 1e-8)

        surr1 = ratio * advantage
        surr2 = jnp.clip(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * advantage
        policy_loss = -jnp.minimum(surr1, surr2).mean()

        # --- Value Loss (Clipped Value Function) ---
        value_clipped = batch["value"] + jnp.clip(value - batch["value"], -self.clip_eps, self.clip_eps)
        surr1_v = (value - batch["returnn"])**2
        surr2_v = (value_clipped - batch["returnn"])**2
        value_loss = 0.5 * jnp.maximum(surr1_v, surr2_v).mean()

        # --- Entropy Bonus ---
        entropy = dist.entropy().mean()

        # --- Total Loss ---
        total_loss = (
            policy_loss
            + self.value_loss_coef * value_loss
            - self.entropy_coef * entropy
        )

        return total_loss, (policy_loss, value_loss, entropy)


    def _update_epochs(self, params: Dict, opt_state: Any, exps: Experience, rng: chex.PRNGKey) -> Tuple[Tuple, Dict]:
        """
        The core update logic that iterates over epochs and minibatches.
        This entire function is JIT-compiled.
        """
        def _epoch_step(carry, _):
            """Represents one full pass (epoch) over the entire dataset."""
            p, o, r = carry # params, opt_state, rng

            # Shuffle the experience data at the start of each epoch
            r, perm_key = jax.random.split(r)
            permutation = jax.random.permutation(perm_key, self.num_frames)
            shuffled_exps = jax.tree_map(lambda x: x[permutation], exps)

            # Reshape data into minibatches
            num_minibatches = self.num_frames // self.batch_size
            minibatches = jax.tree_map(
                lambda x: x.reshape((num_minibatches, self.batch_size) + x.shape[1:]),
                shuffled_exps
            )

            def _minibatch_step(carry, batch):
                """Updates parameters using a single minibatch."""
                params_mb, opt_state_mb = carry
                # Compute gradients and auxiliary loss data
                grad, (pi_loss, v_loss, ent) = jax.grad(self._loss_fn, has_aux=True)(params_mb, batch)
                # Update parameters and optimizer state
                updates, new_opt_state = self.optimizer.update(grad, opt_state_mb, params_mb)
                new_params = optax.apply_updates(params_mb, updates)
                return (new_params, new_opt_state), (pi_loss, v_loss, ent)

            # Scan the update function over all minibatches
            (new_p, new_o), (policy_losses, value_losses, entropies) = jax.lax.scan(
                _minibatch_step, (p, o), minibatches
            )

            # Return updated state and logs for the epoch
            return (new_p, new_o, r), (policy_losses.mean(), value_losses.mean(), entropies.mean())

        # Scan the epoch function over the configured number of epochs
        (final_params, final_opt_state, _), (pl, vl, ent) = jax.lax.scan(
            _epoch_step, (params, opt_state, rng), None, length=self.epochs
        )

        # Aggregate logs by taking the mean across all epochs
        logs = {
            "policy_loss": pl.mean(),
            "value_loss": vl.mean(),
            "entropy": ent.mean()
        }

        return (final_params, final_opt_state), logs

"""# main

"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
import wandb  # For logging
import numpy as np
import time
from tqdm import tqdm  # For progress bar
from abc import ABC, abstractmethod
from typing import Callable, Dict, Any, List, Tuple
import rlax
import chex
from flax.struct import PyTreeNode
import os
import orbax.checkpoint as ocp
from flax.struct import dataclass, field
import json


def create_train_step(ppo_algo: PPO):
    """
    Factory function to create a JIT-compiled training step.
    This closes over the static 'ppo_algo' object.
    """
    
    # Use partial to mark 'ppo_algo' as a static argument
    
    def _train_step(
        algo_state: AlgoState, 
        rollout_state: RolloutState, 
        update_key: chex.PRNGKey
    ) -> Tuple[AlgoState, RolloutState, chex.PRNGKey, Dict]:
        """
        Performs one full JIT-compiled training step:
        1. Collects experiences
        2. Updates parameters
        Returns new states and a dictionary of JAX arrays for logging.
        """
        
        # 1. Collect Experiences
        exps, rollout_jax_logs, new_rollout_state = ppo_algo.collect_experiences(
            algo_state, rollout_state
        )

        # 2. Update Parameters
        update_key, sub_key = jax.random.split(update_key)
        update_logs, new_algo_state = ppo_algo.update_parameters(
            exps, algo_state, sub_key
        )
        
        # 3. Combine logs
        # All logs are still JAX arrays, which is JIT-compatible
        combined_logs = {
            **rollout_jax_logs,
            **update_logs
        }
        
        return new_algo_state, new_rollout_state, update_key, combined_logs
        
    return _train_step


def main():
    print(jax.devices())
    """
    Main training loop for PPO with wandb logging.
    """

    # --- Hyperparameters ---
    config = {
        "LR": 1e-3,
        "NUM_PROCS": 32, # Increased for more parallel data
        "NUM_FRAMES_PER_PROC": 256,
        "DISCOUNT": 0.9,
        "GAE_LAMBDA": 0.5,
        "ENTROPY_COEF": 0.01,
        "VALUE_LOSS_COEF": 0.5,
        "MAX_GRAD_NORM": 0.5,
        "ADAM_EPS": 1e-8,
        "CLIP_EPS": 0.1,
        "EPOCHS": 2,
        "TOTAL_FRAMES": 10_000_000,
        "USE_WANDB": True,
        "WANDB_PROJECT": "LTLBootCamp", # Change this
        "WANDB_ENTITY": "pamfil-1938844",       # <--!! CHANGE THIS !!
        "CHECKPOINT_DIR": "./checkpoints/LTLBootCamp",
        "CHECKPOINT_INTERVAL": 10,
        "MAX_CHECKPOINTS_TO_KEEP": 5,
        "SEED" : 42,
    }

    # Calculate batch_size
    total_frames_per_update = config["NUM_PROCS"] * config["NUM_FRAMES_PER_PROC"]
    # We set batch_size so it divides the total frames, e.g., for 4 minibatches
    config["BATCH_SIZE"] = total_frames_per_update // 8
    assert total_frames_per_update % config["BATCH_SIZE"] == 0, "BATCH_SIZE must divide (NUM_PROCS * NUM_FRAMES_PER_PROC)"


    # --- W&B Setup ---
    if config["USE_WANDB"]:
        wandb.init(
            project=config["WANDB_PROJECT"],
            entity=config["WANDB_ENTITY"],
            mode="online",
            config=config,
            save_code=True,
        )


    # --- JAX Key Setup ---
    key = jax.random.PRNGKey(config["SEED"])
    key, model_key, reset_key, rollout_key, update_key = jax.random.split(key, 5)

    # --- Environment Setup (using your provided snippet) ---
    print("Setting up environment...")
    letters = "abcdefghijkl"

    base_env = JaxSimpleLTLEnvDefault()
    propositions = list(letters)
    sampler=JaxUntilTaskSampler(propositions, min_levels=1, max_levels=3, min_conjunctions=1, max_conjunctions=2)
    params=base_env.default_params
    # Wrap the base env with the LTL wrapper
    # Use intrinsic reward 0.0 for this test
    ltl_env = LTLEnv(base_env, sampler, params, intrinsic=0.0)
    

    # --- Algorithm and Model Setup ---
    print("Setting up model and algorithm...")
    acmodel = ActorCritic(output_dim=base_env.default_params.num_letters)

    # Instantiate the PPO algorithm with its hyperparameters
    algo = PPO(
        env=ltl_env,
        env_params=base_env.default_params,
        acmodel=acmodel,
        num_procs=config["NUM_PROCS"],
        num_frames_per_proc=config["NUM_FRAMES_PER_PROC"],
        discount=config["DISCOUNT"],
        lr=config["LR"],
        gae_lambda=config["GAE_LAMBDA"],
        entropy_coef=config["ENTROPY_COEF"],
        value_loss_coef=config["VALUE_LOSS_COEF"],
        max_grad_norm=config["MAX_GRAD_NORM"],
        adam_eps=config["ADAM_EPS"],
        clip_eps=config["CLIP_EPS"],
        epochs=config["EPOCHS"],
        batch_size=config["BATCH_SIZE"],
        reshape_reward=None  # Add your reward shaping function here if you have one
    )

    # --- Initialization ---
    print("Initializing environment and model parameters...")
    # Vmap the environment reset for parallel processes
    vmapped_env_reset = jax.vmap(ltl_env.reset, in_axes=(0, None))
    reset_keys = jax.random.split(reset_key, config["NUM_PROCS"])

    # Get initial state
    init_obs, init_env_state = vmapped_env_reset(reset_keys, base_env.default_params)

    # Initialize model parameters
    init_params = algo.acmodel.init(model_key, init_obs)['params']

    # Initialize optimizer state
    init_opt_state = algo.optimizer.init(init_params)

    # Initialize the algorithm state
    algo_state = AlgoState(
        params=init_params,
        opt_state=init_opt_state,
    )
    log_state = LoggingState()

    # Initialize the rollout state
    rollout_state = RolloutState(
        rng=rollout_key,
        env_state=init_env_state,
        obs=init_obs,
        mask=jnp.ones((config["NUM_PROCS"],), dtype=jnp.float32),
        ep_return=jnp.zeros((config["NUM_PROCS"],)),
        ep_reshaped_return=jnp.zeros((config["NUM_PROCS"],)),
        ep_num_frames=jnp.zeros((config["NUM_PROCS"],), dtype=jnp.int32)
    )

    # ---  Checkpoint Setup and Loading  ---
    print("Setting up checkpoint manager...")
    os.makedirs(config["CHECKPOINT_DIR"], exist_ok=True)
    
    options = ocp.CheckpointManagerOptions(
        max_to_keep=config["MAX_CHECKPOINTS_TO_KEEP"],
        # We handle save interval manually, but this is good practice
        save_interval_steps=config["CHECKPOINT_INTERVAL"], 
    )
    
    # Define the *structure* of what we are saving using the initialized states
    # This is our "template" for loading
    checkpoint_template = {
        'algo_state': algo_state,
        'rollout_state': rollout_state
    }
    ckpt_dir = os.path.abspath(config["CHECKPOINT_DIR"])
    checkpoint_manager = ocp.CheckpointManager(
        ckpt_dir,
        # Use the modern PyTreeCheckpointHandler for JAX PyTrees
        item_handlers=ocp.PyTreeCheckpointHandler()
    )
    
    # Check for the latest checkpoint
    start_update_idx = 0
    latest_step = checkpoint_manager.latest_step()

    if latest_step is not None:
        print(f"Resuming training from update {latest_step}...")
        
        # Restore the saved state.
        # We provide the template so Orbax knows the PyTree structure.
        restored_data = checkpoint_manager.restore(
            latest_step, 
            # Use the corresponding args for PyTreeCheckpointHandler
            args=ocp.args.PyTreeRestore(checkpoint_template)
        )
        
        algo_state = restored_data['algo_state']
        rollout_state = restored_data['rollout_state']
        # We start from the *next* update index
        start_update_idx = latest_step + 1
        
        # Update the JAX key from the restored rollout state
        update_key = rollout_state.rng 
        
        print(f"Successfully loaded checkpoint. Starting from update {start_update_idx}.")
    else:
        print("No checkpoint found. Starting new training run.")
    # --- End of Checkpoint Setup ---

    
    
        
    train_step = create_train_step(algo)
    algo_state, rollout_state, update_key, jax_logs = train_step(
        algo_state, rollout_state, update_key
    )
    jax_logs["policy_loss"].block_until_ready()
    
    # --- Training Loop ---
    num_updates = config["TOTAL_FRAMES"] // total_frames_per_update
    print(f"Starting training for {num_updates} updates ({config['TOTAL_FRAMES']} total frames)...")
    print(f"Total frames per update: {total_frames_per_update}")
    print(f"Batch size: {config['BATCH_SIZE']}, Minibatches per epoch: {total_frames_per_update // config['BATCH_SIZE']}")

    start_time = time.time()

    for update_idx in tqdm(range(start_update_idx, num_updates), desc="Training Updates"):
        algo_state, rollout_state, update_key, jax_logs = train_step(
            algo_state, rollout_state, update_key
        )
        
        # 2. === Perform Python-based logging ===
        # This happens *outside* the JIT boundary
        
        # Process rollout logs
        # We must copy JAX arrays to CPU/Python lists
        done_mask = np.array(jax_logs['done_mask']).flatten()
        
        log_state.log_return.extend(
            np.array(jax_logs['ep_return_at_done']).flatten()[done_mask].tolist()
        )
        log_state.log_reshaped_return.extend(
            np.array(jax_logs['ep_reshaped_return_at_done']).flatten()[done_mask].tolist()
        )
        log_state.log_num_frames.extend(
            np.array(jax_logs['ep_num_frames_at_done']).flatten()[done_mask].tolist()
        )

        # Process update logs
        total_frames_so_far = (update_idx + 1) * total_frames_per_update
        
        logs = {
            "update": update_idx,
            "total_frames": total_frames_so_far,
            "policy_loss": float(jax_logs["policy_loss"]),
            "value_loss": float(jax_logs["value_loss"]),
            "entropy": float(jax_logs["entropy"]),
        }
        
        # Process episode logs
        num_finished = int(done_mask.sum())
        logs["num_episodes_finished_this_update"] = num_finished
        
        if num_finished > 0:
            # Get the most recent logs from our Python state
            keep = max(num_finished, config["NUM_PROCS"]) 
            logs["mean_return"] = np.mean(log_state.log_return[-keep:])
            logs["mean_reshaped_return"] = np.mean(log_state.log_reshaped_return[-keep:])
            logs["mean_episode_length"] = np.mean(log_state.log_num_frames[-keep:])
            
        if config["USE_WANDB"]:
            try:
                # 1. Select one env to log (e.g., proc 0)
                log_proc_id = 0
                
                # 2. Get the JAX state for that proc from the *batched* rollout state
                # This state is the *result* of the last rollout
                current_ltl_env_state = jax.tree_util.tree_map(
                    lambda x: x[log_proc_id], rollout_state.env_state
                )
    
                # 3. Get the last action (proposition) from the simple env state
                last_action_id = int(current_ltl_env_state.env_state.proposition)
                # Use the env's (non-JAX) map to get the string
                action_str = base_env._letter_map.get(last_action_id, "None (at reset)")
    
           
    
                # 5. Decode the array back into a readable string
                # This uses the `decode_array_to_formula` function you provided
                formula_str = sampler.decode_formula(current_ltl_env_state.ltl_goal)
               # print(formula_str)
                # 6. Add to the wandb log dictionary
                logs["example_last_action"] = action_str
                logs["example_progressed_formula"] = formula_str

            except Exception as e:
                # Log an error if something went wrong, but don't crash training
                print(f"Warning: Failed to log formula. Error: {e}")
                logs["example_logging_error"] = 1.0
            wandb.log(logs, step=total_frames_so_far)

        # ---  Checkpointing  ---
        # We save based on the *completed* update index
        current_step_to_save = update_idx 
        is_last_step = (current_step_to_save + 1) == num_updates
        
        if (current_step_to_save + 1) % config["CHECKPOINT_INTERVAL"] == 0 or is_last_step:
            print(f"\nSaving checkpoint at update {current_step_to_save}...")
            
            # Create the dictionary of state to save
            save_state = {
                'algo_state': algo_state,
                'rollout_state': rollout_state
            }
            
            # Save the PyTree
            checkpoint_manager.save(
                current_step_to_save, 
                # Use the corresponding args for PyTreeCheckpointHandler
                args=ocp.args.PyTreeSave(save_state)
            )
            # Wait for the save to complete
            checkpoint_manager.wait_until_finished()
            print(f"Checkpoint {current_step_to_save} saved to {config['CHECKPOINT_DIR']}.")
        # --- End of Checkpointing ---

        


        

    # --- End of Training ---
    print("Training finished.")
    if config["USE_WANDB"]:
        wandb.finish()

def maino():
    # 1. Initialize Environment
    base_env = JaxSimpleLTLEnvDefault()
    propositions = base_env.get_propositions()
    sampler=JaxUntilTaskSampler(propositions, min_levels=1, max_levels=3, min_conjunctions=1, max_conjunctions=2)
    params=base_env.default_params
    # Wrap the base env with the LTL wrapper
    # Use intrinsic reward 0.0 for this test
    env = LTLEnv(base_env, sampler, params, intrinsic=0.0)
    
    # 2. Setup PRNG Key
    key = jax.random.PRNGKey(42)
    
    # 3. Reset Environment
    print("--- Initializing and Resetting Environment ---")
    key, reset_key = jax.random.split(key)
    obs, state = env.reset(reset_key, params)
    print(sampler.decode_formula(state.ltl_goal))
    
    print("-" * 40)

    # 4. Define a sequence of actions to test LTL logic
    # Our goal is "p0 U p1" (action 0 U action 1)
    #
    # Action 0 ('p0'): p1 is False, p0 is True. Reward=0.0, Done=False. LTL goal remains "p0 U p1"
    # Action 0 ('p0'): p1 is False, p0 is True. Reward=0.0, Done=False. LTL goal remains "p0 U p1"
    # Action 1 ('p1'): p1 is True.           Reward=1.0, Done=True.  LTL goal becomes "True"
    # Action 5 ('f'):  (This step will be a new episode)
    #                  Reset samples "p0 U p1". 
    #                  p1 is False, p0 is False. Reward=-1.0, Done=True. LTL goal becomes "False"
    
    actions_to_take = [0, 0, 1, 5,7,1,2,3,4,5,6,7,9,5,3,2,1]
    
    for i, action in enumerate(actions_to_take):
        print(f"\n--- Step {i+1} ---")
        
        # Split key for the step
        key, step_key = jax.random.split(key)
        
        print(f"Taking Action: {action}")
        
        # Get the underlying proposition string (non-JIT)
        prop_str = base_env._letter_map.get(action, "None")
        print(f"  (Proposition '{prop_str}')")
        
        # Perform the step
        obs, reward, done, info, state = env.step(step_key, state, action, params)
        
        print(f"  Reward Received: {reward}")
        print(f"  Done Flag: {done}")
        print(sampler.decode_formula(state.ltl_goal))

        if done:
            print("\n  *** Episode Finished (Done=True) ***")
            print("  The 'Next Obs' and 'Next State' are from a fresh reset.")
            print("  The LTL goal for the next step is 'p0 U p1' again.")
if __name__ == "__main__":
    main()