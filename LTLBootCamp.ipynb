{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba1MhYxkHdk2",
        "outputId": "32d0b494-c48c-4881-dfa5-a164c8c6332a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnax\n",
            "  Downloading gymnax-0.0.9-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.6)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (0.5.3)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from gymnax) (0.10.6)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from gymnax) (1.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from gymnax) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from gymnax) (0.13.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax) (0.1.90)\n",
            "Requirement already satisfied: jaxlib>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from optax) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optax) (2.0.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax) (1.16.2)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (1.1.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (0.11.24)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (0.1.78)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (0.1.10)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->gymnax) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->gymnax) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn->gymnax) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->gymnax) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->gymnax) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->gymnax) (2.19.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (4.13.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->gymnax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (3.23.0)\n",
            "Downloading gymnax-0.0.9-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnax\n",
            "Successfully installed gymnax-0.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install  gymnax  optax jax\n",
        "# !pip uninstall -y jax jaxlib jax-cuda12-plugin\n",
        "# # The new, simplified command for installing JAX with CUDA support\n",
        "# !pip install -U \"jax[cuda]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade rlax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivKWmtOiux1y",
        "outputId": "2896e746-e1b5-4f25-a806-600eaeb72322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rlax\n",
            "  Downloading rlax-0.1.8-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting absl-py>=2.3.1 (from rlax)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: chex>=0.1.90 in /usr/local/lib/python3.12/dist-packages (from rlax) (0.1.90)\n",
            "Collecting distrax>=0.1.7 (from rlax)\n",
            "  Downloading distrax-0.1.7-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting dm_env>=1.6 (from rlax)\n",
            "  Downloading dm_env-1.6-py3-none-any.whl.metadata (966 bytes)\n",
            "Collecting jax>=0.7.0 (from rlax)\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib>=0.7.0 (from rlax)\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from rlax) (2.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.90->rlax) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.90->rlax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.90->rlax) (0.12.1)\n",
            "Collecting tfp-nightly (from distrax>=0.1.7->rlax)\n",
            "  Downloading tfp_nightly-0.26.0.dev20251017-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from dm_env>=1.6->rlax) (0.1.9)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.0->rlax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.0->rlax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.0->rlax) (1.16.2)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->dm_env>=1.6->rlax) (25.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.12/dist-packages (from dm-tree->dm_env>=1.6->rlax) (1.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from tfp-nightly->distrax>=0.1.7->rlax) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from tfp-nightly->distrax>=0.1.7->rlax) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.12/dist-packages (from tfp-nightly->distrax>=0.1.7->rlax) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from tfp-nightly->distrax>=0.1.7->rlax) (0.6.0)\n",
            "Downloading rlax-0.1.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.2/116.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distrax-0.1.7-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.7/312.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Downloading jax-0.8.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl (79.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tfp_nightly-0.26.0.dev20251017-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: absl-py, jaxlib, tfp-nightly, jax, dm_env, distrax, rlax\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.3\n",
            "    Uninstalling jaxlib-0.5.3:\n",
            "      Successfully uninstalled jaxlib-0.5.3\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.3\n",
            "    Uninstalling jax-0.5.3:\n",
            "      Successfully uninstalled jax-0.5.3\n",
            "Successfully installed absl-py-2.3.1 distrax-0.1.7 dm_env-1.6 jax-0.8.0 jaxlib-0.8.0 rlax-0.1.8 tfp-nightly-0.26.0.dev20251017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "dAw_wPP1OQyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "propositions = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\"]\n",
        "\n",
        "LTL_BASE_VOCAB = {\n",
        "    \"and\": 0, \"or\": 1, \"not\": 2, \"next\": 3, \"until\": 4,\n",
        "    \"always\": 5, \"eventually\": 6, \"True\": 7, \"False\": 8,\n",
        "}\n",
        "\n",
        "PROP_OFFSET = len(LTL_BASE_VOCAB)\n",
        "for i, el in enumerate(propositions):\n",
        "    LTL_BASE_VOCAB[el] = PROP_OFFSET + i\n",
        "# Define constants AND OR ... for easier access\n",
        "globals().update({k.upper(): v for k, v in LTL_BASE_VOCAB.items() if v < PROP_OFFSET})\n",
        "NUM_PROPS = len(propositions)\n",
        "TRUE_VAL = LTL_BASE_VOCAB[\"True\"]\n",
        "FALSE_VAL = LTL_BASE_VOCAB[\"False\"]\n",
        "VOCAB_SIZE = len(LTL_BASE_VOCAB)\n",
        "VOCAB_INV = {v: k for k, v in LTL_BASE_VOCAB.items()}\n",
        "\n",
        "# Use jax.numpy for constants to be used in JIT'd code\n",
        "_is_unary_op_np = np.zeros(VOCAB_SIZE, dtype=bool)\n",
        "_is_unary_op_np[NOT] = True\n",
        "_is_unary_op_np[NEXT] = True\n",
        "_is_unary_op_np[EVENTUALLY] = True\n",
        "_is_unary_op_np[ALWAYS] = True\n",
        "IS_UNARY_OP = jnp.array(_is_unary_op_np)\n",
        "\n",
        "_is_binary_op_np = np.zeros(VOCAB_SIZE, dtype=bool)\n",
        "_is_binary_op_np[AND] = True\n",
        "_is_binary_op_np[OR] = True\n",
        "_is_binary_op_np[UNTIL] = True\n",
        "IS_BINARY_OP = jnp.array(_is_binary_op_np)\n",
        "\n",
        "MAX_NODES = 200 # Maximum size of the formula array\n",
        "\n",
        "def encode_letters(letter_str: str) -> tuple:\n",
        "    \"\"\"Helper function to encode a string of letters into a tuple of IDs.\"\"\"\n",
        "    return tuple(LTL_BASE_VOCAB[l] for l in letter_str)\n",
        "\n",
        "def encode_formula(formula):\n",
        "        \"\"\"Recursively encodes a formula from string/tuple to integer representation.\"\"\"\n",
        "        if isinstance(formula, str):\n",
        "            return LTL_BASE_VOCAB[formula]\n",
        "        if isinstance(formula, tuple): return tuple(encode_formula(f) for f in formula)\n",
        "        raise ValueError(f\"Unsupported element type: {formula}\")\n",
        "\n",
        "\n",
        "def encode_formula_to_array(formula, vocab, array, index=0):\n",
        "    if isinstance(formula, int):\n",
        "        array[index] = [formula, 0, 0]\n",
        "        return index + 1\n",
        "\n",
        "    op, children = formula[0], formula[1:]\n",
        "\n",
        "    if len(children) == 1:\n",
        "        array[index] = [op, index + 1, 0]\n",
        "        return encode_formula_to_array(children[0], vocab, array, index + 1)\n",
        "    elif len(children) == 2:\n",
        "        left_index = index + 1\n",
        "        right_start_index = encode_formula_to_array(children[0], vocab, array, left_index)\n",
        "        array[index] = [op, left_index, right_start_index]\n",
        "        return encode_formula_to_array(children[1], vocab, array, right_start_index)\n",
        "    raise ValueError(\"Formulas must have 1 or 2 children\")\n",
        "\n",
        "\n",
        "def decode_array_to_formula(array, node_index, num_valid_nodes, visited_nodes=None):\n",
        "    if visited_nodes is None:\n",
        "        visited_nodes = set()\n",
        "    node_index=int(node_index)\n",
        "    if not (0 <= node_index < num_valid_nodes):\n",
        "        return f\"invalid_ref_{node_index}\"\n",
        "\n",
        "    # A cycle is detected only if we revisit a non-terminal node.\n",
        "    # Shared terminal nodes like 'True' or 'False' are valid.\n",
        "    op_val, left_idx, right_idx = array[node_index]\n",
        "\n",
        "    op_val, left_idx, right_idx = int(op_val), int(left_idx), int(right_idx)\n",
        "    is_terminal = not (IS_UNARY_OP[op_val] or IS_BINARY_OP[op_val])\n",
        "\n",
        "    if not is_terminal and node_index in visited_nodes:\n",
        "        return f\"ref_{node_index}\"\n",
        "\n",
        "    visited_nodes.add(node_index)\n",
        "\n",
        "    op_str = VOCAB_INV.get(op_val, f\"p{op_val}\")\n",
        "\n",
        "    if is_terminal:\n",
        "        # Once we decode a terminal, we can remove it from the visited set\n",
        "        # to allow it to be decoded again if shared by another branch.\n",
        "        # This is not strictly necessary with the above check, but is good practice.\n",
        "        visited_nodes.remove(node_index)\n",
        "        return op_str\n",
        "\n",
        "    if IS_UNARY_OP[op_val]:\n",
        "        child = decode_array_to_formula(array, int(left_idx), num_valid_nodes, visited_nodes)\n",
        "        visited_nodes.remove(node_index)\n",
        "        return (op_str, child)\n",
        "\n",
        "    if IS_BINARY_OP[op_val]:\n",
        "        left_child = decode_array_to_formula(array, int(left_idx), num_valid_nodes, visited_nodes)\n",
        "        right_child = decode_array_to_formula(array, int(right_idx), num_valid_nodes, visited_nodes)\n",
        "        visited_nodes.remove(node_index)\n",
        "        return (op_str, left_child, right_child)\n",
        "\n",
        "    # Should be unreachable if logic is correct\n",
        "    visited_nodes.remove(node_index)\n",
        "    return op_str\n"
      ],
      "metadata": {
        "id": "3seGJ-JNOP7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ee40b1-5413-477a-df2b-08c5f4baa822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.5.3 is installed, but it is not compatible with the installed jaxlib version 0.8.0, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if6hf0qdHpQ-"
      },
      "source": [
        "# sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJc8ePjrHiEP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This class is responsible for sampling LTL formulas typically from\n",
        "given template(s).\n",
        "\n",
        "@ propositions: The set of propositions to be used in the sampled\n",
        "                formula at random.\n",
        "\"\"\"\n",
        "import random\n",
        "\n",
        "\n",
        "class LTLSampler():\n",
        "    def __init__(self, propositions):\n",
        "        self.propositions = propositions\n",
        "\n",
        "    def sample(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "# Samples from one of the other samplers at random. The other samplers are sampled by their default args.\n",
        "class SuperSampler(LTLSampler):\n",
        "    def __init__(self, propositions):\n",
        "        super().__init__(propositions)\n",
        "        self.reg_samplers = getRegisteredSamplers(self.propositions)\n",
        "\n",
        "    def sample(self):\n",
        "        return random.choice(self.reg_samplers).sample()\n",
        "\n",
        "# This class samples formulas of form (or, op_1, op_2), where op_1 and 2 can be either specified as samplers_ids\n",
        "# or by default they will be sampled at random via SuperSampler.\n",
        "class OrSampler(LTLSampler):\n",
        "    def __init__(self, propositions, sampler_ids = [\"SuperSampler\"]*2):\n",
        "        super().__init__(propositions)\n",
        "        self.sampler_ids = sampler_ids\n",
        "\n",
        "    def sample(self):\n",
        "        return ('or', getLTLSampler(self.sampler_ids[0], self.propositions).sample(),\n",
        "                        getLTLSampler(self.sampler_ids[1], self.propositions).sample())\n",
        "\n",
        "# This class generates random LTL formulas using the following template:\n",
        "#   ('until',('not','a'),('and', 'b', ('until',('not','c'),'d')))\n",
        "# where p1, p2, p3, and p4 are randomly sampled propositions\n",
        "class DefaultSampler(LTLSampler):\n",
        "    def sample(self):\n",
        "        p = random.sample(self.propositions,4)\n",
        "        return ('until',('not',p[0]),('and', p[1], ('until',('not',p[2]),p[3])))\n",
        "\n",
        "# This class generates random conjunctions of Until-Tasks.\n",
        "# Each until tasks has *n* levels, where each level consists\n",
        "# of avoiding a proposition until reaching another proposition.\n",
        "#   E.g.,\n",
        "#      Level 1: ('until',('not','a'),'b')\n",
        "#      Level 2: ('until',('not','a'),('and', 'b', ('until',('not','c'),'d')))\n",
        "#      etc...\n",
        "# The number of until-tasks, their levels, and their propositions are randomly sampled.\n",
        "# This code is a generalization of the DefaultSampler---which is equivalent to UntilTaskSampler(propositions, 2, 2, 1, 1)\n",
        "class UntilTaskSampler(LTLSampler):\n",
        "    def __init__(self, propositions, min_levels=2, max_levels=2, min_conjunctions=2 , max_conjunctions=2):\n",
        "        super().__init__(propositions)\n",
        "        self.levels       = (int(min_levels), int(max_levels))\n",
        "        self.conjunctions = (int(min_conjunctions), int(max_conjunctions))\n",
        "        assert 2*int(max_levels)*int(max_conjunctions) <= len(propositions), \"The domain does not have enough propositions!\"\n",
        "\n",
        "    def sample(self):\n",
        "        # Sampling a conjuntion of *n_conjs* (not p[0]) Until (p[1]) formulas of *n_levels* levels\n",
        "        n_conjs = random.randint(*self.conjunctions)\n",
        "        p = random.sample(self.propositions,2*self.levels[1]*n_conjs)\n",
        "        ltl = None\n",
        "        b = 0\n",
        "        for i in range(n_conjs):\n",
        "            n_levels = random.randint(*self.levels)\n",
        "            # Sampling an until task of *n_levels* levels\n",
        "            until_task = ('until',('not',p[b]),p[b+1])\n",
        "            b +=2\n",
        "            for j in range(1,n_levels):\n",
        "                until_task = ('until',('not',p[b]),('and', p[b+1], until_task))\n",
        "                b +=2\n",
        "            # Adding the until task to the conjunction of formulas that the agent have to solve\n",
        "            if ltl is None: ltl = until_task\n",
        "            else:           ltl = ('and',until_task,ltl)\n",
        "        return ltl\n",
        "\n",
        "\n",
        "# This class generates random LTL formulas that form a sequence of actions.\n",
        "# @ min_len, max_len: min/max length of the random sequence to generate.\n",
        "class SequenceSampler(LTLSampler):\n",
        "    def __init__(self, propositions, min_len=2, max_len=4):\n",
        "        super().__init__(propositions)\n",
        "        self.min_len = int(min_len)\n",
        "        self.max_len = int(max_len)\n",
        "\n",
        "    def sample(self):\n",
        "        length = random.randint(self.min_len, self.max_len)\n",
        "        seq = \"\"\n",
        "\n",
        "        while len(seq) < length:\n",
        "            c = random.choice(self.propositions)\n",
        "            if len(seq) == 0 or seq[-1] != c:\n",
        "                seq += c\n",
        "\n",
        "        ret = self._get_sequence(seq)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def _get_sequence(self, seq):\n",
        "        if len(seq) == 1:\n",
        "            return ('eventually',seq)\n",
        "        return ('eventually',('and', seq[0], self._get_sequence(seq[1:])))\n",
        "\n",
        "# This generates several sequence tasks which can be accomplished in parallel.\n",
        "# e.g. in (eventually (a and eventually c)) and (eventually b)\n",
        "# the two sequence tasks are \"a->c\" and \"b\".\n",
        "class EventuallySampler(LTLSampler):\n",
        "    def __init__(self, propositions, min_levels = 1, max_levels=4, min_conjunctions=1, max_conjunctions=3):\n",
        "        super().__init__(propositions)\n",
        "        assert(len(propositions) >= 3)\n",
        "        self.conjunctions = (int(min_conjunctions), int(max_conjunctions))\n",
        "        self.levels = (int(min_levels), int(max_levels))\n",
        "\n",
        "    def sample(self):\n",
        "        conjs = random.randint(*self.conjunctions)\n",
        "        ltl = None\n",
        "\n",
        "        for i in range(conjs):\n",
        "            task = self.sample_sequence()\n",
        "            if ltl is None:\n",
        "                ltl = task\n",
        "            else:\n",
        "                ltl = ('and',task,ltl)\n",
        "        return ltl\n",
        "\n",
        "\n",
        "    def sample_sequence(self):\n",
        "        length = random.randint(*self.levels)\n",
        "        seq = []\n",
        "\n",
        "        last = []\n",
        "        while len(seq) < length:\n",
        "            # Randomly replace some propositions with a disjunction to make more complex formulas\n",
        "            population = [p for p in self.propositions if p not in last]\n",
        "\n",
        "            if random.random() < 0.25:\n",
        "                c = random.sample(population, 2)\n",
        "            else:\n",
        "                c = random.sample(population, 1)\n",
        "\n",
        "            seq.append(c)\n",
        "            last = c\n",
        "\n",
        "        ret = self._get_sequence(seq)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def _get_sequence(self, seq):\n",
        "        term = seq[0][0] if len(seq[0]) == 1 else ('or', seq[0][0], seq[0][1])\n",
        "        if len(seq) == 1:\n",
        "            return ('eventually',term)\n",
        "        return ('eventually',('and', term, self._get_sequence(seq[1:])))\n",
        "\n",
        "\n",
        "class AdversarialEnvSampler(LTLSampler):\n",
        "    def sample(self):\n",
        "        p = random.randint(0,1)\n",
        "        if p == 0:\n",
        "            return ('eventually', ('and', 'a', ('eventually', 'b')))\n",
        "        else:\n",
        "            return ('eventually', ('and', 'a', ('eventually', 'c')))\n",
        "\n",
        "def getRegisteredSamplers(propositions):\n",
        "    return [SequenceSampler(propositions),\n",
        "            UntilTaskSampler(propositions),\n",
        "            DefaultSampler(propositions),\n",
        "            EventuallySampler(propositions)]\n",
        "\n",
        "# The LTLSampler factory method that instantiates the proper sampler\n",
        "# based on the @sampler_id.\n",
        "def getLTLSampler(sampler_id, propositions):\n",
        "    if sampler_id is None:\n",
        "        return DefaultSampler(propositions)\n",
        "    tokens = [\"Default\"]\n",
        "    if (sampler_id != None):\n",
        "        tokens = sampler_id.split(\"_\")\n",
        "\n",
        "    # Don't change the order of ifs here otherwise the OR sampler will fail\n",
        "    if (tokens[0] == \"OrSampler\"):\n",
        "        return OrSampler(propositions)\n",
        "    elif (\"_OR_\" in sampler_id): # e.g., Sequence_2_4_OR_UntilTask_3_3_1_1\n",
        "        sampler_ids = sampler_id.split(\"_OR_\")\n",
        "        return OrSampler(propositions, sampler_ids)\n",
        "    elif (tokens[0] == \"Sequence\"):\n",
        "        return SequenceSampler(propositions, tokens[1], tokens[2])\n",
        "    elif (tokens[0] == \"Until\"):\n",
        "        return UntilTaskSampler(propositions, tokens[1], tokens[2], tokens[3], tokens[4])\n",
        "    elif (tokens[0] == \"SuperSampler\"):\n",
        "        return SuperSampler(propositions)\n",
        "    elif (tokens[0] == \"Adversarial\"):\n",
        "        return AdversarialEnvSampler(propositions)\n",
        "    elif (tokens[0] == \"Eventually\"):\n",
        "        return EventuallySampler(propositions, tokens[1], tokens[2], tokens[3], tokens[4])\n",
        "    else: # \"Default\"\n",
        "        return DefaultSampler(propositions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from sfl.envs.ltl_env.utils import *\n",
        "from jax import random\n",
        "class JaxUntilTaskSampler():\n",
        "    def __init__(self, propositions, min_levels=1, max_levels=3, min_conjunctions=1, max_conjunctions=2):\n",
        "        self.prop_tokens = jnp.array([LTL_BASE_VOCAB[p] for p in propositions],dtype=jnp.int32)\n",
        "        self.num_props = len(propositions)\n",
        "        self.min_levels = min_levels\n",
        "        self.max_levels = max_levels\n",
        "        self.min_conjunctions = min_conjunctions\n",
        "        self.max_conjunctions = max_conjunctions\n",
        "        # Probability of choosing a disjunction ('or') of two propositions\n",
        "        self.disjunction_prob = 0.25\n",
        "        self.max_props_needed = 2 * max_levels * max_conjunctions\n",
        "        assert self.max_props_needed <= len(self.prop_tokens), \"Not enough propositions for the given max settings!\"\n",
        "\n",
        "\n",
        "    @partial(jax.jit, static_argnames=(\"self\"))\n",
        "    def sample(self, key):\n",
        "        \"\"\"\n",
        "        JAX-compatible function to sample a complex 'Until' task.\n",
        "\n",
        "        This function builds a formula of the form:\n",
        "        (Task_1) AND (Task_2) AND ... AND (Task_n_conjs)\n",
        "        where each Task_i is a nested 'Until' formula:\n",
        "        U(!p1, p2 AND U(!p3, p4 AND ...))\n",
        "\n",
        "        Args:\n",
        "            key: A jax.random.PRNGKey for random operations.\n",
        "            min_levels, max_levels: Min/max nesting depth for each 'Until' sub-formula.\n",
        "            min_conjunctions, max_conjunctions: Min/max number of 'Until' sub-formulas to be joined by 'AND'.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "            - formula_array (jnp.ndarray): The encoded formula in a (MAX_NODES, 3) array.\n",
        "            - num_nodes (int): The number of valid nodes used in the array.\n",
        "            - root_idx (int): The index of the root node of the final formula.\n",
        "            - num_conjuncts (int): The number of conjunctions in the formula.\n",
        "            - num_levels (int): The total number of levels across all conjunctions.\n",
        "        \"\"\"\n",
        "        # --- 1. Initial Setup and Random Sampling ---\n",
        "        key, n_conjs_key, p_key = jax.random.split(key, 3)\n",
        "\n",
        "        # Sample the number of conjunctions\n",
        "        n_conjs = jax.random.randint(n_conjs_key, (), self.min_conjunctions, self.max_conjunctions + 1)\n",
        "\n",
        "        # Sample all propositions needed upfront without replacement.\n",
        "        # We must sample the maximum possible number to ensure a static shape for JIT.\n",
        "\n",
        "        p = jax.random.choice(p_key, self.prop_tokens, shape=(self.max_props_needed,), replace=False)\n",
        "\n",
        "        # --- 2. Define Loop Bodies for jax.lax.fori_loop ---\n",
        "\n",
        "        def build_nested_until_task(key, formula_array, start_node_idx, start_prop_idx, n_levels):\n",
        "            \"\"\"Builds one nested 'Until' sub-formula.\"\"\"\n",
        "\n",
        "            # Base case: U(not p[b], p[b+1])\n",
        "            # This requires 4 nodes: p[b], p[b+1], NOT, and UNTIL.\n",
        "            p1_idx = start_node_idx\n",
        "            p0_idx = start_node_idx + 1\n",
        "            not_p0_idx = start_node_idx + 2\n",
        "            until_root_idx = start_node_idx + 3\n",
        "\n",
        "            formula_array = formula_array.at[p1_idx].set(jnp.array([p[start_prop_idx + 1], 0, 0]))\n",
        "            formula_array = formula_array.at[p0_idx].set(jnp.array([p[start_prop_idx], 0, 0]))\n",
        "            formula_array = formula_array.at[not_p0_idx].set(jnp.array([NOT, p0_idx, 0]))\n",
        "            formula_array = formula_array.at[until_root_idx].set(jnp.array([UNTIL, not_p0_idx, p1_idx]))\n",
        "\n",
        "            prop_idx = start_prop_idx + 2\n",
        "            node_idx = start_node_idx + 4\n",
        "\n",
        "            # Inner loop state: (key, formula_array, node_idx, prop_idx, current_until_root)\n",
        "            initial_inner_carry = (key, formula_array, node_idx, prop_idx, until_root_idx)\n",
        "\n",
        "            def inner_loop_body(j, carry):\n",
        "                \"\"\"Adds one level of nesting: U(not p_new, (and p_next, old_until_task))\"\"\"\n",
        "                l_key, l_formula_array, l_node_idx, l_prop_idx, l_until_root_idx = carry\n",
        "\n",
        "                # This requires 5 new nodes: p_next, p_new, NOT, AND, UNTIL\n",
        "                p_next_idx      = l_node_idx\n",
        "                p_new_idx       = l_node_idx + 1\n",
        "                not_p_new_idx   = l_node_idx + 2\n",
        "                and_idx         = l_node_idx + 3\n",
        "                new_until_root  = l_node_idx + 4\n",
        "\n",
        "                # p_next\n",
        "                l_formula_array = l_formula_array.at[p_next_idx].set(jnp.array([p[l_prop_idx + 1], 0, 0]))\n",
        "                # p_new\n",
        "                l_formula_array = l_formula_array.at[p_new_idx].set(jnp.array([p[l_prop_idx], 0, 0]))\n",
        "                # not p_new\n",
        "                l_formula_array = l_formula_array.at[not_p_new_idx].set(jnp.array([NOT, p_new_idx, 0]))\n",
        "                # p_next AND old_until_task\n",
        "                l_formula_array = l_formula_array.at[and_idx].set(jnp.array([AND, p_next_idx, l_until_root_idx]))\n",
        "                # U(not p_new, (...))\n",
        "                l_formula_array = l_formula_array.at[new_until_root].set(jnp.array([UNTIL, not_p_new_idx, and_idx]))\n",
        "\n",
        "                return (l_key, l_formula_array, l_node_idx + 5, l_prop_idx + 2, new_until_root)\n",
        "\n",
        "            # Loop n_levels - 1 times to add the nested layers\n",
        "            key, formula_array, node_idx, prop_idx, until_root_idx = jax.lax.fori_loop(\n",
        "                0, n_levels - 1, inner_loop_body, initial_inner_carry\n",
        "            )\n",
        "            return key, formula_array, node_idx, prop_idx, until_root_idx\n",
        "\n",
        "        def outer_loop_body(i, carry):\n",
        "            \"\"\"Builds one 'Until' task and ANDs it with the main formula.\"\"\"\n",
        "            key, formula_array, node_idx, prop_idx, ltl_root_idx, total_levels = carry\n",
        "            key, n_levels_key, build_key = jax.random.split(key, 3)\n",
        "\n",
        "            # Sample levels for this specific sub-formula\n",
        "            n_levels = jax.random.randint(n_levels_key, (), self.min_levels, self.max_levels + 1)\n",
        "\n",
        "            new_total_levels = total_levels + n_levels\n",
        "\n",
        "            # Build the sub-formula\n",
        "            build_key, formula_array, new_node_idx, new_prop_idx, until_task_root = build_nested_until_task(\n",
        "                build_key, formula_array, node_idx, prop_idx, n_levels\n",
        "            )\n",
        "\n",
        "            # If this is the first task, it becomes the root.\n",
        "            # Otherwise, create an AND node to join it with the existing formula.\n",
        "            def first_task_fn(_):\n",
        "                return until_task_root, formula_array, new_node_idx\n",
        "\n",
        "            def subsequent_task_fn(_):\n",
        "                and_node_idx = new_node_idx\n",
        "                new_array = formula_array.at[and_node_idx].set(jnp.array([AND, until_task_root, ltl_root_idx]))\n",
        "                return and_node_idx, new_array, new_node_idx + 1\n",
        "\n",
        "            new_ltl_root, formula_array, node_idx = jax.lax.cond(\n",
        "                ltl_root_idx == -1,  # Use -1 as a sentinel for the first task\n",
        "                first_task_fn,\n",
        "                subsequent_task_fn,\n",
        "                operand=None\n",
        "            )\n",
        "\n",
        "            return key, formula_array, node_idx, new_prop_idx, new_ltl_root, new_total_levels\n",
        "\n",
        "        # --- 3. Execute Main Loop ---\n",
        "\n",
        "        # Initial state for the main loop\n",
        "        # carry = (key, formula_array, node_idx, prop_idx, ltl_root_idx)\n",
        "        initial_carry = (\n",
        "            key,\n",
        "            jnp.full((MAX_NODES, 3), -1, dtype=jnp.int32), # Formula array\n",
        "            0,                                             # Next available node index\n",
        "            0,                                             # Next available proposition index\n",
        "            -1,\n",
        "             0,                                                                                         # Root of the combined formula\n",
        "        )\n",
        "\n",
        "        # Run the loop for n_conjs iterations\n",
        "        _, final_array, num_nodes, _, root_idx, total_levels = jax.lax.fori_loop(\n",
        "            0, n_conjs, outer_loop_body, initial_carry\n",
        "        )\n",
        "        avg_levels = total_levels.astype(jnp.float32) / n_conjs.astype(jnp.float32)\n",
        "        return final_array, num_nodes, root_idx, n_conjs, avg_levels\n",
        "\n",
        "\n",
        "\n",
        "class JaxEventuallySampler:\n",
        "    \"\"\"\n",
        "    A class to generate complex LTL formulas using a JIT-compiled JAX sampler.\n",
        "    The sampler's static configuration is provided during initialization, and\n",
        "    the JIT compilation happens once.\n",
        "    \"\"\"\n",
        "    def __init__(self, propositions, min_levels=1, max_levels=5, min_conjunctions=1, max_conjunctions=4):\n",
        "        self.propositions = jnp.array([LTL_BASE_VOCAB[p] for p in propositions],dtype=jnp.int32)\n",
        "        self.min_levels = min_levels\n",
        "        self.max_levels = max_levels\n",
        "        self.min_conjunctions = min_conjunctions\n",
        "        self.max_conjunctions = max_conjunctions\n",
        "        assert(len(propositions) >= 3)\n",
        "\n",
        "        self._jitted_sampler = partial(\n",
        "            jax.jit(self._static_sampler, static_argnames=(\n",
        "                \"min_levels\", \"max_levels\", \"min_conjunctions\", \"max_conjunctions\"\n",
        "            )),\n",
        "            min_levels=self.min_levels,\n",
        "            max_levels=self.max_levels,\n",
        "            min_conjunctions=self.min_conjunctions,\n",
        "            max_conjunctions=self.max_conjunctions,\n",
        "            propositions=self.propositions\n",
        "        )\n",
        "\n",
        "    def sample(self, key):\n",
        "        \"\"\"\n",
        "        Generates a new LTL formula sample.\n",
        "        Args:\n",
        "            key (jax.random.PRNGKey): The random key for this specific sample generation.\n",
        "        Returns:\n",
        "            A tuple of (formula_array, num_nodes, root_id).\n",
        "        \"\"\"\n",
        "        return self._jitted_sampler(key=key)\n",
        "\n",
        "    @staticmethod\n",
        "    def _static_sampler(key, propositions, min_levels, max_levels, min_conjunctions, max_conjunctions):\n",
        "        \"\"\"\n",
        "        The core JAX-jittable static method to generate LTL formulas.\n",
        "        \"\"\"\n",
        "        formula_array = jnp.zeros((MAX_NODES, 3), dtype=jnp.int32)\n",
        "\n",
        "        key, subkey = random.split(key)\n",
        "        num_conjs = random.randint(subkey, shape=(), minval=min_conjunctions, maxval=max_conjunctions + 1)\n",
        "\n",
        "        def _sample_sequence_task(carry, _):\n",
        "            key, formula_array, next_node_idx = carry\n",
        "            key, subkey = random.split(key)\n",
        "            seq_length = random.randint(subkey, shape=(), minval=min_levels, maxval=max_levels + 1)\n",
        "\n",
        "            def _generate_seq_body(i, state):\n",
        "                key, formula_array, next_node_idx, last_prop_ids, seq_node_ids = state\n",
        "                mask = jnp.all(propositions[:, None] != last_prop_ids[None, :], axis=1)\n",
        "                safe_mask = jnp.where(mask.sum() == 0, jnp.ones_like(mask), mask)\n",
        "                probs = safe_mask.astype(jnp.float32) / safe_mask.sum()\n",
        "\n",
        "                key, subkey_cond, subkey_disj = random.split(key, 3)\n",
        "\n",
        "                def _create_disjunction(k):\n",
        "                    p1, p2 = random.choice(k, propositions, shape=(2,), replace=False, p=probs)\n",
        "                    node_idx, p1_idx, p2_idx = next_node_idx, next_node_idx + 1, next_node_idx + 2\n",
        "                    arr = formula_array.at[node_idx].set(jnp.array([OR, p1_idx, p2_idx]))\n",
        "                    arr = arr.at[p1_idx].set(jnp.array([p1, 0, 0]))\n",
        "                    arr = arr.at[p2_idx].set(jnp.array([p2, 0, 0]))\n",
        "                    return arr, node_idx, next_node_idx + 3, jnp.array([p1, p2])\n",
        "\n",
        "                def _create_single_prop(k):\n",
        "                    p1 = random.choice(k, propositions, shape=(1,), p=probs)[0]\n",
        "                    node_idx = next_node_idx\n",
        "                    arr = formula_array.at[node_idx].set(jnp.array([p1, 0, 0]))\n",
        "                    return arr, node_idx, next_node_idx + 1, jnp.array([p1, -1])\n",
        "\n",
        "                arr, node_id, next_idx, new_last_props = jax.lax.cond(\n",
        "                    random.uniform(subkey_cond) < 0.25, _create_disjunction, _create_single_prop, subkey_disj)\n",
        "                seq_node_ids = seq_node_ids.at[i].set(node_id)\n",
        "                return key, arr, next_idx, new_last_props, seq_node_ids\n",
        "\n",
        "            init_seq_state = (key, formula_array, next_node_idx, jnp.array([-1, -1]), jnp.full((max_levels,), -1, dtype=jnp.int32))\n",
        "            key, formula_array, next_node_idx, _, seq_node_ids = jax.lax.fori_loop(0, seq_length, _generate_seq_body, init_seq_state)\n",
        "\n",
        "            def _build_nested_formula(i, state):\n",
        "                rev_i = seq_length - 2 - i\n",
        "                _, formula_array, next_node_idx, current_root_id = state\n",
        "                prop_node_id = seq_node_ids[rev_i]\n",
        "                and_node_id = next_node_idx\n",
        "                formula_array = formula_array.at[and_node_id].set(jnp.array([AND, prop_node_id, current_root_id]))\n",
        "                eventually_node_id = next_node_idx + 1\n",
        "                formula_array = formula_array.at[eventually_node_id].set(jnp.array([EVENTUALLY, and_node_id, 0]))\n",
        "                return key, formula_array, next_node_idx + 2, eventually_node_id\n",
        "\n",
        "            last_prop_node_id = seq_node_ids[seq_length - 1]\n",
        "            initial_root_id = next_node_idx\n",
        "            formula_array = formula_array.at[initial_root_id].set(jnp.array([EVENTUALLY, last_prop_node_id, 0]))\n",
        "\n",
        "            init_build_state = (key, formula_array, next_node_idx + 1, initial_root_id)\n",
        "\n",
        "            _, formula_array, next_node_idx, final_root_id = jax.lax.cond(\n",
        "                seq_length > 1,\n",
        "                lambda: jax.lax.fori_loop(0, seq_length - 1, _build_nested_formula, init_build_state),\n",
        "                lambda: init_build_state)\n",
        "\n",
        "            return (key, formula_array, next_node_idx), final_root_id, seq_length\n",
        "\n",
        "        def _main_conj_loop_body(i, state):\n",
        "            key, formula_array, next_node_idx, overall_root_id, total_levels = state\n",
        "            (key, formula_array, next_node_idx), new_task_root_id, seq_length = _sample_sequence_task((key, formula_array, next_node_idx), 0)\n",
        "            new_total_levels = total_levels + seq_length\n",
        "            def _combine_with_and(op):\n",
        "                prev_root_id, new_root_id, arr, idx = op\n",
        "                and_node_id = idx\n",
        "                arr = arr.at[and_node_id].set(jnp.array([AND, new_root_id, prev_root_id]))\n",
        "                return arr, idx + 1, and_node_id\n",
        "\n",
        "            def _first_task(op):\n",
        "                _, new_root_id, arr, idx = op\n",
        "                return arr, idx, new_root_id\n",
        "\n",
        "            formula_array, next_node_idx, overall_root_id = jax.lax.cond(\n",
        "                i > 0, _combine_with_and, _first_task, (overall_root_id, new_task_root_id, formula_array, next_node_idx))\n",
        "            return key, formula_array, next_node_idx, overall_root_id, new_total_levels\n",
        "\n",
        "        init_main_state = (key, formula_array, 0, -1, 0)\n",
        "        key, formula_array, num_nodes, root_id, total_levels= jax.lax.fori_loop(0, num_conjs, _main_conj_loop_body, init_main_state)\n",
        "        avg_levels = total_levels.astype(jnp.float32) / num_conjs.astype(jnp.float32)\n",
        "        return formula_array, num_nodes, root_id, num_conjs, avg_levels\n",
        "\n"
      ],
      "metadata": {
        "id": "8BrHVnf5h6E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCXIFZTmHue5"
      },
      "source": [
        "# progression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHsWoSSbHtXM"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.lax as lax\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List, Union\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import spot\n",
        "from sfl.envs.ltl_env.utils import *\n",
        "\n",
        "def simplify_spot(array, node_index, num_valid_nodes):\n",
        "    tuple_string_format=decode_array_to_formula(array, node_index, num_valid_nodes)\n",
        "    array=np.array(array)\n",
        "    ltl_spot = _get_spot_format(tuple_string_format)\n",
        "    f = spot.formula(ltl_spot)\n",
        "    f = spot.simplify(f)\n",
        "    ltl_spot = f.__format__(\"l\")\n",
        "    ltl_std,r = _get_std_format(ltl_spot.split(' '))\n",
        "    new_array = np.zeros_like(array)\n",
        "    num_nodes=encode_formula_to_array(encode_formula(ltl_std), LTL_BASE_VOCAB, array)\n",
        "    array=jnp.array(array)\n",
        "    return array, 0, num_nodes\n",
        "\n",
        "def spotify(ltl_formula):\n",
        "    ltl_spot = _get_spot_format(ltl_formula)\n",
        "    f = spot.formula(ltl_spot)\n",
        "    f = spot.simplify(f)\n",
        "    ltl_spot = f.__format__(\"l\")\n",
        "    # return ltl_spot\n",
        "    return f#.to_str('latex')\n",
        "\n",
        "\n",
        "def _get_spot_format(ltl_std):\n",
        "    ltl_spot = str(ltl_std).replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\")\n",
        "    ltl_spot = ltl_spot.replace(\"'until'\",\"U\").replace(\"'not'\",\"!\").replace(\"'or'\",\"|\").replace(\"'and'\",\"&\")\n",
        "    ltl_spot = ltl_spot.replace(\"'next'\",\"X\").replace(\"'eventually'\",\"F\").replace(\"'always'\",\"G\").replace(\"'True'\",\"t\").replace(\"'False'\",\"f\").replace(\"\\'\",\"\\\"\")\n",
        "    return ltl_spot\n",
        "\n",
        "def _get_std_format(ltl_spot):\n",
        "\n",
        "    s = ltl_spot[0]\n",
        "    r = ltl_spot[1:]\n",
        "\n",
        "    if s in [\"X\",\"U\",\"&\",\"|\"]:\n",
        "        v1,r1 = _get_std_format(r)\n",
        "        v2,r2 = _get_std_format(r1)\n",
        "        if s == \"X\": op = 'next'\n",
        "        if s == \"U\": op = 'until'\n",
        "        if s == \"&\": op = 'and'\n",
        "        if s == \"|\": op = 'or'\n",
        "        return (op,v1,v2),r2\n",
        "\n",
        "    if s in [\"F\",\"G\",\"!\"]:\n",
        "        v1,r1 = _get_std_format(r)\n",
        "        if s == \"F\": op = 'eventually'\n",
        "        if s == \"G\": op = 'always'\n",
        "        if s == \"!\": op = 'not'\n",
        "        return (op,v1),r1\n",
        "\n",
        "    if s == \"f\":\n",
        "        return 'False', r\n",
        "\n",
        "    if s == \"t\":\n",
        "        return 'True', r\n",
        "\n",
        "    if s[0] == '\"':\n",
        "        return s.replace('\"',''), r\n",
        "\n",
        "    assert False, \"Format error in spot2std\"\n",
        "\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def progress_and_clean_jax(formula_array, truth_assignment, root_index, num_nodes):\n",
        "    \"\"\"\n",
        "    The main JIT-compiled function that orchestrates the workflow.\n",
        "    It calls the core JAX logic and then the NumPy callback for simplification.\n",
        "    \"\"\"\n",
        "    # 1. Run the initial JAX-compatible part of your logic\n",
        "    dirty_root_idx, dirty_array, dirty_num_nodes = jax_static_iterative_progress_no_copy(\n",
        "        formula_array, truth_assignment, root_index, num_nodes\n",
        "    )\n",
        "\n",
        "\n",
        "    # 2. Define the shapes and dtypes for the callback's output.\n",
        "    #    This is the \"contract\" that JAX needs to compile the rest of the graph.\n",
        "    result_shape_and_dtype = (\n",
        "        jax.ShapeDtypeStruct(dirty_array.shape, dirty_array.dtype),\n",
        "        jax.ShapeDtypeStruct(dirty_root_idx.shape, dirty_root_idx.dtype),\n",
        "        jax.ShapeDtypeStruct(dirty_num_nodes.shape, dirty_num_nodes.dtype),\n",
        "    )\n",
        "\n",
        "    # 3. Call the Python function via jax.pure_callback\n",
        "    simplified_array, simplified_root_idx, simplified_num_nodes = jax.pure_callback(\n",
        "        simplify_spot, # The Python function to call\n",
        "        result_shape_and_dtype,      # The \"contract\" for the output\n",
        "        dirty_array,                 # Arguments to the callback\n",
        "        dirty_root_idx,\n",
        "        dirty_num_nodes,\n",
        "    )\n",
        "\n",
        "    return simplified_array, simplified_root_idx, simplified_num_nodes\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def jax_static_iterative_progress_no_copy(formula_array, truth_assignment, root_index, num_nodes):\n",
        "    true_node_idx = num_nodes\n",
        "    formula_array = formula_array.at[true_node_idx].set(jnp.array([TRUE_VAL, 0, 0]))\n",
        "    num_nodes += 1\n",
        "\n",
        "    false_node_idx = num_nodes\n",
        "    formula_array = formula_array.at[false_node_idx].set(jnp.array([FALSE_VAL, 0, 0]))\n",
        "    num_nodes += 1\n",
        "\n",
        "    results = jnp.full(MAX_NODES, -1, dtype=jnp.int32)\n",
        "    stack = jnp.zeros((MAX_NODES, 2), dtype=jnp.int32)\n",
        "    stack_ptr = 0\n",
        "    stack = stack.at[stack_ptr].set(jnp.array([root_index, 0]))\n",
        "    stack_ptr += 1\n",
        "\n",
        "    init_state = (formula_array, results, stack, stack_ptr, num_nodes, true_node_idx, false_node_idx)\n",
        "\n",
        "    def main_loop_body(i, state):\n",
        "        fa, res, st, sp, nn, true_idx, false_idx = state\n",
        "\n",
        "        def process_stack_top(carry):\n",
        "            fa, res, st, sp, nn, t_idx, f_idx = carry\n",
        "            sp -= 1\n",
        "            node_index, processed = st[sp]\n",
        "            op, left_idx, right_idx = fa[node_index]\n",
        "\n",
        "            is_atomic = (left_idx == 0) & (right_idx == 0) & ~(IS_UNARY_OP[op] | IS_BINARY_OP[op])\n",
        "\n",
        "\n",
        "            def compute_node(compute_carry):\n",
        "                fa, res, st, sp, nn, t_idx, f_idx = compute_carry\n",
        "\n",
        "                def process_parent(parent_carry):\n",
        "                    fa, res, st, sp, nn, t_idx, f_idx = parent_carry\n",
        "\n",
        "                    def handle_unary(unary_carry):\n",
        "                        fa_u, res_u, nn_u, sp_u = unary_carry\n",
        "                        child_res_idx = res_u[left_idx]\n",
        "                        child_res_val = fa_u[child_res_idx, 0]\n",
        "\n",
        "                        def case_not(c):\n",
        "                            res = c[0]\n",
        "                            res_idx = jnp.where(child_res_val == TRUE_VAL, f_idx, t_idx)\n",
        "                            return res.at[node_index].set(res_idx)\n",
        "\n",
        "                        def case_next(c):\n",
        "                            res = c[0]\n",
        "                            return res.at[node_index].set(child_res_idx)\n",
        "\n",
        "                        def case_temporal(op_val, c):\n",
        "                            fa, res, nn = c\n",
        "                            new_fa = fa.at[nn].set(jnp.array([op_val, node_index, child_res_idx]))\n",
        "                            return res.at[node_index].set(nn), new_fa, nn + 1\n",
        "\n",
        "                        res = lax.cond(\n",
        "                            op == LTL_BASE_VOCAB[\"not\"], case_not,\n",
        "                            lambda c: lax.cond(op == LTL_BASE_VOCAB[\"next\"], case_next, lambda x: x[0], c),\n",
        "                            (res_u,)\n",
        "                        )\n",
        "                        res, fa, nn = lax.cond(\n",
        "                            op == LTL_BASE_VOCAB[\"always\"], lambda c: case_temporal(LTL_BASE_VOCAB[\"and\"], c),\n",
        "                            lambda c: lax.cond(\n",
        "                                op == LTL_BASE_VOCAB[\"eventually\"], lambda c2: case_temporal(LTL_BASE_VOCAB[\"or\"], c2),\n",
        "                                lambda c3: (c3[1], c3[0], c3[2]), c), # (res, fa, nn)\n",
        "                            (fa_u, res, nn_u)\n",
        "                        )\n",
        "                        return fa, res, st, sp, nn, t_idx, f_idx\n",
        "\n",
        "                    def handle_binary(binary_carry):\n",
        "                        fa_b, res_b, nn_b, sp_b = binary_carry\n",
        "                        left_res_idx, right_res_idx = res_b[left_idx], res_b[right_idx]\n",
        "                        left_res_val, right_res_val = fa_b[left_res_idx, 0], fa_b[right_res_idx, 0]\n",
        "\n",
        "                        def handle_and_or(carry):\n",
        "                            fa_ao, res_ao, nn_ao, sp_ao = carry\n",
        "                            is_and = op == LTL_BASE_VOCAB[\"and\"]\n",
        "                            term_val = jnp.where(is_and, FALSE_VAL, TRUE_VAL)\n",
        "                            ident_val = jnp.where(is_and, TRUE_VAL, FALSE_VAL)\n",
        "\n",
        "                            def make_new_op_node(c):\n",
        "                                fa, nn = c\n",
        "                                new_fa = fa.at[nn].set(jnp.array([op, left_res_idx, right_res_idx]))\n",
        "                                return new_fa, nn + 1, nn\n",
        "\n",
        "                            is_term = (left_res_val == term_val) | (right_res_val == term_val)\n",
        "                            res_idx = jnp.where(is_and, f_idx, t_idx)\n",
        "\n",
        "                            fa, nn, res_idx = lax.cond(\n",
        "                               is_term, lambda c: (c[0],c[1], res_idx),\n",
        "                               lambda c: lax.cond( (left_res_val==ident_val) & (right_res_val==ident_val), lambda c2: (c2[0],c2[1], jnp.where(is_and, t_idx, f_idx)),\n",
        "                               lambda c2: lax.cond( left_res_val==ident_val, lambda c3: (c3[0],c3[1],right_res_idx),\n",
        "                               lambda c3: lax.cond( right_res_val==ident_val, lambda c4: (c4[0],c4[1],left_res_idx), make_new_op_node,c3),c2),c),\n",
        "                               (fa_ao, nn_ao))\n",
        "\n",
        "                            res = res_ao.at[node_index].set(res_idx)\n",
        "                            return fa, res, st, sp, nn, t_idx, f_idx\n",
        "\n",
        "                        def handle_until(carry):\n",
        "                            fa_u, res_u, nn_u, sp_u = carry\n",
        "\n",
        "                            def make_and_for_f1(c):\n",
        "                                fa, nn = c\n",
        "                                new_fa = fa.at[nn].set(jnp.array([LTL_BASE_VOCAB[\"and\"], left_res_idx, node_index]))\n",
        "                                return new_fa, nn + 1, nn\n",
        "\n",
        "                            fa, nn, f1_idx = lax.cond(\n",
        "                                left_res_val == FALSE_VAL, lambda c: (c[0], c[1], f_idx),\n",
        "                                lambda c: lax.cond(left_res_val == TRUE_VAL, lambda c2: (c2[0], c2[1], node_index), make_and_for_f1, c),\n",
        "                                (fa_u, nn_u))\n",
        "\n",
        "                            def make_or_for_final(c):\n",
        "                                fa_in, nn_in = c\n",
        "                                new_fa = fa_in.at[nn_in].set(jnp.array([LTL_BASE_VOCAB[\"or\"], right_res_idx, f1_idx]))\n",
        "                                return new_fa, nn_in + 1, nn_in\n",
        "\n",
        "                            fa, nn, final_res_idx = lax.cond(\n",
        "                                right_res_val == TRUE_VAL, lambda c: (c[0], c[1], t_idx),\n",
        "                                lambda c: lax.cond(right_res_val == FALSE_VAL, lambda c2: (c2[0], c2[1], f1_idx), make_or_for_final, c),\n",
        "                                (fa, nn))\n",
        "\n",
        "                            res = res_u.at[node_index].set(final_res_idx)\n",
        "                            return fa, res, st, sp, nn, t_idx, f_idx\n",
        "\n",
        "                        return lax.cond(\n",
        "                            (op == LTL_BASE_VOCAB[\"and\"]) | (op == LTL_BASE_VOCAB[\"or\"]),\n",
        "                            handle_and_or,\n",
        "                            handle_until,\n",
        "                            binary_carry\n",
        "                        )\n",
        "\n",
        "                    return lax.cond(\n",
        "                        IS_UNARY_OP[op],\n",
        "                        handle_unary,\n",
        "                        handle_binary,\n",
        "                        (fa, res, nn, sp)\n",
        "                    )\n",
        "\n",
        "                def push_children(pre_carry):\n",
        "                    fa, res, st, sp, nn, t_idx, f_idx = pre_carry\n",
        "                    st = st.at[sp].set(jnp.array([node_index, 1]))\n",
        "                    sp += 1\n",
        "                    st, sp = lax.cond(IS_BINARY_OP[op], lambda c: (c[0].at[c[1]].set(jnp.array([right_idx, 0])), c[1] + 1), lambda c: c, (st, sp))\n",
        "                    st = st.at[sp].set(jnp.array([left_idx, 0]))\n",
        "                    sp += 1\n",
        "                    return fa, res, st, sp, nn, t_idx, f_idx\n",
        "\n",
        "                return lax.cond(processed == 1, process_parent, push_children, compute_carry)\n",
        "\n",
        "            def process_atomic(atomic_carry):\n",
        "                fa, res, st, sp, nn, t_idx, f_idx = atomic_carry\n",
        "                is_true = jnp.any(op == truth_assignment)\n",
        "                prop_res = jnp.where(is_true, t_idx, f_idx)\n",
        "                final_res = jnp.where(op == TRUE_VAL, t_idx, jnp.where(op == FALSE_VAL, f_idx, prop_res))\n",
        "                return fa, res.at[node_index].set(final_res), st, sp, nn, t_idx, f_idx\n",
        "\n",
        "            return lax.cond(\n",
        "                res[node_index] != -1, lambda x: x,\n",
        "                lambda y: lax.cond(is_atomic, process_atomic, compute_node, y),\n",
        "                (fa, res, st, sp, nn, true_idx, false_idx)\n",
        "            )\n",
        "\n",
        "        return lax.cond(sp > 0, process_stack_top, lambda x: x, state)\n",
        "\n",
        "    final_state = lax.fori_loop(0, MAX_NODES * 2, main_loop_body, init_state)\n",
        "    final_fa, final_res, _, _, final_nn, _, _ = final_state\n",
        "\n",
        "    return final_res[root_index], final_fa, final_nn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB25sCGTH6Ne"
      },
      "source": [
        "# AST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2rD7NLHIBA0"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from typing import Callable, List, NamedTuple, Optional\n",
        "from sfl.envs.ltl_env.utils import *\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "edge_types = {\"self\": 0, \"arg\": 1, \"arg1\": 2, \"arg2\": 3}\n",
        "NUM_EDGE_TYPES = len(edge_types)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class JaxASTBuilder:\n",
        "    \"\"\"\n",
        "    Builds a GNN-compatible graph representation from an LTL formula array.\n",
        "    This version is designed to be fully JAX-compilable and compatible with\n",
        "    jax.ops.segment_sum by using a dedicated padding node at index 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, max_formula_nodes: int):\n",
        "        self.vocab_size = vocab_size\n",
        "        # The total number of nodes in the graph is max_formula_nodes + 1 (for the padding node)\n",
        "        self.max_graph_nodes = max_formula_nodes + 1\n",
        "        self.max_formula_nodes = max_formula_nodes\n",
        "\n",
        "\n",
        "    \"\"\"Creates and JIT-compiles the static graph building function.\"\"\"\n",
        "    @staticmethod\n",
        "    @partial(jax.jit, static_argnames=['max_formula_nodes', 'vocab_size', 'max_graph_nodes'])\n",
        "    def _build_graph_static(encoded_array, num_nodes: int, max_formula_nodes: int, vocab_size: int, max_graph_nodes: int):\n",
        "        \"\"\"\n",
        "        JIT-compiled static method to construct the graph.\n",
        "        This function returns PADDED graph arrays compatible with segment_sum.\n",
        "        - Node 0 is a dedicated padding node.\n",
        "        - Real formula nodes are indexed from 1 to num_nodes.\n",
        "        - Padded edges are self-loops on node 0.\n",
        "        \"\"\"\n",
        "        # === 1. Node Feature Construction ===\n",
        "        # We have max_graph_nodes = max_formula_nodes + 1 total nodes. Node 0 is for padding.\n",
        "        node_tokens = encoded_array[:max_formula_nodes, 0]\n",
        "        one_hot_features = jax.nn.one_hot(node_tokens, num_classes=vocab_size, dtype=jnp.float32)\n",
        "\n",
        "        # Initialize all node features to zero, including the padding node at index 0.\n",
        "        node_features = jnp.zeros((max_graph_nodes, vocab_size), dtype=jnp.float32)\n",
        "        # Place the one-hot features for the real nodes starting from index 1.\n",
        "        node_features = node_features.at[1:].set(one_hot_features)\n",
        "\n",
        "        # Mask out features for padded formula nodes (beyond num_nodes).\n",
        "        # The mask starts from index 1, as node 0 is always a padding node.\n",
        "        valid_node_mask = jnp.arange(max_graph_nodes) < (num_nodes + 1)\n",
        "        node_features = node_features * valid_node_mask[:, None]\n",
        "\n",
        "        # The root is now at index 1.\n",
        "        is_root_feature = jnp.zeros((max_graph_nodes, 1), dtype=jnp.float32).at[1].set(1.0)\n",
        "        node_features = jnp.concatenate([node_features, is_root_feature], axis=-1)\n",
        "\n",
        "        # === 2. Edge Construction Loop ===\n",
        "        def edge_construction_body(i, carry):\n",
        "            senders, receivers, edge_type_indices = carry\n",
        "            op, left_idx, right_idx = encoded_array[i]\n",
        "\n",
        "            # **MODIFIED**: Shift all node indices by +1 to account for the padding node.\n",
        "            current_node_idx = i + 1\n",
        "            left_child_idx = left_idx + 1\n",
        "            right_child_idx = right_idx + 1\n",
        "\n",
        "            # Self-loop for the current real node.\n",
        "            senders = senders.at[3 * i].set(current_node_idx)\n",
        "            receivers = receivers.at[3 * i].set(current_node_idx)\n",
        "            edge_type_indices = edge_type_indices.at[3 * i].set(edge_types['self'])\n",
        "\n",
        "            # Unary op\n",
        "            def unary_true_fn(vals):\n",
        "                s, r, e = vals\n",
        "                s = s.at[3 * i + 1].set(left_child_idx)\n",
        "                r = r.at[3 * i + 1].set(current_node_idx)\n",
        "                e = e.at[3 * i + 1].set(edge_types['arg'])\n",
        "                return s, r, e\n",
        "            senders, receivers, edge_type_indices = jax.lax.cond(\n",
        "                IS_UNARY_OP[op], unary_true_fn, lambda vals: vals, (senders, receivers, edge_type_indices)\n",
        "            )\n",
        "\n",
        "            # Binary op\n",
        "            def binary_true_fn(vals):\n",
        "                s, r, e = vals\n",
        "                s = s.at[3 * i + 1].set(left_child_idx)\n",
        "                r = r.at[3 * i + 1].set(current_node_idx)\n",
        "                e = e.at[3 * i + 1].set(edge_types['arg1'])\n",
        "                s = s.at[3 * i + 2].set(right_child_idx)\n",
        "                r = r.at[3 * i + 2].set(current_node_idx)\n",
        "                e = e.at[3 * i + 2].set(edge_types['arg2'])\n",
        "                return s, r, e\n",
        "            senders, receivers, edge_type_indices = jax.lax.cond(\n",
        "                IS_BINARY_OP[op], binary_true_fn, lambda vals: vals, (senders, receivers, edge_type_indices)\n",
        "            )\n",
        "            return senders, receivers, edge_type_indices\n",
        "\n",
        "        # Initialize with a sentinel value to detect validly created edges.\n",
        "        num_potential_edges = max_formula_nodes * 3\n",
        "        init_val = jnp.full(num_potential_edges, -1, dtype=jnp.int32)\n",
        "        senders_padded, receivers_padded, edge_types_padded = jax.lax.fori_loop(\n",
        "            0, num_nodes, edge_construction_body, (init_val, init_val, init_val)\n",
        "        )\n",
        "\n",
        "        # === 3. Gather Valid Edges and Pad for segment_sum ===\n",
        "        valid_edge_mask = senders_padded != -1\n",
        "        num_valid_edges = valid_edge_mask.sum()\n",
        "\n",
        "        # Use argsort to efficiently move all valid edges to the front.\n",
        "        sort_key = jnp.where(valid_edge_mask, jnp.arange(num_potential_edges), num_potential_edges)\n",
        "        permutation = jnp.argsort(sort_key)\n",
        "        senders_clean = senders_padded[permutation]\n",
        "        receivers_clean = receivers_padded[permutation]\n",
        "        edge_types_clean = edge_types_padded[permutation]\n",
        "\n",
        "        # **MODIFIED**: Create a mask for padded edges (those after the valid ones).\n",
        "        # All padded senders and receivers should point to the padding node (index 0).\n",
        "        is_padded_edge_mask = jnp.arange(num_potential_edges) >= num_valid_edges\n",
        "        final_senders = jnp.where(is_padded_edge_mask, 0, senders_clean)\n",
        "        final_receivers = jnp.where(is_padded_edge_mask, 0, receivers_clean)\n",
        "        # We can also set the edge type to 'self' for padded edges.\n",
        "        final_edge_types = jnp.where(is_padded_edge_mask, edge_types['self'], edge_types_clean)\n",
        "\n",
        "        # # === 4. Edge Feature Construction ===\n",
        "        # edge_features = jax.nn.one_hot(final_edge_types, num_classes=NUM_EDGE_TYPES, dtype=jnp.float32)\n",
        "        # # Mask out features for padded edges to ensure they are zero.\n",
        "        # edge_features = edge_features * (~is_padded_edge_mask)[:, None]\n",
        "\n",
        "        return OrderedDict([\n",
        "            ('nodes', node_features),\n",
        "            ('senders', final_senders),\n",
        "            ('receivers', final_receivers),\n",
        "            ('n_node', jnp.array([num_nodes])),\n",
        "            ('edge_types', final_edge_types)\n",
        "        ])\n",
        "\n",
        "\n",
        "    @partial(jax.jit, static_argnums=0)\n",
        "    def __call__(self, encoded_array: jnp.ndarray, num_nodes: int):\n",
        "        \"\"\"\n",
        "        Processes an encoded formula array to produce a padded graph representation\n",
        "        that is compatible with jax.ops.segment_sum.\n",
        "        \"\"\"\n",
        "        # The __call__ can't be jitted with self, as it would recompile for every instance.\n",
        "        # The performance comes from jitting the static _build_graph_static method.\n",
        "        return JaxASTBuilder._build_graph_static(encoded_array, num_nodes,max_formula_nodes=self.max_formula_nodes,max_graph_nodes=self.max_graph_nodes,vocab_size=VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et8sKweyIFcq"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTwKSjw2IJ0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3be613-1477-4450-a81e-e6a28234e40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "  TEST 1: STANDARD 5x5 (WORLD-CENTRIC)\n",
            "==================================================\n",
            "\n",
            "\n",
            "Step: 0\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|A|.|e|a|.|\n",
            "|.|.|.|.|b|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|.|e|\n",
            "|.|a|c|.|d|\n",
            "-----------\n",
            "Current Event: None\n",
            "Move (w/a/s/d) and press Enter, or q to quit: a\n",
            "\n",
            "Step: 1\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|.|.|e|a|A|\n",
            "|.|.|.|.|b|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|.|e|\n",
            "|.|a|c|.|d|\n",
            "-----------\n",
            "Current Event: None\n",
            "Move (w/a/s/d) and press Enter, or q to quit: s\n",
            "\n",
            "Step: 2\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|.|.|e|a|.|\n",
            "|.|.|.|.|A|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|.|e|\n",
            "|.|a|c|.|d|\n",
            "-----------\n",
            "Current Event: b\n",
            "Move (w/a/s/d) and press Enter, or q to quit: a\n",
            "\n",
            "Step: 3\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|.|.|e|a|.|\n",
            "|.|.|.|A|b|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|.|e|\n",
            "|.|a|c|.|d|\n",
            "-----------\n",
            "Current Event: None\n",
            "Move (w/a/s/d) and press Enter, or q to quit: w\n",
            "\n",
            "Step: 4\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|.|.|e|A|.|\n",
            "|.|.|.|.|b|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|.|e|\n",
            "|.|a|c|.|d|\n",
            "-----------\n",
            "Current Event: a\n",
            "Move (w/a/s/d) and press Enter, or q to quit: w\n",
            "\n",
            "Step: 5\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|.|.|e|a|.|\n",
            "|.|.|.|.|b|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|.|e|\n",
            "|.|a|c|A|d|\n",
            "-----------\n",
            "Current Event: None\n",
            "Move (w/a/s/d) and press Enter, or q to quit: \n",
            "Invalid key.\n",
            "\n",
            "Step: 5\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|.|.|e|a|.|\n",
            "|.|.|.|.|b|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|.|e|\n",
            "|.|a|c|A|d|\n",
            "-----------\n",
            "Current Event: None\n",
            "Move (w/a/s/d) and press Enter, or q to quit: w\n",
            "\n",
            "Step: 6\n",
            "--- True Map View ---\n",
            "-----------\n",
            "|.|.|e|a|.|\n",
            "|.|.|.|.|b|\n",
            "|d|.|.|b|c|\n",
            "|.|.|.|A|e|\n",
            "|.|a|c|.|d|\n",
            "-----------\n",
            "Current Event: None\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import chex\n",
        "from flax import struct\n",
        "from gymnax.environments import spaces  # Assuming gymnax is available per the wrapper\n",
        "from functools import partial\n",
        "from typing import Tuple\n",
        "\n",
        "# --- JAX-compatible State ---\n",
        "# This dataclass holds all dynamic variables of the environment.\n",
        "# An instance of this is passed to step() and returned by reset() and step().\n",
        "@struct.dataclass\n",
        "class SimpleLTLState:\n",
        "    time: chex.Array\n",
        "    proposition: chex.Array  # Stores the integer index of the last action\n",
        "    num_episodes: chex.Array\n",
        "    key: chex.PRNGKey\n",
        "\n",
        "# --- JAX-compatible Parameters ---\n",
        "# This holds static configuration.\n",
        "@struct.dataclass\n",
        "class SimpleLTLEnvParams:\n",
        "    timeout: int\n",
        "    num_letters: int\n",
        "\n",
        "class JaxSimpleLTLEnv:\n",
        "    \"\"\"\n",
        "    A JAX-compatible, functional version of SimpleLTLEnv.\n",
        "\n",
        "    This environment's logic is simple:\n",
        "    - The state consists of the current time and the last proposition (action).\n",
        "    - An action IS a proposition.\n",
        "    - The episode ends when 'timeout' is exceeded.\n",
        "    - The reward is always 0.\n",
        "    - The observation is always 0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, letters: str, timeout: int):\n",
        "        \"\"\"\n",
        "        letters:\n",
        "            - (str) String of propositions, e.g., \"abcdef\"\n",
        "        timeout:\n",
        "            - (int) Maximum length of the episode\n",
        "        \"\"\"\n",
        "        # --- Static (compile-time) attributes ---\n",
        "        unique_letters = sorted(list(set(letters)))\n",
        "        self.num_letter_types = len(unique_letters)\n",
        "\n",
        "        # This string map is NOT JAX-compatible and cannot be used\n",
        "        # inside jitted functions. It's only for non-jit helpers.\n",
        "        self._letter_map = {i: letter for i, letter in enumerate(unique_letters)}\n",
        "\n",
        "        # Store default parameters\n",
        "        self.default_params = SimpleLTLEnvParams(\n",
        "            timeout=timeout,\n",
        "            num_letters=self.num_letter_types\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def params(self) -> SimpleLTLEnvParams:\n",
        "        \"\"\"Default environment parameters.\"\"\"\n",
        "        return self.default_params\n",
        "\n",
        "    @partial(jax.jit, static_argnames=(\"self\",))\n",
        "    def reset(\n",
        "        self, key: chex.PRNGKey, params: SimpleLTLEnvParams\n",
        "    ) -> Tuple[chex.Array, SimpleLTLState]:\n",
        "        \"\"\"Resets the environment state.\"\"\"\n",
        "\n",
        "        # Get observation (always 0)\n",
        "        obs = self._get_observation(None) # State-independent\n",
        "\n",
        "        state = SimpleLTLState(\n",
        "            time=jnp.array(0),\n",
        "            proposition=jnp.array(-1), # -1 indicates no proposition yet\n",
        "            num_episodes=jnp.array(0), # This counter resets with the env\n",
        "            key=key\n",
        "        )\n",
        "        return obs, state\n",
        "\n",
        "    @partial(jax.jit, static_argnames=(\"self\",))\n",
        "    def step(\n",
        "        self,\n",
        "        key: chex.PRNGKey,\n",
        "        state: SimpleLTLState,\n",
        "        action: int,\n",
        "        params: SimpleLTLEnvParams,\n",
        "    ) -> Tuple[chex.Array, SimpleLTLState, chex.Array, chex.Array, dict]:\n",
        "        \"\"\"\n",
        "        This function executes an action in the environment.\n",
        "        \"\"\"\n",
        "\n",
        "        # Update time and check for timeout\n",
        "        new_time = state.time + 1\n",
        "        done = new_time > params.timeout\n",
        "\n",
        "        # Reward is always 0\n",
        "        reward = jnp.array(0.0)\n",
        "\n",
        "        # Observation is always 0\n",
        "        obs = self._get_observation(state)\n",
        "\n",
        "        # The new proposition is the action taken\n",
        "        new_proposition = jnp.array(action)\n",
        "\n",
        "        # Create the new, immutable state\n",
        "        new_state = SimpleLTLState(\n",
        "            time=new_time,\n",
        "            proposition=new_proposition,\n",
        "            num_episodes=state.num_episodes, # Does not increment\n",
        "            key=key\n",
        "        )\n",
        "\n",
        "        return obs, new_state, reward, done, {}\n",
        "\n",
        "    # === Property & Helper Functions ===\n",
        "    # These match the API expected by your LTLEnv wrapper\n",
        "\n",
        "    def action_space(self, params: SimpleLTLEnvParams) -> spaces.Discrete:\n",
        "        \"\"\"Action space: one discrete action per letter.\"\"\"\n",
        "        return spaces.Discrete(params.num_letters)\n",
        "\n",
        "    def observation_space(self, params: SimpleLTLEnvParams) -> spaces.Discrete:\n",
        "        \"\"\"Observation space: always 0.\"\"\"\n",
        "        return spaces.Discrete(1)\n",
        "\n",
        "    @partial(jax.jit, static_argnames=(\"self\",))\n",
        "    def _get_observation(self, state: SimpleLTLState) -> chex.Array:\n",
        "        \"\"\"Returns the observation (which is always 0).\"\"\"\n",
        "        return jnp.array(0)\n",
        "\n",
        "    @partial(jax.jit, static_argnames=(\"self\",))\n",
        "    def get_events(self, state: SimpleLTLState, params: SimpleLTLEnvParams) -> chex.Array:\n",
        "        \"\"\"\n",
        "        Gets the current \"truth assignment\" based on the state.\n",
        "        In this env, the truth assignment is just a one-hot vector\n",
        "        of the last action (proposition) taken.\n",
        "        \"\"\"\n",
        "        # If proposition is -1 (at reset), return all-false vector\n",
        "        return jax.lax.cond(\n",
        "            state.proposition == -1,\n",
        "            lambda: jnp.zeros(params.num_letters, dtype=jnp.bool_),\n",
        "            lambda: jax.nn.one_hot(state.proposition, params.num_letters, dtype=jnp.bool_)\n",
        "        )\n",
        "\n",
        "    def get_propositions(self, params: SimpleLTLEnvParams) -> chex.Array:\n",
        "        \"\"\"\n",
        "        Returns the set of all possible propositions *as integer indices*.\n",
        "        \"\"\"\n",
        "        return jnp.arange(params.num_letters)\n",
        "\n",
        "    # --- Non-JAX helper for debugging ---\n",
        "\n",
        "    def get_events_str(self, state: SimpleLTLState) -> str:\n",
        "        \"\"\"\n",
        "        Non-JITtable helper to get the string name of the current proposition.\n",
        "        DO NOT use this inside a jitted function.\n",
        "        \"\"\"\n",
        "        prop_idx = int(state.proposition)\n",
        "        if prop_idx in self._letter_map:\n",
        "            return self._letter_map[prop_idx]\n",
        "        return \"None\"\n",
        "\n",
        "# Example of how to use it (similar to the original SimpleLTLEnvDefault)\n",
        "class JaxSimpleLTLEnvDefault(JaxSimpleLTLEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__(letters=\"abcdefghijkl\", timeout=75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjfoosDqIJGg"
      },
      "source": [
        "# Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsDePU8LIPkr"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from gymnax.environments.environment import Environment # Corrected import\n",
        "from gym import spaces\n",
        "from dataclasses import dataclass, replace\n",
        "from flax.struct import PyTreeNode\n",
        "\n",
        "class LTLEnvState(PyTreeNode):\n",
        "    env_state: any  # Underlying env state\n",
        "    ltl_goal: jnp.ndarray   # Current LTL formula\n",
        "    ltl_original: jnp.ndarray  # Original LTL formula\n",
        "    key: jnp.ndarray  # PRNG key\n",
        "    num_nodes: jnp.ndarray\n",
        "    root_idx: jnp.ndarray\n",
        "\n",
        "class LTLEnv:\n",
        "    \"\"\"\n",
        "    Functional wrapper adding LTL goals to a Gymnax environment.\n",
        "    Adds LTL formula to observations, progresses it, and modifies rewards.\n",
        "    \"\"\"\n",
        "    def __init__(self, env: Environment, params: any, progression_mode: str = \"full\",\n",
        "                 ltl_sampler: str = None, intrinsic: float = 0.0):\n",
        "        self.env = env\n",
        "        self.params = params\n",
        "        self.progression_mode = progression_mode\n",
        "        self.propositions = env.get_propositions(params)\n",
        "        self.sampler = JaxUntilTaskSampler(propositions,propositions, min_levels=1, max_levels=3, min_conjunctions=1, max_conjunctions=2)\n",
        "        self.intrinsic = intrinsic\n",
        "        self.observation_space = spaces.Dict({\n",
        "            'features': env.observation_space(params),\n",
        "            'text': spaces.Box(low=0, high=0, shape=(), dtype=object) if progression_mode in [\"full\", \"none\"]\n",
        "                   else spaces.Box(low=-1, high=1, shape=(len(self.propositions),), dtype=jnp.float32)\n",
        "        })\n",
        "        self.ast_builder = JaxASTBuilder(VOCAB_SIZE, MAX_NODES)\n",
        "    @partial(jax.jit, static_argnames=(\"self\", \"params\"))\n",
        "    def reset(self, key: jnp.ndarray, params: any) -> tuple[dict, LTLEnvState]:\n",
        "        \"\"\"Reset env, sample LTL goal, return dict obs and state.\"\"\"\n",
        "        key, subkey, sample_key= jax.random.split(key,3)\n",
        "        obs, env_state = self.env.reset_env(subkey, params)\n",
        "\n",
        "        final_array, num_nodes, root_idx = self.sample_ltl_goal(sample_key)\n",
        "        ltl_state = LTLEnvState(\n",
        "            env_state=env_state,\n",
        "            ltl_goal=final_array,\n",
        "            ltl_original=final_array,\n",
        "            key=key,\n",
        "            num_nodes=num_nodes,\n",
        "            root_idx=root_idx\n",
        "        )\n",
        "        graph = self.ast_builder(final_array, num_nodes)\n",
        "        ltl_obs=graph\n",
        "        return ltl_obs, ltl_state\n",
        "\n",
        "    # @partial(jax.jit, static_argnames=(\"self\", \"params\"))\n",
        "    # def step(self, key: jnp.ndarray, state: LTLEnvState, action: int, params: any) -> tuple[dict, float, bool, dict, LTLEnvState]:\n",
        "    #     \"\"\"Step env, progress LTL, and auto-reset if done.\"\"\"\n",
        "    #     # 1. Split the key for the step and a potential reset\n",
        "    #     key_step, key_reset = jax.random.split(key)\n",
        "\n",
        "    #     # --- Calculate the outcome of a single step (the \"state_st\" path) ---\n",
        "    #     obs_st, reward_st, done_st, info, new_env_state = self.env.step_env(\n",
        "    #         key_step, state.env_state, action, params\n",
        "    #     )\n",
        "\n",
        "    #     # Progress LTL formula\n",
        "    #     truth_assignment = self.get_events(new_env_state, params)\n",
        "    #     ltl_goal, root_index, num_nodes = progress_and_clean_jax(\n",
        "    #         state.ltl_goal, truth_assignment, state.root_idx, state.num_nodes\n",
        "    #     )\n",
        "\n",
        "    #     # Create the next state if the episode is not done\n",
        "    #     state_st = LTLEnvState(\n",
        "    #         env_state=new_env_state,\n",
        "    #         ltl_goal=ltl_goal,\n",
        "    #         ltl_original=state.ltl_original,\n",
        "    #         key=key_step, # This key is not used after this, but we keep the structure\n",
        "    #         num_nodes=num_nodes,\n",
        "    #         root_idx=root_index\n",
        "    #     )\n",
        "\n",
        "    #     # Compute LTL reward and combine done signals\n",
        "    #     ltl_reward = jax.lax.cond(\n",
        "    #         ltl_goal[0][0] == LTL_BASE_VOCAB['True'], lambda: 1.0,\n",
        "    #         lambda: jax.lax.cond(\n",
        "    #             ltl_goal[0][0] == LTL_BASE_VOCAB['False'], lambda: -1.0,\n",
        "    #             lambda: self.intrinsic\n",
        "    #         )\n",
        "    #     )\n",
        "    #     is_true = (ltl_goal[0][0] == LTL_BASE_VOCAB['True'])\n",
        "    #     is_false = (ltl_goal[0][0] == LTL_BASE_VOCAB['False'])\n",
        "    #     ltl_done = jnp.logical_or(is_true, is_false)\n",
        "\n",
        "    #     # Final done condition and observation for the step path\n",
        "    #     final_done = jnp.logical_or(done_st, ltl_done)\n",
        "    #     obs_st = {'image': obs_st, 'text': ltl_goal}\n",
        "    #     reward = reward_st + ltl_reward\n",
        "\n",
        "    #     # --- Calculate the outcome of a reset (the \"state_re\" path) ---\n",
        "    #     # 2. Call the wrapper's own reset method to get the reset state and obs\n",
        "    #     obs_re, state_re = self.reset(key_reset, params)\n",
        "\n",
        "    #     # --- Conditionally select the next state and observation ---\n",
        "    #     # 3. If final_done is true, pick the reset state/obs; otherwise, pick the step state/obs.\n",
        "    #     #    jax.tree.map is used because both state and obs are pytrees (custom class and dict).\n",
        "    #     state = jax.tree.map(\n",
        "    #         lambda x, y: jax.lax.select(final_done, x, y), state_re, state_st\n",
        "    #     )\n",
        "    #     obs = jax.tree.map(\n",
        "    #         lambda x, y: jax.lax.select(final_done, x, y), obs_re, obs_st\n",
        "    #     )\n",
        "\n",
        "    #     return obs, reward, final_done, info, state\n",
        "    @partial(jax.jit, static_argnames=(\"self\", \"params\"))\n",
        "    def step(self, key: jnp.ndarray, state: LTLEnvState, action: int, params: any) -> tuple[dict, float, bool, dict, LTLEnvState]:\n",
        "        \"\"\"Step env, progress LTL, and reset automatically if done.\"\"\"\n",
        "        # Split key for the step and a potential reset\n",
        "        step_key, reset_key = jax.random.split(key)\n",
        "\n",
        "        # --- 1. Perform the normal environment step ---\n",
        "        obs, reward, done, info, new_env_state = self.env.step_env(step_key, state.env_state, action, params)\n",
        "\n",
        "        # --- 2. Progress the LTL goal ---\n",
        "        truth_assignment = self.get_events(new_env_state, params)\n",
        "        ltl_goal, root_index, num_nodes = progress_and_clean_jax(state.ltl_goal, truth_assignment, state.root_idx, state.num_nodes)\n",
        "\n",
        "        # --- 3. Compute LTL reward and done status ---\n",
        "        is_true = (ltl_goal[0][0] == LTL_BASE_VOCAB['True'])\n",
        "        is_false = (ltl_goal[0][0] == LTL_BASE_VOCAB['False'])\n",
        "        ltl_done = jnp.logical_or(is_true, is_false)\n",
        "        ltl_reward = jax.lax.cond(\n",
        "            is_true,\n",
        "            lambda: 1.0,\n",
        "            lambda: jax.lax.cond(\n",
        "                is_false,\n",
        "                lambda: -1.0,\n",
        "                lambda: self.intrinsic\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # --- 4. Determine final reward and done for the current transition ---\n",
        "        final_reward = reward + ltl_reward\n",
        "        final_done = jnp.logical_or(done, ltl_done)\n",
        "\n",
        "        # --- 5. Conditionally determine the next state and observation ---\n",
        "        # This is the core of the auto-reset logic.\n",
        "        # We define two functions: one for the 'done' case (reset) and one for the 'not done' case.\n",
        "        # `jax.lax.cond` will execute one of them based on `final_done`.\n",
        "        # Both functions must return PyTrees (e.g., tuples, dicts) with the exact same structure.\n",
        "\n",
        "        def reset_case(_):\n",
        "            \"\"\"Called when final_done is True. Resets the environment.\"\"\"\n",
        "            # We pass the separate reset_key to the reset function.\n",
        "            return self.reset(reset_key, params)\n",
        "\n",
        "        def step_case(_):\n",
        "            \"\"\"Called when final_done is False. Returns the result of the normal step.\"\"\"\n",
        "            new_state = LTLEnvState(\n",
        "                env_state=new_env_state,\n",
        "                ltl_goal=ltl_goal,\n",
        "                ltl_original=state.ltl_original,\n",
        "                key=step_key, # The new state gets the used step_key\n",
        "                num_nodes=num_nodes,\n",
        "                root_idx=root_index\n",
        "            )\n",
        "            graph = self.ast_builder(ltl_goal, num_nodes)\n",
        "            new_obs = graph\n",
        "            return new_obs, new_state\n",
        "\n",
        "        # `cond` returns the output of either `reset_case` or `step_case`.\n",
        "        # Both functions return a tuple of (observation, state).\n",
        "        final_obs, final_state = jax.lax.cond(\n",
        "            final_done,\n",
        "            reset_case,\n",
        "            step_case,\n",
        "            operand=None # No operand needed as functions use variables from the outer scope\n",
        "        )\n",
        "\n",
        "        # --- 6. Return the final transition ---\n",
        "        # The reward and done flags are from the current step, but the observation\n",
        "        # and state are for the *next* step (which is a reset if done).\n",
        "        return final_obs, final_reward, final_done, info, final_state\n",
        "\n",
        "\n",
        "    # def sample_ltl_goal(self) -> any:\n",
        "    #     \"\"\"Sample LTL formula, adjust timeout for SequenceSampler.\"\"\"\n",
        "    #     formula = self.sampler.sample()\n",
        "    #     if isinstance(self.sampler, SequenceSampler):\n",
        "    #         def count_and(formula):\n",
        "    #             return sum(count_and(item) for item in formula if isinstance(item, tuple)) + 1\n",
        "    #         length = count_and(formula)\n",
        "    #         self.params = self.params.replace(timeout=25)  # 10 * length\n",
        "    #     return formula\n",
        "    def sample_ltl_goal(self,key) -> any:\n",
        "        \"\"\"Sample LTL formula, adjust timeout for SequenceSampler.\"\"\"\n",
        "        final_array, num_nodes, root_idx = self.sampler.sample(key)\n",
        "        return final_array, num_nodes, root_idx\n",
        "\n",
        "    def get_events(self, env_state: EnvState, params: EnvParams) -> chex.Array:\n",
        "        \"\"\"Get current propositions from the underlying env using its state.\"\"\"\n",
        "        return self.env.get_events(env_state, params)\n",
        "\n",
        "class NoLTLWrapper:\n",
        "    \"\"\"Remove LTL wrapper, return plain env observations.\"\"\"\n",
        "    def __init__(self, env: Environment, params: any):\n",
        "        self.env = env\n",
        "        self.params = params\n",
        "        self.observation_space = env.observation_space(params)\n",
        "\n",
        "    def reset(self, key: jnp.ndarray, params: any) -> tuple[any, any]:\n",
        "        return self.env.reset_env(key, params)\n",
        "\n",
        "    def step(self, key: jnp.ndarray, state: any, action: int, params: any) -> tuple[any, float, bool, dict, any]:\n",
        "        return self.env.step_env(key, state, action, params)\n",
        "\n",
        "    def get_propositions(self, params: any) -> list:\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "# --- import your LetterEnv implementation ---\n",
        "# from letter_env_fixed import LetterEnv, EnvParams   # <-- adjust to your filename\n",
        "# from ltl_env_wrapper import LTLEnv                  # <-- adjust if you saved wrapper separately\n",
        "\n",
        "def main_wrap():\n",
        "    # Create the base LetterEnv\n",
        "    env = LetterEnv()\n",
        "    params = EnvParams(grid_size=5, letters=\"aabbccddee\", use_fixed_map=False, use_agent_centric_view=False, timeout=10)\n",
        "\n",
        "    # Wrap with the LTLEnv wrapper\n",
        "    ltl_env = LTLEnv(env, params, progression_mode=\"full\", ltl_sampler=None, intrinsic=0.0)\n",
        "\n",
        "    # PRNG key\n",
        "    key = jax.random.PRNGKey(0)\n",
        "\n",
        "    # Reset\n",
        "    obs, state = ltl_env.reset(key, params)\n",
        "    print(\"\\n=== RESET ===\")\n",
        "    show(env, state.env_state, params)\n",
        "    show_features(env, obs['features'], params)\n",
        "    print(\"Initial LTL goal:\", state.ltl_goal)\n",
        "\n",
        "    str_to_action = {\"w\": 0, \"s\": 1, \"a\": 2, \"d\": 3}\n",
        "    step_idx = 0\n",
        "\n",
        "    while True:\n",
        "        step_idx += 1\n",
        "        print(\"\\n--- Step\", step_idx, \"---\")\n",
        "        cmd = input(\"Action? (w/a/s/d)  r=random  q=quit  > \").strip().lower()\n",
        "        if cmd == \"q\":\n",
        "            print(\"Quitting.\")\n",
        "            break\n",
        "        if cmd == \"r\":\n",
        "            action = random.choice([0, 1, 2, 3])\n",
        "        elif cmd in str_to_action:\n",
        "            action = str_to_action[cmd]\n",
        "        else:\n",
        "            print(\"Unknown command; try w/a/s/d, r, or q.\")\n",
        "            continue\n",
        "\n",
        "        # Step the LTL-wrapped env (we pass state.key so wrapper can split it internally)\n",
        "        obs, total_reward, done, info, state = ltl_env.step(state.key, state, action, params)\n",
        "\n",
        "        # Show grid and features (from underlying env_state stored in wrapper state)\n",
        "        show(env, state.env_state, params)\n",
        "        show_features(env, obs['features'], params)\n",
        "\n",
        "        # Print debug info\n",
        "        print(\"Action:\", action, \"(w/up=0, s/down=1, a/left=2, d/right=3)\")\n",
        "        print(\"Underlying env reward:\", info.get('env_reward'))\n",
        "        print(\"LTL reward:\", info.get('ltl_reward'))\n",
        "        print(\"Total reward returned:\", total_reward)\n",
        "        print(\"Truth assignment at new agent pos:\", repr(info.get('truth_assignment')))\n",
        "        print(\"Progressed LTL goal (in state):\", state.ltl_goal)\n",
        "        print(\"Is episode done?:\", done)\n",
        "\n",
        "        if done:\n",
        "            print(\"\\nEpisode finished (env or LTL termination).\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gje-VbyCIk10"
      },
      "source": [
        "# GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from typing import Callable, List, NamedTuple, Optional\n",
        "\n",
        "def segment_sum(data: jnp.ndarray, segment_ids: jnp.ndarray, num_segments: int) -> jnp.ndarray:\n",
        "    \"\"\"Computes the sum of elements within segments of an array.\"\"\"\n",
        "    # Note: jax.ops.segment_sum is deprecated. Using jax.lax.segment_sum instead.\n",
        "    # We pad segment_ids to avoid jax.lax.segment_sum's check.\n",
        "    # This assumes segment_ids are contiguous from 0 to num_segments - 1.\n",
        "    return jax.ops.segment_sum(data, segment_ids, num_segments=num_segments)\n",
        "\n",
        "class RelationalUpdate(nn.Module):\n",
        "    \"\"\"\n",
        "    A Flax module to compute messages based on relation type.\n",
        "    It applies a different linear transformation for each relation.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    num_relations: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, nodes: jnp.ndarray, senders: jnp.ndarray, edge_types: jnp.ndarray) -> jnp.ndarray:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            nodes: The node features array of shape `[num_nodes, in_features]`.\n",
        "            senders: The sender node indices for each edge of shape `[num_edges]`.\n",
        "            edge_types: The integer type for each edge of shape `[num_edges]`.\n",
        "\n",
        "        Returns:\n",
        "            An array of computed messages of shape `[num_edges, out_features]`.\n",
        "        \"\"\"\n",
        "        in_features = nodes.shape[-1]\n",
        "\n",
        "        # Create a stack of weight matrices, one for each relation type.\n",
        "        kernels = self.param(\n",
        "            'kernels',\n",
        "            nn.initializers.lecun_normal(),\n",
        "            (self.num_relations, in_features, self.features)\n",
        "        )\n",
        "\n",
        "        # Get the features of the sender nodes for each edge.\n",
        "        sender_features = nodes[senders]  # Shape: [num_edges, in_features]\n",
        "\n",
        "        # Select the appropriate kernel for each edge based on its type.\n",
        "        edge_kernels = kernels[edge_types] # Shape: [num_edges, in_features, out_features]\n",
        "\n",
        "        # Compute messages: messages[e] = W_type(e) * h_sender(e)\n",
        "        # einsum is efficient for this batched matrix-vector product.\n",
        "        messages = jnp.einsum('eif,ei->ef', edge_kernels, sender_features) # Shape: [num_edges, out_features]\n",
        "\n",
        "        return messages\n",
        "\n",
        "def CustomRelationalGraphConvolution(\n",
        "    update_node_module: nn.Module,\n",
        "    symmetric_normalization: bool = True\n",
        ") -> Callable[[dict], dict]:\n",
        "    \"\"\"\n",
        "    Returns a function that applies a Relational Graph Convolution layer.\n",
        "    This function wraps the message computation and performs aggregation.\n",
        "    \"\"\"\n",
        "    def _ApplyRGCN(graph: dict) -> dict:\n",
        "        nodes, senders, receivers, edge_types = (\n",
        "            graph[\"nodes\"], graph[\"senders\"], graph[\"receivers\"], graph[\"edge_types\"]\n",
        "        )\n",
        "\n",
        "        # Compute messages using the provided relation-specific update module.\n",
        "        messages = update_node_module(nodes, senders, edge_types)\n",
        "\n",
        "        total_num_nodes = nodes.shape[0]\n",
        "\n",
        "        # Aggregate messages at receiver nodes.\n",
        "        if symmetric_normalization:\n",
        "            ones = jnp.ones_like(senders, dtype=jnp.float32)\n",
        "            # Ensure degrees are calculated correctly even for isolated nodes\n",
        "            sender_degree = segment_sum(ones, senders, total_num_nodes).clip(1.0)\n",
        "            receiver_degree = segment_sum(ones, receivers, total_num_nodes).clip(1.0)\n",
        "\n",
        "            norm_senders = jax.lax.rsqrt(sender_degree)\n",
        "            norm_receivers = jax.lax.rsqrt(receiver_degree)\n",
        "\n",
        "            messages = messages * norm_senders[senders, None]\n",
        "            aggregated_nodes = segment_sum(messages, receivers, total_num_nodes)\n",
        "            aggregated_nodes = aggregated_nodes * norm_receivers[:, None]\n",
        "        else:\n",
        "            aggregated_nodes = segment_sum(messages, receivers, total_num_nodes)\n",
        "\n",
        "        return {**graph, \"nodes\": aggregated_nodes}\n",
        "\n",
        "    return _ApplyRGCN\n",
        "\n",
        "\n",
        "# --- GNN Module (Provided) ---\n",
        "\n",
        "class RGCNRootShared_no_jraph(nn.Module):\n",
        "    \"\"\"An RGCN with shared weights and root-based readout.\"\"\"\n",
        "    hidden_dim: int\n",
        "    num_layers: int\n",
        "    output_dim: int\n",
        "    num_edge_types: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, graph: dict) -> jnp.ndarray:\n",
        "        # Separate the 'is_root' flag from the node features.\n",
        "        h_features = graph[\"nodes\"][:, :-1]\n",
        "        is_root_nodes = graph[\"nodes\"][:, -1:]\n",
        "\n",
        "        # Initial linear projection.\n",
        "        h_0 = nn.Dense(features=self.hidden_dim, name='input_dense')(h_features)\n",
        "        h = h_0\n",
        "\n",
        "        # Define the single, shared convolutional layer module.\n",
        "        # Its input will have size 2 * hidden_dim due to the skip connection.\n",
        "        shared_update_module = RelationalUpdate(\n",
        "            features=self.hidden_dim,\n",
        "            num_relations=self.num_edge_types,\n",
        "            name='shared_rgcn_update'\n",
        "        )\n",
        "        rgcn_layer = CustomRelationalGraphConvolution(update_node_module=shared_update_module)\n",
        "\n",
        "        # Prepare graph structure (excluding nodes) for convolution loops\n",
        "        conv_graph = {key: val for key, val in graph.items() if key != 'nodes'}\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "            h_cat = jnp.concatenate([h, h_0], axis=-1)\n",
        "            current_layer_graph = {**conv_graph, \"nodes\": h_cat}\n",
        "\n",
        "            graph_after_rgcn = rgcn_layer(current_layer_graph)\n",
        "            # Use tanh activation as in the DGL example.\n",
        "            h = nn.tanh(graph_after_rgcn[\"nodes\"])\n",
        "\n",
        "        # Graph Readout: Select and sum root node embeddings.\n",
        "        num_graphs = graph[\"n_node\"].shape[0]\n",
        "\n",
        "        # This logic handles batching (num_graphs > 1) and single instances (num_graphs=1)\n",
        "        if num_graphs > 0:\n",
        "            # Create segment_ids for segment_sum\n",
        "            # This assumes nodes are packed contiguously per graph\n",
        "            num_total_nodes = h.shape[0]\n",
        "            num_nodes_per_graph = num_total_nodes // num_graphs\n",
        "            segment_ids = jnp.repeat(jnp.arange(num_graphs), repeats=num_nodes_per_graph)\n",
        "            graph_embeddings = segment_sum(h * is_root_nodes, segment_ids, num_segments=num_graphs)\n",
        "        else:\n",
        "            graph_embeddings = jnp.zeros((0, h.shape[-1]))\n",
        "\n",
        "        output = nn.Dense(features=self.output_dim, name='output_dense')(graph_embeddings)\n",
        "        return jnp.squeeze(output, axis=0) # Squeeze just in case batch size was 1\n",
        "\n"
      ],
      "metadata": {
        "id": "sMUuGGD6q9Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACModel"
      ],
      "metadata": {
        "id": "ICYkjC-lReq0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJXITbc_Intt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc38b097-fda6-4de9-d92c-be2e298b7f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    JAX-native Actor-Critic model that combines visual and textual (GNN) embeddings.\n",
        "    Accepts an Observation dataclass and returns a distrax.Distribution.\n",
        "    \"\"\"\n",
        "    text_embedding_size: int = 32\n",
        "    output_dim: int = 12\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"Initializes the sub-modules of the actor-critic model.\"\"\"\n",
        "        self.env_model = EnvModel()\n",
        "        VmappedGNN = nn.vmap(\n",
        "            RGCNRootShared_no_jraph,\n",
        "            in_axes=0,             # Map over the first axis of the input PyTree (the Graph dict)\n",
        "            out_axes=0,            # Stack outputs along the first axis\n",
        "            variable_axes={'params': None}, # Do not map/split the model parameters\n",
        "            split_rngs={'params': False}    # Do not split RNGs for parameter initialization\n",
        "        )\n",
        "\n",
        "        self.gnn = VmappedGNN(\n",
        "            output_dim=self.text_embedding_size,\n",
        "            hidden_dim=32,\n",
        "            num_layers=8,\n",
        "            num_edge_types=4\n",
        "        )\n",
        "\n",
        "        # Define actor network layers directly to output logits.\n",
        "        actor_layers = []\n",
        "\n",
        "\n",
        "        actor_layers.append(nn.Dense(features=self.output_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)))\n",
        "\n",
        "        self.actor_net = nn.Sequential(actor_layers)\n",
        "\n",
        "        # Critic network remains the same\n",
        "        self.critic_net = nn.Sequential([\n",
        "            nn.Dense(features=1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))\n",
        "        ])\n",
        "\n",
        "    def __call__(self, obs: Dict, carry=None, reset=None) -> Tuple[jnp.ndarray, distrax.Distribution, Any]:\n",
        "        \"\"\"\n",
        "        Forward pass for the Actor-Critic model.\n",
        "\n",
        "        Args:\n",
        "            obs: An Observation dataclass instance.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "            - v (jnp.ndarray): The state-value estimate (critic).\n",
        "            - distribution (distrax.Distribution): The policy distribution (actor).\n",
        "            - carry (Any): The recurrent hidden state (None for this model).\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        # Process text features with GNN\n",
        "        embedding_gnn = self.gnn(obs)\n",
        "        # --- End Modification ---\n",
        "\n",
        "\n",
        "        # --- Actor Pass ---\n",
        "        # Get unnormalized logits from the actor network\n",
        "        logits = self.actor_net(embedding)\n",
        "\n",
        "        # --- MODIFICATION: Create distribution ---\n",
        "        distribution = distrax.Categorical(logits=logits)\n",
        "        # --- End Modification ---\n",
        "\n",
        "        # --- Critic Pass ---\n",
        "        # Get the state-value estimate from the critic network\n",
        "        v = self.critic_net(embedding)\n",
        "\n",
        "\n",
        "        return distribution, v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEBV30WFIbNF"
      },
      "source": [
        "# Base Algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SxJTSUiIeRF"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from typing import Callable, Dict, Any, List\n",
        "from gymnax.environments.environment import Environment\n",
        "import gymnax\n",
        "import rlax\n",
        "import chex\n",
        "from flax.struct import PyTreeNode\n",
        "\n",
        "\n",
        "# This dataclass is still useful for defining the expected structure of experience data.\n",
        "@dataclass\n",
        "class Experience(PyTreeNode):\n",
        "    \"\"\"Stores all data for a batch of experiences.\"\"\"\n",
        "    obs: chex.ArrayTree\n",
        "    mask: chex.Array\n",
        "    action: chex.Array\n",
        "    value: chex.Array\n",
        "    reward: chex.Array\n",
        "    advantage: chex.Array\n",
        "    returnn: chex.Array\n",
        "    log_prob: chex.Array\n",
        "\n",
        "@dataclass\n",
        "class AlgoState:\n",
        "    \"\"\"High-level state of the algorithm, including model parameters and non-JAX logs.\"\"\"\n",
        "    params: Dict[str, Any]\n",
        "    opt_state: Any\n",
        "    log_return: List[float] = field(default_factory=list)\n",
        "    log_reshaped_return: List[float] = field(default_factory=list)\n",
        "    log_num_frames: List[int] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class RolloutState(PyTreeNode):\n",
        "    \"\"\"State carried through the JAX scan loop. Must be JAX-compatible.\"\"\"\n",
        "    rng: chex.PRNGKey\n",
        "    env_state: Any\n",
        "    obs: chex.ArrayTree\n",
        "    mask: chex.Array\n",
        "    ep_return: chex.Array\n",
        "    ep_reshaped_return: chex.Array\n",
        "    ep_num_frames: chex.Array\n",
        "\n",
        "\n",
        "class BaseAlgo(ABC):\n",
        "    \"\"\"Base class for JAX-based RL algorithms with JIT-compiled experience collection.\"\"\"\n",
        "    def __init__(self, env: Environment, env_params: Any, acmodel: nn.Module, num_procs: int,\n",
        "                 num_frames_per_proc: int, discount: float, gae_lambda: float,\n",
        "                 preprocess_obss: Callable = None, reshape_reward: Callable = None):\n",
        "        self.env = env\n",
        "        self.env_params = env_params\n",
        "        self.acmodel = acmodel\n",
        "        self.num_procs = num_procs\n",
        "        self.num_frames_per_proc = num_frames_per_proc\n",
        "        self.num_frames = num_procs * num_frames_per_proc\n",
        "        self.discount = discount\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.preprocess_obss = preprocess_obss or (lambda x, rng=None: x)\n",
        "        self.reshape_reward = reshape_reward\n",
        "        # Vmap the environment step function for parallel execution.\n",
        "        step_environment = lambda key, state, action, params: self.env.step(key, state, action, params)\n",
        "        self.vmapped_env_step = jax.vmap(\n",
        "            step_environment, in_axes=(0, 0, 0, None)\n",
        "        )\n",
        "    @jax.jit\n",
        "    def collect_experiences(self, algo_state: AlgoState, rollout_state: RolloutState) -> tuple[dict, dict, AlgoState, RolloutState]:\n",
        "        \"\"\"Collects rollouts and computes advantages. Can be JIT-compiled.\"\"\"\n",
        "\n",
        "\n",
        "        @jax.jit\n",
        "        def step_fn(carry: RolloutState, _):\n",
        "            \"\"\"\n",
        "            This function is scanned over, representing one time step for ALL parallel processes.\n",
        "            \"\"\"\n",
        "            rng, env_state, obs, mask = carry.rng, carry.env_state, carry.obs, carry.mask\n",
        "            ep_return, ep_reshaped_return, ep_num_frames = carry.ep_return, carry.ep_reshaped_return, carry.ep_num_frames\n",
        "\n",
        "            rng, subkey_policy, subkey_step = jax.random.split(rng, 3)\n",
        "\n",
        "            dist, value = self.acmodel.apply({'params': algo_state.params}, obs)\n",
        "\n",
        "            # Sample actions for all processes from the batched distribution\n",
        "            action = dist.sample(seed=subkey_policy)\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "            step_keys = jax.random.split(subkey_step, self.num_procs)\n",
        "            next_obs,reward,done, info, next_env_state= self.vmapped_env_step(\n",
        "                step_keys,\n",
        "                env_state,\n",
        "                action,\n",
        "                self.env_params\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "            # Assuming reshape_reward is a simple function. If not, it needs to be JAX-compatible.\n",
        "            reshaped_reward = reward * (self.discount ** ep_num_frames)\n",
        "\n",
        "            new_ep_return = ep_return + reward\n",
        "            new_ep_reshaped_return = ep_reshaped_return + reshaped_reward\n",
        "            new_ep_num_frames = ep_num_frames + 1\n",
        "\n",
        "            experience_step = {\n",
        "                \"obs\": obs, \"mask\": mask, \"action\": action, \"value\": value,\n",
        "                \"reward\": reshaped_reward, \"log_prob\": log_prob,\n",
        "                \"done\": done, \"ep_return_at_done\": new_ep_return,\n",
        "                \"ep_reshaped_return_at_done\": new_ep_reshaped_return,\n",
        "                \"ep_num_frames_at_done\": new_ep_num_frames\n",
        "            }\n",
        "            next_mask = 1.0 - done\n",
        "            next_carry = RolloutState(\n",
        "                rng=rng, env_state=next_env_state, obs=next_obs,\n",
        "                mask=next_mask, ep_return=new_ep_return * next_mask,\n",
        "                ep_reshaped_return=new_ep_reshaped_return * next_mask,\n",
        "                ep_num_frames=jnp.int32(new_ep_num_frames * next_mask)\n",
        "            )\n",
        "            return next_carry, experience_step\n",
        "\n",
        "        # --- KEY CHANGE: Scan the batch-oriented step_fn directly ---\n",
        "        # No vmap is needed here because step_fn already handles the batch of processes.\n",
        "        final_rollout_state, experiences = jax.lax.scan(\n",
        "            step_fn, rollout_state, None, length=self.num_frames_per_proc\n",
        "        )\n",
        "\n",
        "        # 'experiences' is now a dictionary of arrays, a standard JAX Pytree.\n",
        "        # Each value has shape: (num_frames_per_proc, num_procs, ...)\n",
        "\n",
        "        # --- KEY CHANGE: Use a single batched call for the next value ---\n",
        "        _, next_value = self.acmodel.apply({'params': algo_state.params}, final_rollout_state.obs)\n",
        "\n",
        "         # --- FIX: Construct the full value sequence for GAE calculation ---\n",
        "        # GAE requires values from step 0 to k, where k is the last step.\n",
        "        # `experiences['value']` contains values [v0, v1, ..., vk-1]\n",
        "        # `next_value` is vk. We concatenate them for the rlax function.\n",
        "        all_values = jnp.concatenate(\n",
        "            [experiences['value'], next_value[None, :]], axis=0\n",
        "        )\n",
        "\n",
        "        # Ensure discounts are zero for terminal states to reset GAE calculation\n",
        "        # Note: The reward and discount sequences should have length k.\n",
        "        discounts = self.discount * (1.0 - experiences['done'])\n",
        "\n",
        "        # --- FIX: Vmap the GAE calculation over the batch dimension ---\n",
        "        vmapped_gae = jax.vmap(\n",
        "            rlax.truncated_generalized_advantage_estimation,\n",
        "            # This tuple maps to the POSITIONAL arguments of the function below\n",
        "            in_axes=(1, 1, None, 1),\n",
        "            out_axes=1\n",
        "        )\n",
        "\n",
        "        # --- FIX: Call the vmapped function with POSITIONAL arguments ---\n",
        "        # The order must match the signature: (r_t, discount_t, lambda_, values)\n",
        "        advantages = vmapped_gae(\n",
        "            experiences['reward'],   # Corresponds to r_t\n",
        "            discounts,               # Corresponds to discount_t\n",
        "            self.gae_lambda,         # Corresponds to lambda_\n",
        "            all_values               # Corresponds to values\n",
        "        )\n",
        "\n",
        "        experiences['advantage'] = advantages\n",
        "        experiences['returnn'] = advantages + experiences['value']\n",
        "\n",
        "        keys_to_keep = [\"obs\", \"mask\", \"action\", \"value\", \"reward\", \"log_prob\", \"advantage\", \"returnn\"]\n",
        "        exps_dict = {key: experiences[key] for key in keys_to_keep}\n",
        "\n",
        "        # Reshape data to (num_procs * num_frames_per_proc, ...) for the update step\n",
        "        exps = jax.tree.map(lambda x: jnp.swapaxes(x, 0, 1).reshape(-1, *x.shape[2:]), exps_dict)\n",
        "\n",
        "        # Logging logic remains the same, operating on the collected experiences\n",
        "        done_mask = experiences['done'].flatten()\n",
        "\n",
        "        log_return = algo_state.log_return + experiences['ep_return_at_done'].flatten()[done_mask].tolist()\n",
        "        log_reshaped_return = algo_state.log_reshaped_return + experiences['ep_reshaped_return_at_done'].flatten()[done_mask].tolist()\n",
        "        log_num_frames = algo_state.log_num_frames + experiences['ep_num_frames_at_done'].flatten()[done_mask].tolist()\n",
        "\n",
        "        # dataclasses.replace is a good pattern for immutable updates\n",
        "        new_algo_state = AlgoState(\n",
        "            params=algo_state.params,\n",
        "            opt_state=algo_state.opt_state,\n",
        "            log_return=log_return,\n",
        "            log_reshaped_return=log_reshaped_return,\n",
        "            log_num_frames=log_num_frames\n",
        "        )\n",
        "\n",
        "        keep = max(int(done_mask.sum()), self.num_procs)\n",
        "        logs = {\n",
        "            \"return_per_episode\": new_algo_state.log_return[-keep:],\n",
        "            \"reshaped_return_per_episode\": new_algo_state.log_reshaped_return[-keep:],\n",
        "            \"num_frames_per_episode\": new_algo_state.log_num_frames[-keep:],\n",
        "            \"num_frames\": self.num_frames,\n",
        "        }\n",
        "        return exps, logs, new_algo_state, final_rollout_state\n",
        "\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def update_parameters(self, exps: dict, state: AlgoState, rng: jnp.ndarray) -> tuple[Dict, AlgoState]:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO Algo"
      ],
      "metadata": {
        "id": "Ew7IEP3nJzsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# PPO JAX Implementation\n",
        "#\n",
        "from typing import Callable, Dict, Any, Tuple\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "from gymnax.environments.environment import Environment\n",
        "\n",
        "# Import the dataclasses and BaseAlgo class provided in the prompt\n",
        "# Note: Some definitions from the prompt are included here for completeness.\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AlgoState:\n",
        "    \"\"\"High-level state of the algorithm, including model parameters and optimizer state.\"\"\"\n",
        "    params: Dict[str, Any]\n",
        "    opt_state: Any\n",
        "    # Non-JAX-compatible fields for logging are kept from the base class\n",
        "    log_return: list = field(default_factory=list)\n",
        "    log_reshaped_return: list = field(default_factory=list)\n",
        "    log_num_frames: list = field(default_factory=list)\n",
        "\n",
        "# Assuming the BaseAlgo class from the prompt is defined in the same scope\n",
        "# For this code to be executable, the full `BaseAlgo` class must be present.\n",
        "# class BaseAlgo(ABC):\n",
        "#     ... (definition as provided in the prompt) ...\n",
        "\n",
        "class PPO(BaseAlgo):\n",
        "    \"\"\"\n",
        "    The Proximal Policy Optimization (PPO) algorithm implemented in JAX.\n",
        "\n",
        "    This class inherits from the JAX-based `BaseAlgo` and implements the\n",
        "    parameter update step according to the PPO objective function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 env: Environment,\n",
        "                 env_params: Any,\n",
        "                 acmodel: nn.Module,\n",
        "                 num_procs: int,\n",
        "                 num_frames_per_proc: int,\n",
        "                 discount: float = 0.99,\n",
        "                 lr: float = 0.001,\n",
        "                 gae_lambda: float = 0.95,\n",
        "                 entropy_coef: float = 0.01,\n",
        "                 value_loss_coef: float = 0.5,\n",
        "                 max_grad_norm: float = 0.5,\n",
        "                 adam_eps: float = 1e-8,\n",
        "                 clip_eps: float = 0.2,\n",
        "                 epochs: int = 4,\n",
        "                 batch_size: int = 256,\n",
        "                 reshape_reward: Callable = None):\n",
        "        \"\"\"Initializes the PPO algorithm with its specific hyperparameters.\"\"\"\n",
        "        super().__init__(env, env_params, acmodel, num_procs, num_frames_per_proc, discount,\n",
        "                         gae_lambda, reshape_reward)\n",
        "\n",
        "        # PPO-specific hyperparameters\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.value_loss_coef = value_loss_coef\n",
        "        self.clip_eps = clip_eps\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # The total number of frames is flattened for minibatching\n",
        "        assert self.num_frames % self.batch_size == 0, \"Total frames must be divisible by batch_size.\"\n",
        "\n",
        "        # Initialize the optimizer using Optax\n",
        "        # We chain gradient clipping with the Adam optimizer\n",
        "        self.optimizer = optax.chain(\n",
        "            optax.clip_by_global_norm(max_grad_norm),\n",
        "            optax.adam(learning_rate=lr, eps=adam_eps)\n",
        "        )\n",
        "\n",
        "        # JIT-compile the core update loop for maximum performance\n",
        "        self._jitted_update_epochs = jax.jit(self._update_epochs)\n",
        "\n",
        "    def update_parameters(self, exps: Experience, algo_state: AlgoState, rng: chex.PRNGKey) -> Tuple[Dict, AlgoState]:\n",
        "        \"\"\"\n",
        "        Updates the actor-critic model parameters using PPO.\n",
        "\n",
        "        This method serves as a JAX-friendly wrapper around the core, JIT-compiled\n",
        "        update logic contained in `_update_epochs`.\n",
        "\n",
        "        Args:\n",
        "            exps: A PyTree of collected experiences.\n",
        "            algo_state: The current state of the algorithm (parameters, optimizer state).\n",
        "            rng: A JAX random key for shuffling data.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing a dictionary of logs and the updated `AlgoState`.\n",
        "        \"\"\"\n",
        "        # Execute the JIT-compiled update function\n",
        "        (params, opt_state), logs = self._jitted_update_epochs(\n",
        "            algo_state.params,\n",
        "            algo_state.opt_state,\n",
        "            exps,\n",
        "            rng\n",
        "        )\n",
        "\n",
        "        # Create the new state with updated parameters and optimizer state\n",
        "        new_algo_state = AlgoState(\n",
        "            params=params,\n",
        "            opt_state=opt_state,\n",
        "            log_return=algo_state.log_return,\n",
        "            log_reshaped_return=algo_state.log_reshaped_return,\n",
        "            log_num_frames=algo_state.log_num_frames\n",
        "        )\n",
        "\n",
        "        return logs, new_algo_state\n",
        "\n",
        "    def _loss_fn(self, params: Dict, batch: Experience) -> Tuple[chex.Array, Tuple]:\n",
        "        \"\"\"\n",
        "        Computes the PPO loss for a single minibatch of experience.\n",
        "        This function is designed to be used with `jax.grad`.\n",
        "\n",
        "        Args:\n",
        "            params: The model parameters.\n",
        "            batch: A PyTree representing a minibatch of experiences.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the total loss and auxiliary metrics (policy loss,\n",
        "            value loss, entropy) for logging.\n",
        "        \"\"\"\n",
        "        # Forward pass to get policy distribution and value estimates\n",
        "        dist, value = self.acmodel.apply({'params': params}, batch.obs)\n",
        "\n",
        "        # --- Policy Loss (Clipped Surrogate Objective) ---\n",
        "        new_log_prob = dist.log_prob(batch.action)\n",
        "        ratio = jnp.exp(new_log_prob - batch.log_prob)\n",
        "\n",
        "        # Normalize advantages for stability (a common practice)\n",
        "        advantage = (batch.advantage - batch.advantage.mean()) / (batch.advantage.std() + 1e-8)\n",
        "\n",
        "        surr1 = ratio * advantage\n",
        "        surr2 = jnp.clip(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * advantage\n",
        "        policy_loss = -jnp.minimum(surr1, surr2).mean()\n",
        "\n",
        "        # --- Value Loss (Clipped Value Function) ---\n",
        "        value_clipped = batch.value + jnp.clip(value - batch.value, -self.clip_eps, self.clip_eps)\n",
        "        surr1_v = (value - batch.returnn)**2\n",
        "        surr2_v = (value_clipped - batch.returnn)**2\n",
        "        value_loss = 0.5 * jnp.maximum(surr1_v, surr2_v).mean()\n",
        "\n",
        "        # --- Entropy Bonus ---\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        # --- Total Loss ---\n",
        "        total_loss = (\n",
        "            policy_loss\n",
        "            + self.value_loss_coef * value_loss\n",
        "            - self.entropy_coef * entropy\n",
        "        )\n",
        "\n",
        "        return total_loss, (policy_loss, value_loss, entropy)\n",
        "\n",
        "\n",
        "    def _update_epochs(self, params: Dict, opt_state: Any, exps: Experience, rng: chex.PRNGKey) -> Tuple[Tuple, Dict]:\n",
        "        \"\"\"\n",
        "        The core update logic that iterates over epochs and minibatches.\n",
        "        This entire function is JIT-compiled.\n",
        "        \"\"\"\n",
        "        def _epoch_step(carry, _):\n",
        "            \"\"\"Represents one full pass (epoch) over the entire dataset.\"\"\"\n",
        "            p, o, r = carry # params, opt_state, rng\n",
        "\n",
        "            # Shuffle the experience data at the start of each epoch\n",
        "            r, perm_key = jax.random.split(r)\n",
        "            permutation = jax.random.permutation(perm_key, self.num_frames)\n",
        "            shuffled_exps = jax.tree.map(lambda x: x[permutation], exps)\n",
        "\n",
        "            # Reshape data into minibatches\n",
        "            num_minibatches = self.num_frames // self.batch_size\n",
        "            minibatches = jax.tree.map(\n",
        "                lambda x: x.reshape((num_minibatches, self.batch_size) + x.shape[1:]),\n",
        "                shuffled_exps\n",
        "            )\n",
        "\n",
        "            def _minibatch_step(carry, batch):\n",
        "                \"\"\"Updates parameters using a single minibatch.\"\"\"\n",
        "                params_mb, opt_state_mb = carry\n",
        "                # Compute gradients and auxiliary loss data\n",
        "                grad, (pi_loss, v_loss, ent) = jax.grad(self._loss_fn, has_aux=True)(params_mb, batch)\n",
        "                # Update parameters and optimizer state\n",
        "                updates, new_opt_state = self.optimizer.update(grad, opt_state_mb, params_mb)\n",
        "                new_params = optax.apply_updates(params_mb, updates)\n",
        "                return (new_params, new_opt_state), (pi_loss, v_loss, ent)\n",
        "\n",
        "            # Scan the update function over all minibatches\n",
        "            (new_p, new_o), (policy_losses, value_losses, entropies) = jax.lax.scan(\n",
        "                _minibatch_step, (p, o), minibatches\n",
        "            )\n",
        "\n",
        "            # Return updated state and logs for the epoch\n",
        "            return (new_p, new_o, r), (policy_losses.mean(), value_losses.mean(), entropies.mean())\n",
        "\n",
        "        # Scan the epoch function over the configured number of epochs\n",
        "        (final_params, final_opt_state, _), (pl, vl, ent) = jax.lax.scan(\n",
        "            _epoch_step, (params, opt_state, rng), None, length=self.epochs\n",
        "        )\n",
        "\n",
        "        # Aggregate logs by taking the mean across all epochs\n",
        "        logs = {\n",
        "            \"policy_loss\": pl.mean(),\n",
        "            \"value_loss\": vl.mean(),\n",
        "            \"entropy\": ent.mean()\n",
        "        }\n",
        "\n",
        "        return (final_params, final_opt_state), logs"
      ],
      "metadata": {
        "id": "LcMv9XFSJ2JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main\n"
      ],
      "metadata": {
        "id": "Z2GlQ8gATtEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lGkQNBqGYsuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import wandb  # For logging\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm  # For progress bar\n",
        "from dataclasses import dataclass, field\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Callable, Dict, Any, List, Tuple\n",
        "from gymnax.environments.environment import Environment\n",
        "import gymnax\n",
        "import rlax\n",
        "import chex\n",
        "from flax.struct import PyTreeNode\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main training loop for PPO with wandb logging.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Hyperparameters ---\n",
        "    config = {\n",
        "        \"LR\": 0.005,\n",
        "        \"NUM_PROCS\": 64, # Increased for more parallel data\n",
        "        \"NUM_FRAMES_PER_PROC\": 128,\n",
        "        \"DISCOUNT\": 0.9,\n",
        "        \"GAE_LAMBDA\": 0.5,\n",
        "        \"ENTROPY_COEF\": 0.01,\n",
        "        \"VALUE_LOSS_COEF\": 0.5,\n",
        "        \"MAX_GRAD_NORM\": 0.5,\n",
        "        \"ADAM_EPS\": 1e-8,\n",
        "        \"CLIP_EPS\": 0.1,\n",
        "        \"EPOCHS\": 2,\n",
        "        \"TOTAL_FRAMES\": 10_000_000,\n",
        "        \"USE_WANDB\": True,\n",
        "        \"WANDB_PROJECT\": \"jax_ppo_ltl_example\", # Change this\n",
        "        \"WANDB_ENTITY\": \"your_username\",       # <--!! CHANGE THIS !!\n",
        "        \"CHECKPOINT_DIR\": \"./checkpoints/ppo_ltl\",\n",
        "        \"CHECKPOINT_INTERVAL\": 50, # Save every 50 updates\n",
        "    }\n",
        "\n",
        "    # Calculate batch_size\n",
        "    total_frames_per_update = config[\"NUM_PROCS\"] * config[\"NUM_FRAMES_PER_PROC\"]\n",
        "    # We set batch_size so it divides the total frames, e.g., for 4 minibatches\n",
        "    config[\"BATCH_SIZE\"] = total_frames_per_update // 8\n",
        "    assert total_frames_per_update % config[\"BATCH_SIZE\"] == 0, \"BATCH_SIZE must divide (NUM_PROCS * NUM_FRAMES_PER_PROC)\"\n",
        "\n",
        "\n",
        "    # --- W&B Setup ---\n",
        "    if config[\"USE_WANDB\"]:\n",
        "        wandb.init(\n",
        "            project=config[\"WANDB_PROJECT\"],\n",
        "            entity=config[\"WANDB_ENTITY\"],\n",
        "            config=config,\n",
        "            monitor_gym=False, # gymnax not supported by default\n",
        "            save_code=True,\n",
        "        )\n",
        "\n",
        "    os.makedirs(config[\"CHECKPOINT_DIR\"], exist_ok=True)\n",
        "    options = ocp.CheckpointManagerOptions(max_to_keep=5, create=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        config[\"CHECKPOINT_DIR\"],\n",
        "        options=options\n",
        "    )\n",
        "\n",
        "    # --- JAX Key Setup ---\n",
        "    key = jax.random.PRNGKey(int(time.time()))\n",
        "    key, model_key, reset_key, rollout_key, update_key = jax.random.split(key, 5)\n",
        "\n",
        "    # --- Environment Setup (using your provided snippet) ---\n",
        "    print(\"Setting up environment...\")\n",
        "    letters = encode_letters(\"abcdefghijkl\")\n",
        "    env_params = EnvParams(grid_size=7, letters=letters, use_fixed_map=False, use_agent_centric_view=True, timeout=75, num_unique_letters=len(set(letters)))\n",
        "\n",
        "    env = LetterEnv(num_letters=len(set(env_params.letters)), timeout=env_params.timeout)\n",
        "    ltl_env = LTLEnv(env, env_params, progression_mode=\"full\", ltl_sampler=None, intrinsic=0.0)\n",
        "\n",
        "    # --- Algorithm and Model Setup ---\n",
        "    print(\"Setting up model and algorithm...\")\n",
        "    acmodel = ActorCritic(output_dim=len(set(env_params.letters)))\n",
        "\n",
        "    # Instantiate the PPO algorithm with its hyperparameters\n",
        "    algo = PPO(\n",
        "        env=ltl_env,\n",
        "        env_params=env_params,\n",
        "        acmodel=acmodel,\n",
        "        num_procs=config[\"NUM_PROCS\"],\n",
        "        num_frames_per_proc=config[\"NUM_FRAMES_PER_PROC\"],\n",
        "        discount=config[\"DISCOUNT\"],\n",
        "        lr=config[\"LR\"],\n",
        "        gae_lambda=config[\"GAE_LAMBDA\"],\n",
        "        entropy_coef=config[\"ENTROPY_COEF\"],\n",
        "        value_loss_coef=config[\"VALUE_LOSS_COEF\"],\n",
        "        max_grad_norm=config[\"MAX_GRAD_NORM\"],\n",
        "        adam_eps=config[\"ADAM_EPS\"],\n",
        "        clip_eps=config[\"CLIP_EPS\"],\n",
        "        epochs=config[\"EPOCHS\"],\n",
        "        batch_size=config[\"BATCH_SIZE\"],\n",
        "        reshape_reward=None  # Add your reward shaping function here if you have one\n",
        "    )\n",
        "\n",
        "    # --- Initialization ---\n",
        "    print(\"Initializing environment and model parameters...\")\n",
        "    # Vmap the environment reset for parallel processes\n",
        "    vmapped_env_reset = jax.vmap(ltl_env.reset, in_axes=(0, None))\n",
        "    reset_keys = jax.random.split(reset_key, config[\"NUM_PROCS\"])\n",
        "\n",
        "    # Get initial state\n",
        "    init_obs, init_env_state = vmapped_env_reset(reset_keys, env_params)\n",
        "\n",
        "    # Initialize model parameters\n",
        "    init_params = algo.acmodel.init(model_key, init_obs)['params']\n",
        "\n",
        "    # Initialize optimizer state\n",
        "    init_opt_state = algo.optimizer.init(init_params)\n",
        "\n",
        "    # Initialize the algorithm state\n",
        "    algo_state = AlgoState(\n",
        "        params=init_params,\n",
        "        opt_state=init_opt_state,\n",
        "        # Lists are initialized by default factory\n",
        "    )\n",
        "\n",
        "    # Initialize the rollout state\n",
        "    rollout_state = RolloutState(\n",
        "        rng=rollout_key,\n",
        "        env_state=init_env_state,\n",
        "        obs=init_obs,\n",
        "        mask=jnp.ones((config[\"NUM_PROCS\"],), dtype=jnp.float32),\n",
        "        ep_return=jnp.zeros((config[\"NUM_PROCS\"],)),\n",
        "        ep_reshaped_return=jnp.zeros((config[\"NUM_PROCS\"],)),\n",
        "        ep_num_frames=jnp.zeros((config[\"NUM_PROCS\"],), dtype=jnp.int32)\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    num_updates = config[\"TOTAL_FRAMES\"] // total_frames_per_update\n",
        "    print(f\"Starting training for {num_updates} updates ({config['TOTAL_FRAMES']} total frames)...\")\n",
        "    print(f\"Total frames per update: {total_frames_per_update}\")\n",
        "    print(f\"Batch size: {config['BATCH_SIZE']}, Minibatches per epoch: {total_frames_per_update // config['BATCH_SIZE']}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for update_idx in tqdm(range(num_updates), desc=\"Training Updates\"):\n",
        "        # 1. Collect Experiences\n",
        "        exps, rollout_logs, algo_state, rollout_state = algo.collect_experiences(\n",
        "            algo_state, rollout_state\n",
        "        )\n",
        "\n",
        "        # 2. Update Parameters\n",
        "        update_key, sub_key = jax.random.split(update_key)\n",
        "        update_logs, algo_state = algo.update_parameters(\n",
        "            exps, algo_state, sub_key\n",
        "        )\n",
        "\n",
        "        # 3. Logging\n",
        "        total_frames_so_far = (update_idx + 1) * total_frames_per_update\n",
        "        end_time = time.time()\n",
        "        fps = total_frames_so_far / (end_time - start_time)\n",
        "\n",
        "        # Combine logs\n",
        "        logs = {\n",
        "            \"update\": update_idx,\n",
        "            \"total_frames\": total_frames_so_far,\n",
        "            \"fps\": fps,\n",
        "            \"policy_loss\": float(update_logs[\"policy_loss\"]),\n",
        "            \"value_loss\": float(update_logs[\"value_loss\"]),\n",
        "            \"entropy\": float(update_logs[\"entropy\"]),\n",
        "        }\n",
        "\n",
        "        # Process episode logs (mean of all episodes finished *in this update*)\n",
        "        if len(rollout_logs[\"return_per_episode\"]) > 0:\n",
        "            logs[\"mean_return\"] = np.mean(rollout_logs[\"return_per_episode\"])\n",
        "            logs[\"mean_reshaped_return\"] = np.mean(rollout_logs[\"reshaped_return_per_episode\"])\n",
        "            logs[\"mean_episode_length\"] = np.mean(rollout_logs[\"num_frames_per_episode\"])\n",
        "\n",
        "        logs[\"num_episodes_finished_this_update\"] = len(rollout_logs[\"return_per_episode\"])\n",
        "\n",
        "        if config[\"USE_WANDB\"]:\n",
        "            wandb.log(logs, step=total_frames_so_far)\n",
        "\n",
        "        if (update_idx + 1) % config[\"CHECKPOINT_INTERVAL\"] == 0:\n",
        "            print(f\"\\nSaving checkpoint at update {update_idx}...\")\n",
        "            # Save the entire algo_state (params, opt_state, etc.)\n",
        "            checkpoint_manager.save(\n",
        "                update_idx,\n",
        "                args=ocp.args.StandardSave(algo_state)\n",
        "            )\n",
        "            checkpoint_manager.wait_until_finished() # Wait for save to complete\n",
        "            print(\"Checkpoint saved.\")\n",
        "\n",
        "        # Clear the per-update episode logs from the algo_state\n",
        "        # to prevent logging stale data\n",
        "        algo_state = algo_state.replace(\n",
        "            log_return=[],\n",
        "            log_reshaped_return=[],\n",
        "            log_num_frames=[]\n",
        "        )\n",
        "\n",
        "    # --- End of Training ---\n",
        "    print(\"Training finished.\")\n",
        "    if config[\"USE_WANDB\"]:\n",
        "        wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "DVOZG25E0hEX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}